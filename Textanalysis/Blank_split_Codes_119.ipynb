{"cells":[{"cell_type":"markdown","metadata":{"id":"auZSKtn62CAg"},"source":["# 1. 파이썬 기초"]},{"cell_type":"markdown","metadata":{"id":"fr7g7o7d2CAj"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPG32AtY2CAj"},"outputs":[],"source":["# 병렬 실행은 같은 주피터 파일에 있으면 실행되지 않으므로 별도의 파일로 처리\n","from Functions import *\n","\n","import multiprocessing\n","import os\n","import pandas\n","import sqlite3"]},{"cell_type":"markdown","metadata":{"id":"jHpq2_el2CAk"},"source":["# 주석 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7C-YisrK2CAl"},"outputs":[],"source":["# 한 줄 주석\n","'''\n","여러 줄 주석은 이렇게 쓰거나\n","또는....\n","'''\n","\"\"\"\n","이렇게 작성합니다.\n","예시로 작성했습니다.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"-pMWaZt32CAl"},"source":["# 데이터 유형"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j65REO7O2CAl"},"outputs":[],"source":["# =====================================================================================================================================\n","# 예제로 사용할 값입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","value1 = 1.1\n","value2 = '1.2'\n","value3 = 1.3\n","value4 = 1\n","■■■■■ = 0\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 정수/실수/참거짓/문자열 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('정수 유형')\n","print('실제 값', value1, '(', type(■■■■■), ')을 정수로 표현하면', int(value1), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('실수 유형')\n","print('실제 값', value2, '(', type(value2), ')을 실수로 표현하면', float(value2), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('참/거짓 유형')\n","print('실제 값', value4, '(', type(value4), ')을 참/거짓으로 표현하면', bool(value4), '입니다.')\n","print('실제 값', value5, '(', type(value5), ')을 참/거짓으로 표현하면', bool(value5), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('문자 유형')\n","print('실제 값', value3, '(', type(value3), ')을 문자열로 표현하면', str(■■■■■), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# set 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","set_sample = set()\n","set_sample.add(1)\n","set_sample.add(2)\n","set_sample.add(3)\n","set_sample.add(4)\n","set_sample.add(5)\n","set_sample.add(1)\n","print('----------------------------------------------------------------------------------------------------')\n","print('set 유형 (값이 중복되지 않습니다.)')\n","print(set_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# list 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","list_sample = list()\n","list_sample.append(1)\n","list_sample.append(2)\n","list_sample.append(3)\n","list_sample.append(■■■■■)\n","list_sample.append(5)\n","list_sample.append(1)\n","print('----------------------------------------------------------------------------------------------------')\n","print('list 유형 (중복을 허용합니다.)')\n","print(list_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# dict 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","dict_sample = ■■■■■()\n","dict_sample['key:1'] = 1\n","dict_sample['key:2'] = 2\n","dict_sample['key:3'] = 3\n","dict_sample['key:4'] = 4\n","dict_sample['key:5'] = 5\n","dict_sample['key:1'] = 6\n","print('----------------------------------------------------------------------------------------------------')\n","print('dict 유형 (key 에서는 중복을 허용하지 않고, value에서는 중복을 허용합니다.)')\n","print(■■■■■)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# DataFrame 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","df_sample = pandas.DataFrame([\n","    {'C1':1, 'C2':2},\n","    {'C1':3, 'C2':4},\n","    {'C1':■■■■■, 'C2':6},\n","    {'C1':7, 'C2':8},\n","])\n","print('----------------------------------------------------------------------------------------------------')\n","print('DataFrame 유형 (jupyter에서는 display를 사용하면 조금 더 확인이 쉽습니다.')\n","###display(df_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# Series 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('Series 유형 (DataFrame의 부분 집합입니다.)')\n","print(type(df_sample['C1']), df_sample['C1'])\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"Bxy4x6uS2CAm"},"source":["# 조건문"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mD4UasHQ2CAn"},"outputs":[],"source":["value1 = 1\n","value2 = 3.2\n","value3 = True\n","value4 = 'XX'\n","\n","# =====================================================================================================================================\n","# 정수 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 2\n","if value1 > condition_value:\n","    print('value1 은 ', condition_value, '보다 큽니다.')\n","elif ■■■■■ == condition_value:\n","    print('value1 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value1 은 ', condition_value, '보다 작습니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 실수 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 3\n","if value2 > condition_value:\n","    print('value2 은 ', condition_value, '보다 큽니다.')\n","elif value2 == condition_value:\n","    print('value2 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value2 은 ', condition_value, '보다 작습니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 참거짓 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","if ■■■■■:\n","    print('value3 은 ', value3, '참입니다.')\n","else:\n","    print('value3 은 ', value3, '거짓입니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 문자열 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 'YY'\n","if value4 > condition_value:\n","    print('value4 은 ', condition_value, '보다 큽니다.')\n","elif value4 == condition_value:\n","    print('value4 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value4 은 ', ■■■■■, '보다 작습니다.')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"15Kb-pSC2CAn"},"source":["# 반복문"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZ2Yig1Y2CAn"},"outputs":[],"source":["# =====================================================================================================================================\n","# For 구문입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = list()\n","for index in range(10):\n","    list_to_log.append(index)\n","print(list_to_log)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# While 구문 입니다. (무한 반복을 주의합니다.)\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = ■■■■■()\n","MAX_LOOP_COUNT = 10\n","flag_loop = True\n","loop_count = 0\n","while flag_loop:\n","    list_to_log.append(loop_count)\n","    loop_count += 1\n","    if loop_count > MAX_LOOP_COUNT:\n","        flag_loop = False\n","print(■■■■■)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# Do-While 구문 입니다. (파이썬에는 없습니다.)\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = list()\n","MAX_LOOP_COUNT = 10\n","flag_loop = True\n","loop_count = 0\n","list_to_log.append(loop_count)\n","loop_count += 1\n","while flag_loop:\n","    list_to_log.append(loop_count)\n","    ■■■■■ += 1\n","    if loop_count > ■■■■■:\n","        flag_loop = False\n","print(list_to_log)\n","# =====================================================================================================================================\n","print('----------------------------------------------------------------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"4LBYLv122CAo"},"source":["# 예외 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n25et2hL2CAo"},"outputs":[],"source":["value1 = '1.2'\n","try:\n","    print('문자열( 값:', value1, ')을 정수 유형으로 변경합니다.')\n","    print('값을 출력합니다. [', int(value1), ']')\n","    print('첫 시도에서 데이터 유형 변경에 성공하였습니다.')\n","except:\n","    print('데이터 유형 변환에 실패하여, 실수형으로 먼저 변경한 후 정수형으로 변경합니다.')\n","    print('값을 출력합니다. [', int(■■■■■(value1)), ']')\n","    print('예외 처리로 데이터 유형 변경에 성공하였습니다.')"]},{"cell_type":"markdown","metadata":{"id":"eosDFv4J2CAo"},"source":["# 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wyhw6-RZ2CAo"},"outputs":[],"source":["def function_sample(p_args):\n","    print(type(p_args), p_args)\n","\n","value1 = 1\n","value2 = ■■■■■\n","value3 = '2.3'\n","value4 = {'key': 'value'}\n","value5 = ['element']\n","\n","function_sample(value1)\n","function_sample(value2)\n","function_sample(value3)\n","function_sample(value4)\n","function_sample(■■■■■)"]},{"cell_type":"markdown","metadata":{"id":"lmiwzTMH2CAp"},"source":["# 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3VzM-0z2CAp"},"outputs":[],"source":["class class_sample():\n","    def method_sample(self, p_args):\n","        print(■■■■■(p_args), p_args)\n","\n","value1 = 1\n","value2 = 1.2\n","value3 = '2.3'\n","value4 = {'key': 'value'}\n","value5 = ['element']\n","\n","sample = class■■■■■()\n","sample.method_sample(value1)\n","sample.method_sample(value2)\n","sample.method_sample(value3)\n","sample.method_sample(value4)\n","sample.method_sample(value5)"]},{"cell_type":"markdown","metadata":{"id":"fpR2bg4E2CAp"},"source":["# 파일 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqWkpaFS2CAp"},"outputs":[],"source":["# =====================================================================================================================================\n","# 파일에 내용을 기록합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('파일에 내용을 기록합니다.')\n","sample_file_name = 'sample.csv'\n","with ■■■■■(sample_file_name, 'w') as f:\n","    f.write('C1,C2\\n')\n","    f.write('1,2\\n')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 파일 안의 내용을 확인합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('파일 안의 내용을 확인합니다.')\n","###display(pandas.read_csv(sample_file_name))\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","# 파일의 내용을 읽습니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('파일의 내용을 읽습니다.')\n","with open(sample_file_name, 'r') as f:\n","    print(f.read())\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 사용한 파일을 삭제합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('사용한 파일을 삭제합니다.')\n","os.remove(sample_file_name)\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"SpXCU05Q2CAp"},"source":["# 데이터베이스 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJExYfMp2CAp"},"outputs":[],"source":["# =====================================================================================================================================\n","# 데이터베이스와 연결합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('connect to database')\n","■■■■■ = sqlite3.connect(':memory:')\n","curs = conn.cursor()\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 테이블을 생성합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('create table')\n","try:\n","    curs.execute('drop table sample')\n","    conn.commit()\n","except:\n","    pass\n","curs.execute('''\n","create table sample (\n","    c1      int,\n","    c2      float,\n","    c3      text\n",")\n","''')\n","conn.commit()\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 데이터를 입력합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('insert data to table')\n","df = pandas.DataFrame([\n","    {\n","        'c1': 1,\n","        'c2': 1.1,\n","        'c3': '1.1.1',\n","    },\n","    {\n","        'c1': 2,\n","        'c2': 2.2,\n","        'c3': '2.2.2',\n","    },\n","])\n","###display(df)\n","df.to_sql('sample', conn, if_exists='append', ■■■■■=None)\n","\n","print('----------------------------------------------------------------------------------------------------')\n","print('check data in table')\n","df2 = pandas.read_sql('select * from sample', conn)\n","###display(df2)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 데이터를 읽습니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('read data in table')\n","curs.execute('select * from sample')\n","rs = curs.fetchall()\n","for row ■■■■■ rs:\n","    print('c1:[', row[0], '], c2:[', row[1], '], c3:[', row[2], ']')\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"LThKWxv32CAp"},"source":["# 병렬처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4F0Vaxco2CAq"},"outputs":[],"source":["list_param = list()\n","for idx in range(■■■■■):\n","    dict_param = dict()\n","    dict_param['index'] = idx\n","    list_param.append(dict_param)\n","\n","p = multiprocessing.Pool(3)\n","# parallel_print 함수에 대한 정의는 .py 파일에 별도로 저장되어 있어야 합니다.\n","p.map(parallel_print, list_param)"]},{"cell_type":"markdown","metadata":{"id":"w6dw-j1m2CAq"},"source":["# 전처리 기초"]},{"cell_type":"markdown","metadata":{"id":"UA2TBnhm2CAq"},"source":["# 사전 준비"]},{"cell_type":"markdown","metadata":{"id":"DSLTV8Od2CAq"},"source":["# 1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2r9vjok2CAq"},"outputs":[],"source":["import pandas\n","import sqlite3"]},{"cell_type":"markdown","metadata":{"id":"OeV_NJ4e2CAq"},"source":["# 2. 예제 데이터 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGtskN_i2CAq"},"outputs":[],"source":["t1 = pandas.DataFrame([\n","    { 'c1': 11, 'c2': 111, 'c3': 1111, },\n","    { 'c1': 12, 'c2': 121, 'c3': 1211, },\n","    { 'c1': 13, 'c2': 131, 'c3': 1311, },\n","    { 'c1': 14, 'c2': 141, 'c3': 1411, },\n","    { 'c1': 15, 'c2': 151, 'c3': 1511, },\n","    { 'c1': ■■■■■, 'c2': 161, 'c3': 1611, },\n","])\n","t2 = pandas.DataFrame([\n","    { 'c1': 11, 'c2': 111, 'c3': 1111, 'c4': 21111, 'c5': ■■■■■, },\n","    { 'c1': 12, 'c2': 121, 'c3': ■■■■■, 'c4': 22111, 'c5': 221111, },\n","    { 'c1': 13, 'c2': 131, 'c3': 1311, 'c4': ■■■■■, 'c5': 231111, },\n","    { 'c1': 21, 'c2': 211, 'c3': 2111, 'c4': 21111, 'c5': 211111, },\n","    { 'c1': 22, 'c2': 221, 'c3': 2211, 'c4': 22111, 'c5': ■■■■■, },\n","    { 'c1': 23, 'c2': 231, 'c3': 2311, 'c4': 23111, 'c5': 231111, },\n","    { 'c1': 24, 'c2': 241, 'c3': 2411, 'c4': 24111, 'c5': 241111, },\n","    { 'c1': 25, 'c2': 251, 'c3': 2511, 'c4': 25111, 'c5': 251111, },\n","    { 'c1': 26, 'c2': 261, 'c3': 2611, 'c4': 26111, 'c5': 261111, },\n","])\n","\n","db_conn = sqlite3.connect(':memory:')\n","t1.to_sql('t1', db_conn, if_exists='replace')\n","t2.to_sql('t2', db_conn, if_exists='replace')"]},{"cell_type":"markdown","metadata":{"id":"BLKSalXh2CAq"},"source":["# UNION"]},{"cell_type":"markdown","metadata":{"id":"Ke2_EfOL2CAq"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMzO3cXv2CAr"},"outputs":[],"source":["df = ■■■■■([t1, t2], sort=False)\n","df = df[['c1', 'c2', 'c3']]\n","df = df.drop_duplicates()\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"mY-nAhld2CAr"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOp5YaAe2CAr"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," union\n","select c1, c2, c3\n","  from t2\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","#print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"eMg1I0sL2CAr"},"source":["# UNION ALL"]},{"cell_type":"markdown","metadata":{"id":"kp68bDV52CAr"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_tUhn662CAr"},"outputs":[],"source":["df = pandas.concat([t1, t2], ■■■■■=False)\n","df = df[['c1', 'c2', 'c3']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"OYrQ-09y2CAr"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-HJz7ML2CAr"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," union all\n","select c1, c2, c3\n","  from t2\n","'''\n","df_from_db = ■■■■■(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"bSDwbov-2CAr"},"source":["# RELATIVE COMPLEMENT"]},{"cell_type":"markdown","metadata":{"id":"mVWOY6rv2CAr"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUpOssKD2CAr"},"outputs":[],"source":["set_t2 = set(t2['c1'].tolist())\n","df = t1[~t1['c1']■■■■■(set_t2)]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"jS3Jj1Kq2CAs"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctihd5wQ2CAs"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," where not exists (select 1\n","                     from t2\n","                    where t1.c1 = t2.c1)\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","#display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"cBM8bbjZ2CAs"},"source":["# INNER JOIN"]},{"cell_type":"markdown","metadata":{"id":"UAfNl3Mz2CAs"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL8fSMF32CAs"},"outputs":[],"source":["df_t1 = t1.set_index(['c1'])\n","df_t2 = t2.set_index(['c1'])\n","df = df_t1.join(df_t2, how='inner', lsuffix='', rsuffix='_DEL')\n","df = df.reset_index()\n","df = df[['c1', 'c2', 'c3', 'c4']]\n","print(■■■■■shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"HHhx9NV02CAs"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmJebbMJ2CAs"},"outputs":[],"source":["sql_select = '''\n","select t1.c1, t1.c2, t1.c3, t2.c4\n","  from t1\n"," inner join t2\n","         on t1.c1 = t2.c1\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"zmj3PQz22CAs"},"source":["# LEFT OUTER JOIN"]},{"cell_type":"markdown","metadata":{"id":"LTsKCdcQ2CAs"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gl7U4Tg92CAs"},"outputs":[],"source":["df_t1 = t1.set_index(['c1'])\n","df_t2 = t2.set_index(['c1'])\n","df = ■■■■■(df_t2, how='left', lsuffix='', rsuffix='_DEL')\n","df = df.reset_index()\n","df = df[['c1', 'c2', 'c3', 'c4']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"qTFDuAZw2CAs"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lx0gllpj2CAs"},"outputs":[],"source":["sql_select = '''\n","select t1.c1, t1.c2, t1.c3, t2.c4\n","  from t1\n","  left outer join t2\n","               on t1.c1 = t2.c1\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"YZAIMDO22CAt"},"source":["# GROUP BY"]},{"cell_type":"markdown","metadata":{"id":"FgbjCNJ72CAt"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4He__Z0y2CAt"},"outputs":[],"source":["df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ( 'c3', 'count')).reset_index()\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"uvbJ5awo2CAt"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kj-er1t42CAt"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, count(c3) as cnt_c3\n","  from t1\n"," group by c1, c2\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"Qsf-Ho1x2CAt"},"source":["# WHERE"]},{"cell_type":"markdown","metadata":{"id":"Ak16puIU2CAt"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulXxaXs12CAt"},"outputs":[],"source":["df = t2[t2['c4'] == 22111]\n","■■■■■ = df[['c1', 'c2', 'c3']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"JYpOsVwy2CAt"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAT8QygJ2CAt"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t2\n"," where c4 = 22111\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"a0z1meSA2CAt"},"source":["# HAVING"]},{"cell_type":"markdown","metadata":{"id":"TZjQjA4D2CAu"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fkedhl12CAu"},"outputs":[],"source":["import numpy\n","df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ( 'c3', numpy.sum)).reset_index()\n","df = df[df['cnt_c3'] > 1311]\n","print(■■■■■shape)\n","##display(df)"]},{"cell_type":"markdown","metadata":{"id":"pXywJAss2CAu"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocChIFIx2CAu"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, sum(c3) as cnt_c3\n","  from t1\n"," group by c1, c2\n","having sum(c3) > 1311\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"GgZOZ3HQ2CAu"},"source":["# 전처리를 통한 모델 데이터셋 생성하기"]},{"cell_type":"markdown","metadata":{"id":"5LWC3JV32CAu"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVfls1lP2CAu"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Pgjf3OKu2CAu"},"source":["### 데이터 설명\n","\n","#### 파일 및 설명\n","\n","- sales.csv - 2017~2019 3개년치 판매데이터 중 2019년 데이터만 활용\n","- product_hierarchy.csv - 대중소 등 5단계별 상품그룹코드 및 상품 사이즈\n","- store_cities.csv - 매장도시, 유형, 면적\n","\n","#### 컬럼(변수) 설명\n","\n","- store_id - 매장코드\n","- product_id - 상품코드\n","- date - 판매일자\n","- sales - 일별 판매량\n","- revenue - 일별 이익액\n","- stock - 일별 재고량\n","- price - 판매가/가격\n","- promotype1 - 판매채널1의 프로모션1 유형\n","- promobin1 - 판매채널1 프로모션1의 프로모션 binning 적용율\n","- promotype2 - 판매채널2의 프로모션2 유형\n","- promobin2 - 판매채널2 프로모션2의 프로모션 binning 적용율\n","- promodiscount2 - 프로모션2의 적용된 할인율\n","- promodiscounttype_2 - 프모로션2 할인율 유형\n","- product_length - 제품 가로사이즈\n","- product_depth - 제품 세로사이즈\n","- product_width - 제품 높이사이즈\n","- hierarchy1_id - 그룹코드1\n","- hierarchy2_id - 그룹코드2\n","- hierarchy3_id - 그룹코드3\n","- hierarchy4_id - 그룹코드4\n","- hierarchy5_id - 그룹코드5\n","- storetype_id - 매장유형코드\n","- store_size - 매장면적\n","- city_id - 도시코드"]},{"cell_type":"markdown","metadata":{"id":"Z5JMpGuh2CAu"},"source":["#### 결측치 처리 및 모델링 변수 활용 방법 \n","#### 범주형(명목형 또는 순서형) - 각 조건별로 One-hot encoding\n","#### 연속형(수치형) - 구간 설정 후 각 구간별로 One-hot encoding - EDA를 통해 구간 정의 필요\n","#### 또는 연속데이터로 활용\n","\n","#### Shift 및 Rolling을 활용한 시계열 데이터 생성"]},{"cell_type":"markdown","metadata":{"id":"4xZ4w1ge2CAu"},"source":["# 데이터 불러오기 (csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5XZTGW62CAv"},"outputs":[],"source":["#파일 경로 설정\n","PATH = os.path.abspath(\"./Data\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tm_YS0Uo2CAv"},"outputs":[],"source":["# sales_df = pd.read_csv(PATH+'sales.csv') #2019년도 데이터만 추출\n","sales_df = ■■■■■(PATH+'/sales_2019.csv')\n","store_df = pd.read_csv(PATH+'/store_cities.csv')\n","product_df = pd.read_csv(PATH+'/product_hierarchy.csv')"]},{"cell_type":"markdown","metadata":{"id":"i9CjV6HX2CAv"},"source":["# 데이터 불러오기(csv) -> 쓰기(json) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dG3SZ5ui2CAv"},"outputs":[],"source":["file_handler = open(PATH+'/sample_json.csv', 'r')\n","lines = file_handler.readlines()\n","# print(lines)\n","previous_data = None\n","■■■■■ = {}\n","for line in lines:\n","    line_data = [data.strip() for data in line.split(',')]\n","    \n","    # 이전 줄이 있을 때에만 수행\n","    if previous_data is not None:\n","        p1, p2, ■■■■■ = previous_data[0], previous_data[1], previous_data[2]\n","        c1, c2, c3 = line_data[0], ■■■■■[1], line_data[2]\n","        \n","        # level1이 다른 경우 새로운 level1, level2 키를 만들어서 값 하나를 저장\n","        if p1 != c1: \n","            json_dictionary[c1] = {}\n","            json_dictionary[c1][c2]=c3\n","        else:\n","            # level2가 다른 경우 새로운 level2 키를 만들어서 값 하나를 저장\n","            if p2 != c2:\n","                json_dictionary[c1][c2]=c3\n","                # level1, level2 가 이전과 같으므로 level3의 값만 추가하여 갱신\n","            else:\n","                previous_value = json_dictionary[c1][■■■■■]\n","                json_dictionary[c1][c2]= f'{previous_value}:{c3}'        \n","        \n","    previous_data = line_data\n","\n","file_handler.close()\n","\n","json_data = json.dumps(■■■■■)\n","print(json_data)"]},{"cell_type":"markdown","metadata":{"id":"O9I7eYyW2CAv"},"source":["# 데이터 불러오기 (json) -> 쓰기(csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9szoXdy2CAv"},"outputs":[],"source":["json_parsed = json.loads(json_data)\n","columns = ['C1', 'C2', 'C3']\n","csv_string = ','.join(columns)\n","\n","first_keys = ■■■■■(■■■■■(json_parsed.keys()))\n","■■■■■ key1 in first_keys:\n","    second_keys = sorted(list(json_parsed[key1].keys()))\n","    for key2 in second_keys:\n","        third_keys = json_parsed[key1][key2].split(':')\n","        for key3 in third_keys:\n","            csv_string = csv_string + f'\\n{key1},{key2},{key3}'\n","            \n","file_handler = open('json_to_csv.csv', 'w')\n","file_handler.write(csv_string)\n","file_handler.close()"]},{"cell_type":"markdown","metadata":{"id":"5HBym2cf2CAv"},"source":["# 필요한 데이터 유지 (필요없는 변수 삭제)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSlIDU1h2CAv"},"outputs":[],"source":["#### drop 함수 사용하여 'year' 변수 삭제\n","sales_df.drop('year', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv_eCANV2CAv"},"outputs":[],"source":["#### 필요 변수만 유지 - 'year' 변수만 제외하고 유지\n","columns = ['product_id','store_id','date','sales','revenue','stock','price','promo_type_1','promo_bin_1','promo_type_2','promo_bin_2','promo_discount_2','promo_discount_type_2']\n","sales_df = sales_df[columns]"]},{"cell_type":"markdown","metadata":{"id":"0v77-R9g2CAv"},"source":["# 결측치 변수 삭제"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJXbdeC12CAw"},"outputs":[],"source":["#결측치여부 - 각 변수별 결측치 개수 확인\n","print(sales_df.isna().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaZcs2sH2CAw"},"outputs":[],"source":["# \"sales\" 변수 기준으로 결측치 행 삭제\n","#방법 1 - 결측치를 제외한 인덱스 유지\n","sales_df = sales_df[sales_df['sales'].isna()!=True]■■■■■(drop=True)\n","#방법 2 - 결측치 인덱스를 드롭한 남은 인덱스 추출\n","#sales_df = sales_df.iloc[sales_df['sales'].dropna().index].reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CvzIgd32CAw"},"outputs":[],"source":["#판매수량 1이상인 데이터만 유지\n","#sales_df = sales_df[sales_df.sales>0]\n","#인덱스 수정\n","#sales_df.reset_index(drop=True, inplace=True)\n","\n","#또는 \n","sales_df = sales_df[sales_df.sales>0]■■■■■(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"sEJ1_mky2CAw"},"source":["# 결측치 채우기"]},{"cell_type":"markdown","metadata":{"id":"GnGCVCg72CAw"},"source":["# 문자열 변수의 결측치는 ''으로 변경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNEEv_jD2CAw"},"outputs":[],"source":["#결측치 수정\n","sales_df.promo_bin_1.fillna('', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B44jOYhn2CAw"},"outputs":[],"source":["#결측치 수정\n","sales_df.promo_bin_2.fillna('', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpCr1L7N2CAw"},"outputs":[],"source":["sales_df.promo_discount_type_2.fillna('', inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"rOODrW4h2CAw"},"source":["# 가격 결측치 채우기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RboxEt-H2CAw"},"outputs":[],"source":["#가격 데이터와 결측치 데이터 분리\n","#가격정보 유 데이터 df1 으로 생성\n","df1 = sales_df[sales_df.price.isna()!=True]\n","#가격정보 무 데이터 df0 으로 생성\n","df0 = sales_df[sales_df.price.isna()==True]\n","\n","#가격정보 없는 매장의 상품 정보가 가격정보가 있는 데이터에서 존재하는지 확인\n","#상품코드/매장코드 기준으로 병합\n","#날짜별 가격 정보 포함\n","#df1의 상품코드/매장코드/가격/날짜 와 df0의 상품코드/매장코드로 상품코드/매장코드 기준으로 병합 후 dfz로 생성\n","dfz = pd.merge(■■■■■[['product_id', 'store_id', 'price', 'date']], df0[['product_id', 'store_id']], on=['product_id', 'store_id'], how='inner')\n","\n","#상품코드/매장코드/날짜/판매가격 순으로 정렬 (날짜와 판매가격은 낮은순으로)\n","■■■■■ = dfz.sort_values(['product_id', 'store_id', 'date', 'price'], ascending=[True, True, True, True]).reset_index(drop=True)\n","\n","#가격정보 없는 데이터에 가격정보 병합\n","#가격정보 없는 데이터에서 가격 변수 삭제후 병합\n","df0.drop('price', axis=1, inplace=True)\n","df0 = pd.merge(df0, dfz, on=['product_id', 'store_id'], how='left')\n","\n","#수정 후 가격 확인 건 df01으로 생성\n","df01 = df0[df0.price.isna()!=True]\n","# df01.shape\n","\n","#수정 후 가격 미확인 건 df00으로 생성\n","df00 = df0[df0.price.isna()]\n","# df00.shape\n","\n","#수정 후에도 확인 안되는 상품은 매장구분 없이 동일 상품 가격 정보 추출\n","dfz0 = ■■■■■[dfz.product_id.isin(df00.product_id)]\n","\n","#상품의 최소가격으로 맵핑하기 위해 최소가격 기준으로 그룹핑\n","dfz0 = dfz0.groupby('product_id').min()['price'].reset_index()\n","\n","#dfz0로 생성한 최고가격 정보를 df00 테이블에 업데이트 \n","#df00의 가격 변수를 삭제가 필요함 (dfz0와 df00에 각 가격변수가 있기 때문에, 동일명변수가 존재시, _x와 _y로 각각 생성됨\n","df00.drop('price', axis=■■■■■, inplace=True)\n","df00 = pd.merge(df00, dfz0, on='product_id', how='left')\n","\n","#df1, df01, df00으로 가격정보를 다 업데이트 한 테이블을 sales_df_fixed로 병합\n","sales_df_fixed = pd.concat([df1, df01, df00]).reset_index(drop=■■■■■)"]},{"cell_type":"markdown","metadata":{"id":"lQBH9d1B2CAx"},"source":["# One-hot encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYGb3-Wa2CAx"},"outputs":[],"source":["#가격 업데이트 이후에도 가격 정보가 없는 제품의 경우 원핫인코딩으로 가격유무 여부 변수로 구분자 생성\n","#가격 결측치 여부\n","sales_df_fixed.loc[sales_df_fixed['price']■■■■■()==True, 'price_ohe'] = 1\n","sales_df_fixed.loc[sales_df_fixed['price'].isna()==False, 'price_ohe'] = 0\n","sales_df_fixed['price_ohe'] = sales_df_fixed['price_ohe'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5sUsMEX2CAx"},"outputs":[],"source":["#promo_type_1 유형별 원핫인코딩\n","for i in sales_df_fixed.promo_type_1.unique().tolist():\n","    if i == '':\n","        continue\n","    else:\n","        #변수 생성\n","        sales_df_fixed['promo_type_1_'+i] = 0\n","        sales_df_fixed.loc[■■■■■==i, 'promo_type_1_'+i] = 1\n","    \n","#promo_type_1 변수 삭제\n","■■■■■('promo_type_1', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrhTNuXE2CAx"},"outputs":[],"source":["#각 변수의 유형별_원핫인코딩\n","columns = ['promo_bin_1', 'promo_type_2', 'promo_bin_2', 'promo_discount_type_2']\n","for col in columns:\n","    for i in sales_df_fixed[col].unique().tolist():\n","        if i == '':\n","            continue\n","        else:\n","            sales_df_fixed[■■■■■+'_'+i] = ■■■■■\n","            sales_df_fixed.loc[sales_df_fixed[col]==i, col+'_'+i] = 1\n","\n","#원핫인코딩 완룐된 변수 삭제\n","sales_df_fixed.drop(columns, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmvBzAIK2CAx"},"outputs":[],"source":["#할인율 없는 정보는 0으로 대체\n","■■■■■(0, inplace=True)\n","\n","#가격 없는 정보는 0으로 대체\n","sales_df_fixed.price.fillna(0, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"OozzQzFe2CAx"},"source":["# 그룹핑"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaogHCeQ2CAx"},"outputs":[],"source":["#product_id 기준으로 판매수량 및 이익액 총합\n","product_grouping = sales_df_fixed[['product_id', 'sales', 'revenue']]■■■■■('product_id').sum().reset_index()\n","\n","#store_id 기준으로 판매수량 및 이익액 총합\n","store_grouping = sales_df_fixed[['store_id', 'sales', 'revenue']].groupby('store_id').sum().reset_index()\n","\n","#product_id + store_id 기준으로 판매수량 및 이익액 총합\n","product_store_grouping = sales_df_fixed[['product_id', 'store_id', 'sales', 'revenue']].groupby(['product_id', 'store_id']).sum().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wdzSrnF2CAx"},"outputs":[],"source":["#매장별 상품별 판매수량 pivot table\n","store_product_pivot = sales_df_fixed.pivot_table(index='product_id', columns='store_id', values='sales')■■■■■(0)"]},{"cell_type":"markdown","metadata":{"id":"Iv1rbxb82CAx"},"source":["# 데이터 그룹핑 및 합/평균/최소/최대 값 구하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39VPyxw52CAx"},"outputs":[],"source":["#상품의 최저값 구하기 - 0값 제외\n","sales_df_fixed0 = sales_df_fixed[sales_df_fixed.price_ohe==0]\n","min_price = ■■■■■[['product_id', 'price']].groupby('product_id').min().reset_index()\n","\n","#상품의 최고값 구하기\n","■■■■■ = sales_df_fixed0[['product_id', 'price']].groupby('product_id').max().reset_index()\n","\n","#최저/최고값 비교하기\n","price_compare = pd.merge(min_price, max_price, on='product_id')\n","\n","#변수명 변경\n","price_compare.columns = ['product_id', 'min_price', 'max_price']\n","\n","#최고/최저값 차이\n","price_compare['price_diff'] = price_compare['max_price'] - price_compare['min_price']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK5w9xa32CAx"},"outputs":[],"source":["# 최고=최저 동일 상품\n","same_price = price_compare[price_compare.price_diff==0]\n","\n","# 최고!=최저 다른 상품 \n","■■■■■iff_price = price_compare[price_compare.price_diff!=0]"]},{"cell_type":"markdown","metadata":{"id":"iq7EZ1ok2CAy"},"source":["# 테이블 병합 Merging & Concatenating left, inner, outer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiGGVo252CAy"},"outputs":[],"source":["df = pd.merge(sales_df_fixed, store_df, on='store_id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0A-JXL-2CAy"},"outputs":[],"source":["#storetype_id, city_id 유형별 원핫인코딩\n","columns = ['storetype_id', 'city_id']\n","for col in columns:\n","    ■■■■■ i in df[col].unique().tolist():\n","        #변수 생성\n","        df[col+'_'+i] = ■■■■■\n","        df.loc[df[col]==i, col+'_'+i] = 1\n","    \n","#promo_type_1 변수 삭제\n","df.drop(columns, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rtl6ZF-i2CAy"},"outputs":[],"source":["#매장면적의 분포를 확인 후 대중소 기준으로 구분함\n","#plt.hist(df['store_size'], bins=20);"]},{"cell_type":"markdown","metadata":{"id":"wYoJTnqI2CAy"},"source":["# 함수 활용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBs0iGls2CAy"},"outputs":[],"source":["#아래 매장면적 기준으로 3개 그룹으로 구분 \n","#0~30\n","#30~50\n","#50<\n","\n","def storesize_grouping(x):\n","    if ■■■■■<=30:\n","        return 1\n","    elif x<=50:\n","        return 2\n","    else:\n","        return 3\n","    \n","df['storesize_grouping'] = df['store_size'].apply(storesize_grouping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMdsWZo92CAy"},"outputs":[],"source":["#구분한 매장을 store_size 원핫인코딩으로 변환\n","df.loc[df['storesize_grouping']==1, 'storesize_small'] = ■■■■■\n","df.loc[df['storesize_grouping']==2, 'storesize_medium'] = 1\n","df.loc[df['storesize_grouping']==3, 'storesize_large'] = 1\n","\n","#NA 값 0 변경\n","df[['storesize_small', 'storesize_medium', 'storesize_large']] = df[['storesize_small', 'storesize_medium', 'storesize_large']].fillna(0)\n","\n","#store_size 변수 삭제\n","df.drop(['store_size', 'storesize_grouping'], axis=1, ■■■■■=True)\n","\n","## 매장사이즈별 원핫인코딩을 두단계 (분리 + 원핫인코딩)이 아닌 한단계로 실행 가능\n","\n","#97만건으로 계속 전처리 하는 것 보다, 628건으로 우선 전처리 후 병합하는 방법이 효율적임\n","product_df = product_df[product_df.product_id.isin(df.product_id.unique())].reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgciH9o02CAy"},"outputs":[],"source":["#상품별 가로, 세로, 높이로 부피 산출 함수를 생성\n","def get_volume(x, y, z):\n","    return x*y*z\n","\n","product_df['product_volume'] = get_volume(product_df['product_length'], ■■■■■['product_depth'], product_df['product_width'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKqmNo9y2CAz"},"outputs":[],"source":["#사이즈 정보가 누락으로 결측치 확인\n","product_df[product_df.product_volume.isna()]\n","\n","#cluster_0 기준의 평균값 대채\n","product_df.loc[(product_df['cluster_id']=='cluster_0')&(product_df['product_volume'].isna()), 'product_volume'] = product_df[product_df['cluster_id']=='cluster_0']['product_volume'].mean()\n","\n","#cluster_9 기준의 평균값 대체\n","product_df.loc[(product_df['cluster_id']=='cluster_9')&(■■■■■['product_volume'].isna()), 'product_volume'] = product_df[product_df['cluster_id']=='cluster_9']['product_volume'].mean()\n","\n","#상품 부피 사이즈는 백분위수 기준 10단계로 그룹화\n","■■■■■ = np.arange(1,11)\n","■■■■■['volume_qcut'] = pd.qcut(product_df['product_volume'], q=10, labels=labels)\n","\n","\n","#부피 qcut 유형별 원핫인코딩\n","for i in sorted(set(product_df.volume_qcut)):\n","    #변수 생성\n","    product_df['volume_'+str(i)] = 0\n","    product_df.loc[product_df['volume_qcut']==i, 'volume_'+str(i)] = 1\n","\n","#대분류 중분류 기준으로 원핫인코딩 \n","#hierarchy1_id, hierarchy2_id 유형별 원핫인코딩\n","columns = ['cluster_id', 'hierarchy1_id', 'hierarchy2_id']\n","for ■■■■■ in columns:\n","    for i in product_df[col].unique().tolist():\n","        #변수 생성\n","        product_df[i] = 0\n","        product_df.loc[product_df[col]==i, i] = ■■■■■\n","\n","#원핫인코딩 등 변수생성이 완료된 원래 변수는 삭제\n","product_df.drop(['product_length', 'product_depth', 'product_width', 'product_volume', 'volume_qcut', 'cluster_id', 'hierarchy1_id', 'hierarchy2_id', 'hierarchy3_id', 'hierarchy4_id', 'hierarchy5_id'], axis=1, inplace=True)\n","\n","#변수 전처리가 완료된 상품 데이터를 df데이터와 병합\n","df = pd.merge(df, product_df, on='product_id', how='left')"]},{"cell_type":"markdown","metadata":{"id":"b6SMAx5k2CAz"},"source":["# 시계열 데이터 생성하기 Shifting과 Rolling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqAGxT3C2CAz"},"outputs":[],"source":["#날짜별 판매량 시계열 데이터 생성\n","ts = ■■■■■[['date', 'sales']]\n","\n","#날짜별로 총 판매량 그룹핑\n","ts = ts.groupby('date').sum()\n","\n","#판매량 Shifiting \n","ts['sales_p1'] = ts.sales.shift(1)\n","ts['sales_m1'] = ts.sales.shift(-1)\n","\n","#판매량 5단위로 Rolling\n","#합\n","ts['sales_sum5'] = ts.sales.rolling(5).sum()\n","#최소\n","ts['sales_min5'] = ts.sales.rolling(5)■■■■■()\n","#최대\n","ts['sales_max5'] = ts.sales.rolling(5).max()\n","#평균\n","ts['sales_avg5'] = ts.sales.rolling(5).mean()"]},{"cell_type":"markdown","metadata":{"id":"bzO49wrC2CAz"},"source":["# 시각화 기초"]},{"cell_type":"markdown","metadata":{"id":"g2GhFZtm2CAz"},"source":["# 이용 파일 명\n","Groceries_dataset.csv\n","\n","# 이용 컬럼 명\n","['Member_number', 'Date', 'itemDescription']"]},{"cell_type":"markdown","metadata":{"id":"fcv_w6cC2CAz"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnYQR4QX2CAz"},"outputs":[],"source":["import numpy\n","import pandas\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm"]},{"cell_type":"markdown","metadata":{"id":"wHufHMru2CAz"},"source":["# 화면 표시 방식 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvTXTB0F2CAz"},"outputs":[],"source":["%matplotlib inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (50, ■■■■■)\n","plt.rcParams['lines.linewidth'] = 2\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTO2l8xi2CAz"},"outputs":[],"source":["%%html\n","<■■■■■>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html th { text-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"dB8ALuz92CAz"},"source":["# 샘플용 데이터 로딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ej2P1tkO2CA0"},"outputs":[],"source":["df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n","##display(df)"]},{"cell_type":"markdown","metadata":{"id":"R1fjoG152CA0"},"source":["# Pie Chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHTotxbz2CA0"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key'])■■■■■( CNT = ('itemDescription', 'count'))\n","df_pie = df_pie.sort_values(by='CNT', ascending=False)\n","df_pie_top = df_pie.head(4)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","list_data.append(numpy.sum(■■■■■['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","colors = sns.color_palette('spring')[0:10]\n","# plt.pie(list_data, labels=list_labels, colors=colors, autopct='%.0f%%')\n","#plt.pie(list_data, labels=list_labels, autopct='%.0f%%')\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"qfUlaTa12CA0"},"source":["# Bar chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QleXMQEz2CA0"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","df_pie = ■■■■■(■■■■■='CNT', ascending=False)\n","df_pie['CNT'] = df_pie['CNT'] - 3200\n","df_pie_top = ■■■■■(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[■■■■■] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","ax = sns.barplot(x=list_data, y=list_labels)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"N5PeTs5F2CA0"},"source":["# Column chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wpfew8OA2CA0"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","■■■■■['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","■■■■■ = df_pie.sort_values(by='CNT', ascending=False)\n","■■■■■['CNT'] = df_pie['CNT'] - 3200\n","df_pie_top = df_pie.head(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = ■■■■■()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","ax = sns.barplot(y=list_data, x=list_labels)\n","\n","#plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q2I4xmAZ2CA0"},"source":["# Line chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4EH1amH2CA0"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","df_pie = ■■■■■(by='CNT', ■■■■■=False)\n","df_pie['CNT'] = df_pie['CNT'] - 3200\n","df_pie_top = df_pie.head(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = ■■■■■()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 10\n","ax = ■■■■■(y=list_data, x=list_labels)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"kKC7lC7G2CA0"},"source":["# Area chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbEcpDQf2CA0"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key']■■■■■(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = ■■■■■(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n","df_transpose = ■■■■■()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * ■■■■■\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[■■■■■]) * 10\n","df_transpose = df_transpose.head(20)\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 10\n","#plt.stackplot(df_transpose.index, df_transpose['PV-Piece-90'], df_transpose['PV-Piece-80'], df_transpose['PV-Piece-70'], df_transpose['PV-Piece-60'])\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"GhMWyLZX2CA0"},"source":["# Scatter chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SvVIhsI2CA1"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","■■■■■['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=■■■■■)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for ■■■■■ in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * ■■■■■\n","df_transpose = df_transpose.head(20)\n","\n","sns.set(style='darkgrid')\n","\n","sns.set(■■■■■='darkgrid')\n","ax = sns.scatterplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'], s=■■■■■)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"335Y4QUg2CA1"},"source":["# Secondary Axis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdRVIa2p2CA1"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key']■■■■■(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","■■■■■ = pandas.pivot_table(df_groupby, values='CNT', ■■■■■=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + ■■■■■(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10000\n","df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose.head(20)\n","\n","sns.set(■■■■■='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","ax = sns.barplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'])\n","ax2 = ■■■■■()\n","ax = sns.lineplot(x=df_transpose.index, y=df_transpose['PV-Piece-60'], ax=ax2)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"D7WWDUuF2CA1"},"source":["# Heatmap chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bz120mni2CA1"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","■■■■■['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, ■■■■■=0)\n","df_transpose.columns = ['PV-' + str(col) for ■■■■■ in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[■■■■■]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose[['PV-Piece-80', 'PV-Piece-90']]\n","df_transpose = df_transpose.astype(int)\n","df_transpose['CNT'] = 1\n","df_chart = pandas.pivot_table(df_transpose, ■■■■■='CNT', ■■■■■=['PV-Piece-90'], columns=['PV-Piece-80'], aggfunc=numpy.sum, fill_value=0)\n","# #display(df_chart)\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","sns.set(font_scale=1.5)\n","heat_map = sns.heatmap(df_chart, annot=False, cmap='coolwarm', robust=True, fmt='.0f', linewidths=.5)\n","heat_map.tick_params(labelsize=10)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"y2ssWrpQ2CA1"},"source":["# Box plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4itGvCAe2CA1"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = df['key']■■■■■(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) ■■■■■ col in df_transpose.columns.values]\n","■■■■■ = df_transpose.reset_index()\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose[['PV-Piece-80']]\n","# #display(df_transpose)\n","\n","sns.set(style='darkgrid')\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","sns.set(font_scale=1.5)\n","ax = sns.boxplot(x = \"PV-Piece-80\",  data = df_transpose)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7TCflQ4T2CA1"},"source":["# 5. 비지도 학습 기초"]},{"cell_type":"markdown","metadata":{"id":"SOEJ1WcX2CA1"},"source":["# 이용 파일 명\n","Groceries_dataset.csv\n","\n","# 이용 컬럼 명\n","['Member_number', 'Date', 'itemDescription']"]},{"cell_type":"markdown","metadata":{"id":"JSgypxW92CA2"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cynfpj6X2CA2"},"outputs":[],"source":["import mlxtend.frequent_patterns\n","import numpy\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings\n","import yellowbrick.cluster\n","\n","import sklearn.cluster\n","import sklearn.metrics\n","import sklearn.decomposition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNPHtu6C2CA2"},"outputs":[],"source":["%matplotlib inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (50, 10)\n","■■■■■['lines.linewidth'] = 5\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgySH6YU2CA2"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html th { ■■■■■-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"knlRG29K2CA2"},"source":["# 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb9v-4nc2CA2"},"outputs":[],"source":["df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"iELzHJme2CA2"},"source":["# 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eFIk37N2CA2"},"outputs":[],"source":["■■■■■ = dict()\n","\n","dict_args['id_col_name'] = 'Member_number'\n","dict_args['sequence_col_name'] = 'Date'\n","dict_args['product_col_name'] = 'itemDescription'\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/Groceries_dataset.csv'\n","\n","# 최소 지지도\n","dict_args['min_support'] = 0.1\n","# 최소 신뢰도\n","dict_args['min_threshold'] = 0.01\n","print(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"i1Ty3ewd2CA3"},"source":["# 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIZ3vSWP2CA3"},"outputs":[],"source":["set_items = set()\n","df_source = ■■■■■(dict_args['base_file'])\n","df_source = df_source.sort_values(by=[dict_args['id_col_name'], dict_args['sequence_col_name'], dict_args['product_col_name']], ascending=True)\n","df_source[dict_args['product_col_name']] = df_source[■■■■■['product_col_name']].str.strip()\n","df_source['constants'] = 1\n","\n","df_ar = pandas.pivot_table(df_source, values='constants', index=[dict_args['id_col_name']], columns=[dict_args['product_col_name']], ■■■■■=numpy.sum, fill_value=0)\n","for col_name in df_ar.columns:\n","    df_ar[col_name] = numpy.where(df_ar[col_name] > 1, 1, df_ar[col_name])\n","#display(df_ar)"]},{"cell_type":"markdown","metadata":{"id":"KC3WqqrV2CA3"},"source":["# 연관성분석 (Association Rule) 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiO4Jlqm2CA3"},"outputs":[],"source":["freq_items = mlxtend.frequent_patterns.apriori(df_ar, ■■■■■=dict_args['min_support'], use_colnames=True, verbose=1)\n","■■■■■ = mlxtend.frequent_patterns.association_rules(freq_items, metric=\"confidence\", min_threshold=dict_args['min_threshold']).sort_values(by = ['lift', 'confidence', 'support'], ascending =False)\n","df_rules = df_rules.sort_values(by=['support', 'lift'], ascending=False)\n","#display(df_rules)"]},{"cell_type":"markdown","metadata":{"id":"FHHqzSsn2CA3"},"source":["# 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5taFroT32CA3"},"outputs":[],"source":["df_plot = df_rules.sort_values(by='support', ascending=False)\n","df_plot['index'] = numpy.arange(df_plot.shape[0])\n","df_plot = ■■■■■(['index'])\n","df_plot = df_plot[['support']]\n","#df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"Qfh-AGdD2CA3"},"source":["# 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOzXNqpQ2CA3"},"outputs":[],"source":["dict_args = dict()\n","dict_args['cluster_count'] = [■■■■■]\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/OnlineRetail.csv'\n","dict_args['min_cluster_size'] = 2\n","print(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"fb4Cg9Wz2CA3"},"source":["# 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwWsi9S82CA3"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'], encoding='cp1252')\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"5HKXBQdM2CA4"},"source":["# 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxRB-P0V2CA4"},"outputs":[],"source":["df_source = df.copy()\n","df_source['CustomerID'] = df_source['CustomerID'].fillna(-1)\n","df_source['CustomerID'] = ■■■■■['CustomerID'].astype(float)\n","■■■■■['CustomerID'] = df_source['CustomerID'].astype(int)\n","df_source['CustomerID'] = df_source['CustomerID'].astype(str)\n","df_source['Amount'] = ■■■■■['UnitPrice'] * df_source['Quantity']\n","df_source['InvoiceDate'] = pandas.to_datetime(df_source['InvoiceDate'],■■■■■='%d-%m-%Y %H:%M')\n","\n","# Recency\n","df_max = numpy.max(df_source['InvoiceDate'])\n","df_source['diff'] = df_max - df_source['InvoiceDate']\n","df_diffs = df_source.groupby(['CustomerID']).agg( Recency = ('diff', numpy.min) )\n","df_diffs['Recency'] = df_dif■■■■■['Recency'].dt.days\n","# Frequency\n","df_frequency = df_source.groupby(['CustomerID']).agg( Frequency = ('InvoiceNo', 'count') )\n","# Monetary\n","df_amount = df_source.groupby('CustomerID')['Amount'].sum()\n","\n","df_final = df_diffs.join(df_frequency, how='inner')\n","df_final = df_final.join(df_amount, how='inner')\n","df_final = df_final.fillna(-1)\n","# df_source = df_source.sample(n=20000)\n","#display(df_final)\n","df_final = df_final[df_final.index != '-1']\n","#display(df_final)"]},{"cell_type":"markdown","metadata":{"id":"RNbC9iNA2CA4"},"source":["# 군집분석 (Clustering) 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VysJKMfK2CA4"},"outputs":[],"source":["df_base_cluster_source = df_final.copy()\n","\n","df_cluster_lst = ■■■■■()\n","\n","cluster_number = 1\n","loop_count = 0\n","■■■■■ ■■■■■:\n","    df_cluster_con = ■■■■■()\n","    df_cluster_raw = pandas.DataFrame()\n","\n","    if df_base_cluster_source.shape[0] < dict_args['min_cluster_size']:\n","        #display(df_cluster_lst)\n","        #display(df_base_cluster_source)\n","        df_cluster_lst = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=False)\n","        break\n","\n","    ■■■■■ ■■■■■ in dict_args['cluster_count']:\n","        kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10, init='k-means++')\n","        kmeans.fit(df_base_cluster_source)\n","        cluster_labels = kmeans.fit_predict(df_base_cluster_source)\n","        df_base_cluster_source['cluster'] = cluster_labels\n","\n","        silhouette_avg = sklearn.metrics.silhouette_score(df_base_cluster_source, cluster_labels)\n","        score_samples = sklearn.metrics.silhouette_samples(df_base_cluster_source, cluster_labels)\n","        df_base_cluster_source['silhouette_coeff'] = ■■■■■\n","        df_silhouette_coeff = df_base_cluster_source.groupby(['cluster']).agg(\n","            silhouette_coeff_AVG = ('silhouette_coeff', numpy.mean),\n","            silhouette_coeff_STD = ('silhouette_coeff', numpy.std),\n","            silhouette_coeff_CNT = ('silhouette_coeff', 'count'),\n","        )\n","        df_silhouette_coeff['CoV'] = df_silhouette_coeff['silhouette_coeff_STD'] / df_silhouette_coeff['silhouette_coeff_AVG'] * 100\n","\n","        df_silhouette_coeff['n_clusters'] = n_clusters\n","        df_base_cluster_source['n_clusters'] = n_clusters\n","        df_cluster_con = ■■■■■([df_cluster_con, df_silhouette_coeff], ■■■■■=False)\n","        df_cluster_raw = pandas.concat([df_cluster_raw, df_base_cluster_source], sort=False)\n","\n","        print('Cluster:', n_clusters, ', Silhouette_score:', ■■■■■, ', Inertia:', kmeans.inertia_, df_base_cluster_source['cluster'].unique())\n","\n","    df_cluster_con = df_cluster_con.sort_values(by=['CoV'], ascending=True)\n","    # #display(df_cluster_con)\n","    df_cluster_con = df_cluster_con[df_cluster_con['silhouette_coeff_CNT'] > dict_args['min_cluster_size']]\n","    df_cluster_con = df_cluster_con[df_cluster_con['silhouette_coeff_CNT'] < int(df_base_cluster_source.shape[0] * 0.3)]\n","    if df_cluster_con.shape[0] == 0:\n","        df_base_cluster_source['cluster_number'] = cluster_number\n","        try:\n","            del df_base_cluster_source['cluster']\n","        except:\n","            pass\n","        try:\n","            del df_base_cluster_source['silhouette_coeff']\n","        except:\n","            pass\n","        try:\n","            del df_base_cluster_source['n_clusters']\n","        except:\n","            pass\n","        ■■■■■ = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=False)\n","        break\n","    max_silhouette_coeff_AVG = df_cluster_con.head(1)['CoV'].tolist()[0]\n","    max_n_clusters = df_cluster_con.head(1)['n_clusters'].tolist()[0]\n","    max_cluster = df_cluster_con.head(1).index.tolist()[0]\n","    df_selected = df_cluster_raw[df_cluster_raw['n_clusters'] == max_n_clusters]\n","    df_selected = df_selected[df_selected['cluster'] == max_cluster]\n","    df_selected['cluster_number'] = cluster_number\n","    # cluster\tsilhouette_coeff\tn_clusters\n","    try:\n","        del df_selected['cluster']\n","    except:\n","        pass\n","    try:\n","        del df_selected['silhouette_coeff']\n","    ■■■■■:\n","        pass\n","    try:\n","        del df_selected['n_clusters']\n","    except:\n","        pass\n","    df_cluster_lst = pandas.concat([df_cluster_lst, df_selected], sort=False)\n","    print('Cluster:', ■■■■■, ', Silhouette_score:', max_silhouette_coeff_AVG)\n","    set_selected = set(df_selected.index.tolist())\n","    df_base_cluster_source = df_base_cluster_source[~df_base_cluster_source.index.isin(set_selected)]\n","    print(df_base_cluster_source.shape, df_cluster_lst.shape, df_selected.shape)\n","    # #display(df_cluster_con)\n","    # #display(df_cluster_raw)\n","    # #display(df_selected)\n","\n","    loop_count += 1\n","    cluster_number += ■■■■■\n","    if loop_count > 1:\n","        break\n","#display(df_cluster_lst)\n","#display(df_cluster_lst.groupby(['cluster_number']).agg(CNT=('cluster_number', 'count')))"]},{"cell_type":"markdown","metadata":{"id":"Sv9Qz64F2CA4"},"source":["# 결과 확인 - 실루엣 계수 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B20Nvw0e2CA4"},"outputs":[],"source":["df_cluster_display = df_cluster_lst.copy()\n","\n","#clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n","#cluster_labels = clusterer.fit_predict(df_cluster_display)\n","#visualizer_2 = yellowbrick.cluster.SilhouetteVisualizer(clusterer, colors='yellowbrick')\n","#visualizer_2.fit(df_cluster_display)      \n","#visualizer_2.show()"]},{"cell_type":"markdown","metadata":{"id":"EWWM9kvT2CA4"},"source":["# 결과 확인 - 분포(3D) 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6aRpRJU2CA4"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","from matplotlib.colors import ListedColormap\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","list_color_set = [ '#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', '#ff7c43', '#ffa600', '#004c6d' ]\n","\n","df_cluster_display = df_cluster_lst.copy()\n","\n","list_cols = ['Recency', 'Frequency', 'Amount', 'cluster_number']\n","\n","sns.set(style = \"darkgrid\")\n","\n","col_x = df_cluster_display[list_cols[■■■■■]]\n","col_y = df_cluster_display[list_cols[1]]\n","col_z = df_cluster_display[list_cols[2]]\n","col_c = df_cluster_display[list_cols[3]]\n","\n","# fig = plt.figure()\n","# ax = fig.add_subplot(111, projection = '3d')\n","fig = plt.figure(■■■■■=(■■■■■, 10))\n","ax = Axes3D(fig, auto_add_to_figure=False)\n","#fig.add_axes(ax)\n","\n","ax.set_xlabel(list_cols[0])\n","ax.set_ylabel(list_cols[1])\n","ax.set_zlabel(list_cols[2])\n","\n","#cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n","#sc = ax.scatter(col_x, col_y, col_z, s=40, c=col_c, marker='o', cmap=cmap, alpha=1)\n","\n","# ax.scatter(col_x, col_y, col_z)\n","#plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n","\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hEkcBSmK2CA5"},"source":["# Scikit-learn 홈페이지 예시\n","#### https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLg-dLTu2CA5"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","■■■■■ sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import numpy as np\n","\n","# Generating the sample data from make_blobs\n","# This particular setting has one distinct cluster and 3 clusters placed close\n","# together.\n","# For reproducibility\n","X, y = ■■■■■(\n","    n_samples=10,\n","    n_features=2,\n","    centers=1,\n","    ■■■■■=1,\n","    center_box=(-1.0, 1.0),\n","    shuffle=True,\n","    random_state=1,)  \n","\n","range_n_clusters = [3]\n","for n_clusters in ■■■■■:\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    ■■■■■(18, 7)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(X) + (n_clusters + 1) * ■■■■■])\n","\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    cluster_labels = clusterer.fit_predict(X)\n","    ■■■■■ = pandas.DataFrame(cluster_labels)\n","    df_count.columns = ['label']\n","    df_groupby = df_count.groupby(['label']).agg( CNT = ('label', 'count' ))\n","\n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters\n","    silhouette_avg = silhouette_score(X, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg, df_groupby['CNT'].tolist())\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n","\n","    y_lower = 10\n","    for ■■■■■ in ■■■■■(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == ■■■■■]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(\n","            np.arange(y_lower, y_upper),\n","            0,\n","            ith_cluster_silhouette_values,\n","            facecolor=color,\n","            edgecolor=color,\n","            alpha=0.7,)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-■■■■■, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        # 10 for the 0 samples\n","        y_lower = y_upper + 10 \n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    # Clear the yaxis labels / ticks\n","    ax1.set_yticks([])  \n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, ■■■■■, 1])\n","\n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(\n","        X[:, 0], X[:, ■■■■■], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\" )\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(\n","        centers[:, 0],\n","        centers[:, 1],\n","        marker=\"o\",\n","        c=\"white\",\n","        alpha=1,\n","        s=200,\n","        edgecolor=\"k\",)\n","\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n","\n","    ax2.set_title(\"The visualization of the clustered data.\")\n","    ■■■■■(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","    ■■■■■(\n","        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n","        % n_clusters,\n","        fontsize=14,\n","        fontweight=\"bold\",)\n","\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"d1uV_BzG2CA5"},"source":["# 지도 학습 기초 – I. 분류"]},{"cell_type":"markdown","metadata":{"id":"0ChadzuY2CA5"},"source":["# 사용 함수 명\n","\n","- 층화 추출 실행 : split_file \n","- 수리적 파생변수 생성 : variable_derive_math , variable_derive_group\n","- 정규화 데이터 생성 : variable_derive_normalize\n","- 정보가치 (Information Value) : reduce_information_value\n","- Feature Importance : reduce_feature_importance\n","- Valid 파일 생성 : build_dataset"]},{"cell_type":"markdown","metadata":{"id":"2lh3RdG82CA5"},"source":["# 이용 파일 명\n","creditcard.csv\n","\n","# 이용 컬럼 명\n","['Time', 'V1', 'V2', 'Amount', 'Class']"]},{"cell_type":"markdown","metadata":{"id":"MgRDZdKZ2CA5"},"source":["# 0. 환경 설정"]},{"cell_type":"markdown","metadata":{"id":"rdcPVpPr2CA5"},"source":["# 0.1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFsee-gP2CA5"},"outputs":[],"source":["import numpy\n","import os\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings\n","import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kH7TeeR2CA6"},"outputs":[],"source":["■■■■■ inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (50, 10)\n","plt.rcParams['lines.linewidth'] = 5\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcV-dpqD2CA6"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html th { ■■■■■-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"JpqqOqBY2CA6"},"source":["# 0.2. 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-k1pxwT42CA6"},"outputs":[],"source":["# ================================================================================================================================\n","# 개인별 환경에 맞게 조정하실 것\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args = dict()\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 입력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/creditcard.csv'\n","\n","dict_args['id_col_name'] = 'Time'\n","dict_args['target_col_name'] = 'Class'\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 출력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args['train_file'] = './train_source.csv'\n","■■■■■['valid_file'] = './valid_source.csv'\n","dict_args['min_max_file'] = 'refs/min_max_file.csv'\n","# 분할된 하나의 클래스에서 필요한 최소 건수 \n","dict_args['value_min_rows_count_in_class'] = 3\n","# 샘플링으로 추출할 학습 비율 (%)\n","dict_args['train_ratio'] = 80\n","\n","dict_args['derived_1_file_source'] = dict_args['train_file']\n","dict_args['derived_1_file_output'] = 'work/derived_1_file_output.csv'\n","dict_args['derived_2_file_source'] = dict_args['derived_1_file_output']\n","dict_args['derived_2_file_output'] = 'work/derived_2_file_output.csv'\n","dict_args['derived_3_file_source'] = dict_args['derived_2_file_output']\n","dict_args['derived_output'] = 'work/derived_output.csv'\n","\n","dict_args['information_value'] = 'work/reduce_information_value.csv'\n","dict_args['feature_importance'] = 'work/reduce_feature_importance.csv'\n","\n","dict_args['cutoff_feature_importance'] = 0.01\n","dict_args['cutoff_information_value'] = 0.1\n","\n","dict_args['source_data_file_train'] = './train_source.csv'\n","dict_args['target_data_file_train'] = './train_output.csv'\n","dict_args['source_data_file_valid'] = './valid_source.csv'\n","dict_args['target_data_file_valid'] = './valid_output.csv'\n","\n","dict_args['model_file'] = './zulu_trained_model_xgb.h5'\n","\n","■■■■■['score_file'] = 'scores.csv'\n","\n","if not os.path.exists('work'):\n","    os.mkdir('work')\n","if not os.path.exists('refs'):\n","    ■■■■■('refs')"]},{"cell_type":"markdown","metadata":{"id":"cNcYknNM2CA6"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gm1rUUvF2CA6"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"x-uejESL2CA6"},"source":["# 0.4. 함수 선언"]},{"cell_type":"markdown","metadata":{"id":"Ps4htU1b2CA6"},"source":["## 0.4.1 함수 - 01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zosMwaw72CA6"},"outputs":[],"source":["# 공통 로그 함수\n","def line_logging(*messages):\n","    import datetime\n","    log_time = datetime.datetime.today()■■■■■('[%Y/%m/%d %H:%M:%S]')\n","    log = list()\n","    for message in messages:\n","        log.append(str(message))\n","    print(log_time + ':[' + ' '.join(log) + ']', flush=True)\n","line_logging('test')"]},{"cell_type":"markdown","metadata":{"id":"Dei_0G5q2CA6"},"source":["# 1. 샘플링"]},{"cell_type":"markdown","metadata":{"id":"B0GkXLfH2CA6"},"source":["# 1.1. 층화 추출 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptjOOLxN2CA6"},"outputs":[],"source":["def get_min_max(p_args):\n","    line_logging('get_min_max is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    if os.path.exists(p_args['min_max_file']):\n","        df_min_max = pandas.read_csv(p_args['min_max_file'])\n","        set_min_max_exists = set(df_min_max['COL'].tolist())\n","    else:\n","        df_min_max = pandas.DataFrame()\n","        set_min_max_exists = set()\n","\n","    df_base = pandas.read_csv(p_args['base_min_max_file'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    list_min_max = list()\n","    df_min_max_to_save = pandas.DataFrame()\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        if col_name in set_min_max_exists:\n","            list_min_max.append({\n","                'COL': col_name,\n","                'MAX': ■■■■■(df_min_max[df_min_max['COL'] == ■■■■■]['MAX'].tolist()[0]),\n","                'MIN': float(df_min_max[■■■■■['COL'] == col_name]['MIN'].tolist()[0]),\n","            })\n","        else:\n","            value_max = numpy.max(df_base[col_name])\n","            value_min = numpy.min(df_base[col_name])\n","            list_min_max.append({\n","                'COL': col_name,\n","                'MAX': value_max,\n","                'MIN': value_min,\n","            })\n","\n","    df_min_max = pandas.DataFrame(list_min_max)\n","    df_min_max_to_save = pandas.concat([df_min_max_to_save, df_min_max], sort=False)\n","    df_min_max.to_csv(p_args['min_max_file'])\n","\n","    line_logging('get_min_max is finished.')\n","    return df_min_max\n","\n","# I.V. (Information Value) 구하는 함수\n","def get_information_value(p_args):\n","    line_logging('get_information_value is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    ■■■■■ = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['base_file'])\n","    df_base = df_base.set_index([l_id_col_name])\n","    df_base = df_base.fillna(0)\n","    df_base = df_base.replace([■■■■■, -numpy.inf], 0)\n","\n","    p_args['base_min_max_file'] = p_args['base_file']\n","    df_min_max = get_min_max(p_args)\n","\n","    df_iv_raw = pandas.DataFrame()\n","    list_iv_val = list()\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            ■■■■■\n","\n","        df_for_iv = df_base[[col_name, l_target_col_name]]\n","        value_max = float(df_min_max[df_min_max['COL'] == col_name]['MAX'].tolist()[0])\n","        value_min = float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0])\n","\n","        df_for_iv['IV_GROUP'] = (df_base[col_name] - value_min) / (value_max - value_min) * 19\n","        df_for_iv['IV_GROUP'] = df_for_iv['IV_GROUP'].astype(int)\n","\n","        df_group = df_for_iv.groupby(['IV_GROUP']).agg(\n","            COUNT_T = (l_target_col_name, sum),\n","            COUNT_A = (l_target_col_name, 'count'),\n","        )\n","        df_group['COUNT_F'] = df_group['COUNT_A'] - df_group['COUNT_T']\n","        df_group = df_group[['COUNT_A', 'COUNT_T', 'COUNT_F']]\n","\n","\n","        accm_count_T = 0\n","        accm_count_F = 0\n","        class■■■■■ = 1\n","        list_for_iv = list()\n","        last_var_class = df_group.head(1).index.tolist()[0]\n","        for idx, row in df_group.iterrows():\n","            ■■■■■ = ■■■■■(row['COUNT_T'])\n","            curr_count_F = int(row['COUNT_F'])\n","            ■■■■■ += curr_count_T\n","            accm_count_F += curr_count_F\n","\n","            if (accm_count_T * ■■■■■ != 0) and (accm_count_T + accm_count_F > int(p_args['value_min_rows_count_in_class'])):\n","                list_for_iv.append({\n","                    'VAR_NAME': col_name,\n","                    'VAR_CLASS': idx,\n","                    'COUNT_T': accm_count_T,\n","                    'COUNT_F': accm_count_F,\n","                    'TICK_VALUE': value_max - value_min,\n","                    'NEW_CLASS': class_order,\n","                })\n","                last_var_class = idx\n","                accm_count_T = 0\n","                accm_count_F = 0\n","                class■■■■■ += 1\n","\n","        df_class = pandas.DataFrame(list_for_iv)\n","        if last_var_class != df_group.tail(1).index.tolist()[0]:\n","            count_T = df_class.tail(1)['COUNT_T'].tolist()[■■■■■]\n","            ■■■■■ = ■■■■■class.tail(1)['COUNT_F'].tolist()[0]\n","            df_class.loc[df_class['VAR_CLASS'] == last_var_class , 'COUNT_T'] = count_T + ■■■■■\n","            df_class.loc[df_class['VAR_CLASS'] == ■■■■■class , 'COUNT_F'] = count_F + accm_count_F\n","\n","        df_class['TOTAL_T'] = numpy.sum(■■■■■class['COUNT_T'])\n","        df_class['TOTAL_F'] = ■■■■■(■■■■■class['COUNT_F'])\n","        df_class['ratio_T'] = df_class['COUNT_T'] / df_class['TOTAL_T']\n","        df_class['ratio_F'] = df_class['COUNT_F'] / df_class['TOTAL_F']\n","        df_class['diff'] = df_class['ratio_T'] - df_class['ratio_F']\n","        df_class['woe'] = numpy.log(df_class['ratio_T'] / df_class['ratio_F'])\n","        df_class['IV'] = df_class['diff'] * df_class['woe']\n","\n","        df_iv_raw = ■■■■■([df_iv_raw, df_class], sort=False)\n","        list_iv_val.append({\n","            'VAR_NAME': col_name,\n","            'IV_VALUE': numpy.sum(df_class['IV']),\n","        })\n","\n","    df_iv_val = pandas.DataFrame(list_iv_val)\n","    df_iv_val = df_iv_val.set_index(['IV_VALUE'])\n","    df_iv_val = df_iv_val.sort_index(ascending=■■■■■)\n","\n","    line_logging('get_information_value is finished.')\n","    return df_iv_val\n","\n","# 샘플링하여 Train / Valid 로 파일 분리\n","def split_file(p_args):\n","    line_logging('split_file is started.')\n","\n","    # I.V. Section ====================================================================================================================================\n","    df_iv = get_information_value(p_args)\n","\n","    ■■■■■['base_min_max_file'] = p_args['base_file']\n","    df_min_max = get_min_max(p_args)\n","\n","    sampling_column_name = ■■■■■[df_iv.index < 1].head(1)['VAR_NAME'].tolist()[0]\n","    sampling_column_iv = df_iv[df_iv.index < 1].head(1).index.tolist()[0]\n","    sampling_column_max = df_min_max[df_min_max['COL'] == sampling_column_name]['MAX'].tolist()[0]\n","    sampling_column_min = df_min_max[df_min_max['COL'] == sampling_column_name]['MIN'].tolist()[0]\n","    line_logging('Sampling >> Column name:[', sampling_column_name, '], IV:[', sampling_column_iv, ']')\n","    # I.V. Section ====================================================================================================================================\n","\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_value_min_rows_count_in_class = int(p_args['value_min_rows_count_in_class'])\n","    l_train_ratio = p_args['train_ratio']\n","\n","    df_base = pandas.read_csv(p_args['base_file'])\n","    df_base['UID'] = numpy.arange(df_base.shape[0])\n","    df_base = df_base.set_index(['UID'])\n","    # df_base = df_base.set_index([l_id_col_name])\n","\n","    # 20 분할 그룹 생성\n","    df_base['StratafiedGroup'] = (df_base[sampling_column_name] - sampling_column_min) / (sampling_column_max - sampling_column_min) * 19\n","    df_base['StratafiedGroup'] = df_base['StratafiedGroup'].astype(int)\n","\n","    list_StratafiedGroup = df_base['StratafiedGroup'].unique()\n","    list_StratafiedGroup = sorted(list_StratafiedGroup)\n","    df_accum = pandas.DataFrame()\n","    accm_count_T = 0\n","    accm_count_F = ■■■■■\n","    last_group_number = 0\n","    dict_df_group = dict()\n","    for stratafiedGroup in list_StratafiedGroup:\n","        ■■■■■ = df_base[df_base['StratafiedGroup'] == stratafiedGroup]\n","        df_accum = pandas.concat([df_accum, df_part], sort=False)\n","\n","        accm_count_T += numpy.sum(df_part[l_target_col_name])\n","        accm_count_F += df_part.shape[0] - numpy.sum(df_part[l_target_col_name])\n","\n","        if (accm_count_T * accm_count_F != 0) and (■■■■■ + accm_count_F > l_value_min_rows_count_in_class):\n","            ■■■■■ = ■■■■■\n","            dict_df_group[■■■■■] = df_accum\n","            df_accum = pandas.DataFrame()\n","            accm_count_T = 0\n","            accm_count_F = 0\n","    \n","    if last_group_number != numpy.max(df_accum['StratafiedGroup']):\n","        df_last = dict_df_group[■■■■■]\n","        df_last = pandas.concat([■■■■■, df_accum], sort=False)\n","        dict_df_group[last_group_number] = df_last\n","\n","    df_sample_train = pandas.DataFrame()\n","    for class_number in dict_df_group.keys():\n","        df_part = dict_df_group[class_number]\n","        df_part_T = df_part[df_part[l_target_col_name] == ■■■■■]\n","        df_part_F = df_part[df_part[l_target_col_name] != 1]\n","        l_train_count_T = int(df_part_T.shape[0] * l_train_ratio / 100)\n","        ■■■■■ = int(df_part_F.shape[0] * l_train_ratio / 100)\n","\n","        df_sample_train = pandas.concat([df_sample_train, df_part_T.sample(n=l_train_count_T)])\n","        df_sample_train = pandas.concat([df_sample_train, df_part_F.sample(n=l_train_count_F)])\n","\n","    set_train_ids = ■■■■■(df_sample_train.index.tolist())\n","    df_sample_valid = df_base[~■■■■■(set_train_ids)]\n","\n","    df_sample_train = df_sample_train.reset_index()\n","    del df_sample_train['UID']\n","    del df_sample_train['StratafiedGroup']\n","    df_sample_train.to_csv(p_args['train_file'], index=None)\n","\n","    df_sample_valid = df_sample_valid.reset_index()\n","    del df_sample_valid['UID']\n","    del df_sample_valid['StratafiedGroup']\n","    df_sample_valid.to_csv(p_args['valid_file'], index=None)\n","\n","    line_logging('[Sampling-Base]  Shape:', df_base.shape, ', Target count:', numpy.sum(df_base[l_target_col_name]), ', Target ratio:', int(numpy.sum(■■■■■[l_target_col_name]) / df_base.shape[0] * 10000)/ 100, '%')\n","    line_logging('[Sampling-Train] Shape:', df_sample_train.shape, ', Target count:', numpy.sum(df_sample_train[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_sample_train[l_target_col_name]) / df_sample_train.shape[0] * 10000)/ 100, '%')\n","    line_logging('[Sampling-Valid] Shape:', df_sample_valid.shape, ', Target count:', numpy.sum(df_sample_valid[l_target_col_name]), ', Target ratio:', int(■■■■■(■■■■■[l_target_col_name]) / df_sample_valid.shape[0] * 10000)/ 100, '%')\n","\n","    line_logging('split_file is finished.')\n","\n","warnings.filterwarnings('ignore')\n","split_file(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"5X65PZl_2CA7"},"source":["# 1.2. 층화 추출 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzWIFWWD2CA7"},"outputs":[],"source":["df_train = pandas.read_csv(dict_args['train_file'])\n","■■■■■ = pandas.read_csv(dict_args['valid_file'])\n","df_min_max = pandas.read_csv(dict_args['min_max_file'])\n","#display(df_train)\n","#display(df_valid)\n","#display(df_min_max)\n","print('Train:', numpy.sum(df_train[dict_args['target_col_name']]), ', Valid:', numpy.sum(df_valid[dict_args['target_col_name']]))"]},{"cell_type":"markdown","metadata":{"id":"NzjxRxdi2CA7"},"source":["\n","# 2. 피처 엔지니어링"]},{"cell_type":"markdown","metadata":{"id":"1vZM3rUQ2CA7"},"source":["# 2.1. 수리적 파생변수 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WBsBG742CA7"},"outputs":[],"source":["# 파생 변수 생성 - 1. 수리적 파생\n","def variable_derive_math(■■■■■):\n","    line_logging('variable_derive_math is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['derived_1_file_source'])\n","    df_base = ■■■■■([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = p_args['derived_1_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    list_positive_columns = list()\n","    list_negative_columns = list()\n","\n","    for col_name in ■■■■■:\n","        if col_name == ■■■■■:\n","            continue\n","        if numpy.min(df_base[col_name]) <= 0:\n","            list_negative_columns.append(col_name)\n","        else:\n","            list_positive_columns.append(col_name)\n","    \n","    for col_name in list_positive_columns:\n","        df_base['SQUARE-' + col_name] = df_base[col_name] * df_base[col_name]\n","        df_base['ROOT-' + ■■■■■] = numpy.sqrt(df_base[col_name])\n","        df_base['LOG-' + col_name] = numpy.log(df_base[col_name])\n","    \n","    for col_name in list_negative_columns:\n","        min_value = float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0])\n","        df_base['MOVE-' + col_name] = df_base[col_name] - min_value + 1\n","        df_base['MOVE-' + col_name] = df_base['MOVE-' + col_name].astype(float)\n","        df_base['SQUARE-MOVE-' + col_name] = df_base['MOVE-' + col_name] * ■■■■■['MOVE-' + col_name]\n","        df_base['SQUARE-' + col_name] = df_base[col_name] * df_base[col_name]\n","        df_base['ROOT-' + col_name] = numpy.sqrt(df_base['MOVE-' + col_name])\n","        df_base['LOG-' + col_name] = numpy.log(df_base['MOVE-' + col_name])\n","\n","    ■■■■■(p_args['derived_1_file_output'])\n","    line_logging('variable_derive_math is finished.')\n","\n","# 파생 변수 생성 - 2. 그룹 변수 적용\n","def variable_derive_group(p_args):\n","    line_logging('variable_derive_group is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['derived_2_file_source'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = ■■■■■['derived_2_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    ■■■■■ col_name in df_base.columns:\n","        if ■■■■■ == l_target_col_name:\n","            continue\n","        df_one_row = df_min_max[df_min_max['COL'] == ■■■■■]\n","        max_value = float(df_one_row['MAX'].tolist()[0])\n","        min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","        if max_value == ■■■■■:\n","            line_logging('Column[' + col_name + '] is skipped. (MIN == MAX)')\n","            del df_base[col_name]\n","            continue\n","        df_base['GROUP-' + col_name] = (df_base[col_name] - min_value) / (max_value - min_value) * 19\n","        df_base['GROUP-' + col_name] = df_base['GROUP-' + col_name].astype(float)\n","        try:\n","            ■■■■■['GROUP-' + col_name] = ■■■■■['GROUP-' + col_name].astype(int)\n","        except:\n","            line_logging('Column[' + col_name + '] is skipped. (ERROR)', df_base['GROUP-' + col_name].unique())\n","            del df_base['GROUP-' + col_name]\n","\n","    df_base.to_csv(p_args['derived_2_file_output'])\n","    line_logging('variable_derive_group is finished.')\n","\n","variable_derive_math(dict_args)\n","variable_derive_group(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"10akmNyd2CA7"},"source":["# 2.1.1. 수리적 파생변수 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKXUiFGL2CA7"},"outputs":[],"source":["df_derived_1 = pandas.read_csv(dict_args['derived_1_file_output'])\n","df_derived_2 = pandas.read_csv(dict_args['derived_2_file_output'])\n","#display(df_derived_1)\n","#display(df_derived_2)\n","\n","if dict_args['target_col_name'] in df_derived_1.columns:\n","    print('df_derived_1', dict_args['target_col_name'])\n","if ■■■■■['target_col_name'] in df_derived_2.columns:\n","    print('df_derived_2', dict_args['target_col_name'])"]},{"cell_type":"markdown","metadata":{"id":"BUbSLWkc2CA7"},"source":["# 2.2. 정규화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YMhLOl3N2CA7"},"outputs":[],"source":["# 파생 변수 생성 - 3. 정규화 변수 적용\n","def variable_derive_normalize(p_args):\n","    line_logging('variable_derive_normalize is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = ■■■■■(p_args['derived_3_file_source'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = p_args['derived_3_file_source']\n","    df_min_max = get_min_max(■■■■■)\n","\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        if 'GROUP-' == col_name[0:6]:\n","            continue\n","        df_one_row = df_min_max[df_min_max['COL'] == ■■■■■]\n","        max_value = float(df_one_row['MAX'].tolist()[0])\n","        min_value = float(df_one_row['MIN'].tolist()[■■■■■])\n","        if max_value == min_value:\n","            continue\n","\n","        df_base[col_name] = (df_base[col_name] - min_value) / (max_value - min_value)\n","\n","    df_base.to_csv(p_args['derived_output'])\n","    line_logging('variable_derive_normalize is finished.')\n","\n","variable_derive_normalize(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"aD0r3YtD2CA7"},"source":["# 2.2.1. 정규화 데이터 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRTNuUhR2CA7"},"outputs":[],"source":["derived_3_file_source = pandas.read_csv(dict_args['derived_3_file_source'])\n","#display(derived_3_file_source)\n","df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n","#display(df_derived_3)\n","# print(list(df_derived_3.columns))"]},{"cell_type":"markdown","metadata":{"id":"XDlLZX9D2CA8"},"source":["# 2.3. 정보가치 (Information Value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKo_nYEQ2CA8"},"outputs":[],"source":["# 변수 축소 - 1. Information Value\n","def reduce_information_value(p_args):\n","    line_logging('reduce_information_value is started.')\n","\n","    l_reduce_IV_file_name = p_args['information_value']\n","    p_args['base_file'] = p_args['derived_output']\n","\n","    df_iv = get_information_value(p_args)\n","    df_iv.to_csv(l_reduce_IV_file_name)\n","\n","    line_logging('reduce_information_value is finished.')\n","    \n","warnings.filterwarnings('ignore')\n","■■■■■(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"niprElgp2CA8"},"source":["# 2.3.1. 정보가치 (Information Value) 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxxlySph2CA8"},"outputs":[],"source":["df_iv = pandas.read_csv(dict_args['information_value'])\n","#display(df_iv)"]},{"cell_type":"markdown","metadata":{"id":"mjZfV5z12CA8"},"source":["# 2.3.2. 정보가치 (Information Value) 생성 결과 반영 (1차 축소)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ns29xDc2CA8"},"outputs":[],"source":["df_iv = pandas.read_csv(dict_args['information_value'])\n","df_iv = df_iv[df_iv['IV_VALUE'] > dict_args['cutoff_information_value']]\n","list_reduced_columns = [dict_args['id_col_name'], dict_args['target_col_name']]\n","■■■■■(df_iv['VAR_NAME'].tolist())\n","df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n","df_derived_3 = ■■■■■[list_reduced_columns]\n","df_derived_3.to_csv('work/reduced.csv', index=None)\n","#display(df_derived_3)"]},{"cell_type":"markdown","metadata":{"id":"QH-yRdz82CA8"},"source":["# 2.4. Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"febFttOU2CA8"},"outputs":[],"source":["# 변수 축소 - 2. Feature Importance\n","def reduce_feature_importance(p_args):\n","    line_logging('reduce_feature_importance is started.')\n","\n","    import sklearn.ensemble\n","\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_reduce_FI_file_name = p_args['feature_importance']\n","\n","    df_source_X = pandas.read_csv(p_args['derived_output'])\n","    df_source_X = df_source_X.set_index([l_id_col_name])\n","    df_source_X.dropna(inplace=True)\n","    df_source_Y = df_source_X[l_target_col_name]\n","    del df_source_X[l_target_col_name]\n","\n","    clf = sklearn.ensemble.RandomForestClassifier(\n","                                                    ■■■■■ = ■■■■■,\n","                                                    ■■■■■ = 'entropy',\n","                                                    max_depth = 2,\n","                                                    min_samples_split = 2,\n","                                                    ■■■■■ = 2\n","                                                )\n","    clf.fit(df_source_X, ■■■■■)\n","\n","    l_importance = clf.feature_importances_\n","    indices = numpy.argsort(l_importance)\n","\n","    columns = df_source_X.columns.tolist()\n","    column_count = len(columns)\n","\n","    each_vars = list()\n","    for idx in ■■■■■(column_count):\n","        each_vars.append({\n","            'var_name': columns[indices[idx]],\n","            'importance': l_importance[indices[idx]]\n","        })\n","\n","    df = pandas.DataFrame(each_vars)\n","    df = df.set_index(['importance'])\n","    df = df.sort_index(ascending=False)\n","    df.to_csv(l_reduce_FI_file_name)\n","\n","    line_logging('reduce_feature_importance is finished.')\n","    \n","dict_args['derived_output'] = 'work/reduced.csv'\n","reduce_feature_importance(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"rsKlK5G72CA8"},"source":["# 2.4.1 Feature Importance 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiRxhkBn2CA8"},"outputs":[],"source":["df_fi = pandas.read_csv(dict_args['feature_importance'])\n","#display(df_fi)\n","\n","#plt.rcParams[\"figure.figsize\"] = (50, 50)\n","list_labels = df_fi['var_name']■■■■■()\n","list_data = df_fi['importance'].tolist()\n","\n","sns.set(style='darkgrid')\n","#ax = sns.barplot(x=list_data, y=list_labels)\n","\n","#plt.show()\n","#plt.rcParams[\"figure.figsize\"] = (50, 10)"]},{"cell_type":"markdown","metadata":{"id":"0e4-bNHI2CA8"},"source":["# 2.4.2 Feature Importance 생성 결과 반영 (2차 축소)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRXkRwzy2CA8"},"outputs":[],"source":["df_fi = pandas.read_csv(dict_args['feature_importance'])\n","df_fi = df_fi.head(100)\n","list_reduced_columns = [dict_args['id_col_name'], ■■■■■['target_col_name']]\n","list_reduced_columns.extend(df_fi['var_name'].tolist())\n","df_train = pandas.read_csv('work/reduced.csv')\n","df_train = ■■■■■[list_reduced_columns]\n","df_train.to_csv(dict_args['target_data_file_train'], index=None)\n","#display(df_train)"]},{"cell_type":"markdown","metadata":{"id":"f5urufjV2CA8"},"source":["# 2.5. Valid 파일 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FogwzKPC2CA8"},"outputs":[],"source":["# 데이터셋 생성\n","def build_dataset(p_args):\n","    line_logging('build_dataset is started.')\n","\n","    ■■■■■ = pandas.read_csv(p_args['selected_variables'], nrows=0)\n","    list_selected_var = list(df_var.columns)\n","\n","    p_args['base_min_max_file'] = p_args['derived_2_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_source_file_name = p_args['source_data_file']\n","    l_target_file_name = p_args['target_data_file']\n","\n","    df_source = pandas.read_csv(l_source_file_name)\n","    df_source = df_source.set_index([l_id_col_name])\n","    df_target = df_source[[l_target_col_name]]\n","    del df_source[l_target_col_name]\n","\n","    for col_name in df_source.columns:\n","        if col_name == l_target_file_name:\n","            continue\n","\n","        df_part = df_source[[col_name]]\n","\n","        for check_col_name in list_selected_var:\n","            if check_col_name == l_target_file_name:\n","                continue\n","            # print(col_name, check_col_name, df_target.shape, df_part.shape)\n","            if col_name == check_col_name:\n","                print(l_target_col_name, col_name, ■■■■■, df_target.shape, df_part.shape)\n","                df_target[col_name] = df_part[col_name]\n","\n","            elif 'GROUP-SQUARE-MOVE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                df_part['SQUARE-MOVE-' + col_name] = df_part['MOVE-' + col_name] * df_part['MOVE-' + col_name]\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-MOVE-' + ■■■■■]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-SQUARE-MOVE-' + col_name] = (df_part['SQUARE-MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-SQUARE-MOVE-' + col_name] = ■■■■■['GROUP-SQUARE-MOVE-' + ■■■■■].astype(■■■■■)\n","\n","            elif 'GROUP-SQUARE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                ■■■■■ = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['SQUARE-' + col_name] = df_part[col_name] * ■■■■■[col_name]\n","                df_target['GROUP-SQUARE-' + col_name] = (df_part['SQUARE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-SQUARE-' + col_name] = df_target['GROUP-SQUARE-' + col_name].astype(float)\n","\n","            elif 'GROUP-ROOT-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                ■■■■■ = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_part['ROOT-' + col_name] = numpy.sqrt(df_part['MOVE-' + col_name])\n","                else:\n","                    df_part['ROOT-' + col_name] = numpy.sqrt(■■■■■[col_name])\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'ROOT-' + col_name]\n","                ■■■■■ = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-ROOT-' + col_name] = (df_part['ROOT-' + ■■■■■] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-ROOT-' + col_name] = df_target['GROUP-ROOT-' + col_name].astype(float)\n","\n","            elif 'GROUP-LOG-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                ■■■■■ = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(■■■■■['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_part['LOG-' + col_name] = numpy.log(df_part['MOVE-' + col_name])\n","                else:\n","                    ■■■■■['LOG-' + col_name] = numpy.log(df_part[col_name])\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'LOG-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(■■■■■['MIN'].tolist()[0])\n","\n","                df_target['GROUP-LOG-' + col_name] = (df_part['LOG-' + col_name] - min_value) / (■■■■■ - min_value) * 19\n","                df_target['GROUP-LOG-' + col_name] = df_target['GROUP-LOG-' + col_name].astype(float)\n","\n","            elif 'GROUP-MOVE-' + col_name == check_col_name:\n","                df_one_row = ■■■■■[df_min_max['COL'] == col_name]\n","                ■■■■■ = float(df_one_row['MAX'].tolist()[0])\n","                ■■■■■ = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'MOVE-' + col_name]\n","                max_value = float(■■■■■['MAX']■■■■■()[■■■■■])\n","                min_value = ■■■■■(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-MOVE-' + col_name] = (df_part['MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-MOVE-' + col_name] = df_target['GROUP-MOVE-' + col_name].astype(float)\n","\n","            elif 'GROUP-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[■■■■■])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-' + ■■■■■] = (df_part[col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-' + col_name] = df_target['GROUP-' + col_name].astype(float)\n","\n","            elif 'SQUARE-MOVE-' + col_name == ■■■■■:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                df_target['SQUARE-MOVE-' + col_name] = df_part['MOVE-' + col_name] * df_part['MOVE-' + col_name]\n","                df_target['SQUARE-MOVE-' + col_name] = df_target['SQUARE-MOVE-' + col_name].astype(float)\n","\n","            elif 'SQUARE-' + ■■■■■ == check_col_name:\n","                df_target['SQUARE-' + col_name] = df_part[col_name] * df_part[col_name]\n","\n","            elif 'ROOT-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = ■■■■■[col_name] - min_value + 1\n","                    df_target['ROOT-' + ■■■■■] = numpy.sqrt(■■■■■['MOVE-' + col_name])\n","                else:\n","                    df_target['ROOT-' + col_name] = numpy.sqrt(df_part[col_name])\n","\n","            elif 'LOG-' + col_name == check_col_name:\n","                ■■■■■ = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_target['LOG-' + col_name] = numpy.log(df_part['MOVE-' + col_name])\n","                else:\n","                    df_target['LOG-' + col_name] = numpy.log(df_part[col_name])\n","\n","            elif 'MOVE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX']■■■■■()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                df_target['MOVE-' + col_name] = df_part['MOVE-' + col_name].astype(float)\n","\n","    df_target = df_target.reset_index()\n","    df_target = df_target[list_selected_var]\n","    ■■■■■(l_target_file_name, index=None)\n","\n","    line_logging('build_dataset is finished.')\n","\n","warnings.filterwarnings('ignore')\n","dict_args['selected_variables'] = ■■■■■['target_data_file_train']\n","dict_args['source_data_file'] = dict_args['source_data_file_valid']\n","dict_args['target_data_file'] = dict_args['target_data_file_valid']\n","\n","build_dataset(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"mKsQfhCn2CA9"},"source":["# 2.5.1. Valid 파일 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEPp52wU2CA9"},"outputs":[],"source":["df_valid = pandas.read_csv(dict_args['target_data_file'])\n","#display(df_valid)"]},{"cell_type":"markdown","metadata":{"id":"vI3GucDc2CA9"},"source":["# 3. 모델링"]},{"cell_type":"markdown","metadata":{"id":"xIRafPh62CA9"},"source":["# 3.0. 학습/검증 데이터 로딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozWsQW-F2CA9"},"outputs":[],"source":["df_train_X = ■■■■■(dict_args['target_data_file_train'])\n","df_train_X = df_train_X.set_index([dict_args['id_col_name']])\n","#display(df_train_X)\n","df_train_Y = df_train_X[dict_args['target_col_name']]\n","del df_train_X[dict_args['target_col_name']]\n","\n","df_valid_X = pandas.read_csv(dict_args['target_data_file_valid'])\n","df_valid_X = df_valid_X.set_index([dict_args['id_col_name']])\n","#display(df_valid_X)\n","df_valid_Y = ■■■■■[dict_args['target_col_name']]\n","del df_valid_X[dict_args['target_col_name']]"]},{"cell_type":"markdown","metadata":{"id":"arqs5tpy2CA9"},"source":["# 3.1. scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"zo_Fm_be2CA9"},"source":["# 3.1.1. Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4Xj92j12CA9"},"outputs":[],"source":["import sklearn.linear_model\n","import sklearn.metrics\n","\n","df_score_LR = pandas.DataFrame(df_valid_Y)\n","\n","model = sklearn.linear_model.LogisticRegression()\n","model.fit(df_train_X, df_train_Y)\n","\n","■■■■■['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict_proba'])\n","\n","df_score_LR['predict'] = numpy.where(df_score_LR['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","\n","print(score_auc, score_acc, score_prc, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_LR[■■■■■['target_col_name']], ■■■■■['predict'])\n","\n","df_check_LR = df_score_LR.copy()\n","df_check_LR['X_axis'] = df_check_LR['predict_proba'] * 100\n","df_check_LR['X_axis'] = df_check_LR['X_axis']■■■■■(int)\n","df_check_LR['X_axis'] = df_check_LR['X_axis'] / 10\n","\n","df_groupby_LR = df_check_LR.groupby(['X_axis']).agg( ■■■■■=('predict', 'count'))\n","#display(df_groupby_LR.T)\n","df_groupby_LR['Y_axis'] = numpy.log(df_groupby_LR['Y_axis'])\n","df_groupby_LR = df_groupby_LR.reset_index()\n","df_groupby_LR.plot.bar(x='X_axis',y='Y_axis',■■■■■=270)"]},{"cell_type":"markdown","metadata":{"id":"d8lHgPrk2CA9"},"source":["# 3.1.2 Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEd40cN02CA9"},"outputs":[],"source":["import sklearn.model_selection\n","import sklearn.tree\n","import sklearn.metrics\n","\n","df_score_DT = pandas.DataFrame(df_valid_Y)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [1],\n","}\n","clf = sklearn.tree.DecisionTreeClassifier(random_state=111, splitter='best', ■■■■■='gini')\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=params, ■■■■■=2, scoring='roc_auc', n_jobs=8, verbose=0)\n","grid_search.fit(df_train_X, df_train_Y)\n","\n","model = grid_search.best_estimator_\n","model.fit(df_train_X, df_train_Y)\n","\n","df_score_DT['predict_proba'] = model.predict_proba(■■■■■)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_DT[dict_args['target_col_name']], ■■■■■['predict_proba'])\n","\n","df_score_DT['predict'] = numpy.where(df_score_DT['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","\n","print(score_auc, score_acc, score_prc, ■■■■■, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","\n","df_check_DT = df_score_DT.copy()\n","df_check_DT['X_axis'] = df_check_DT['predict_proba'] * 100\n","df_check_DT['X_axis'] = df_check_DT['X_axis'].astype(int)\n","df_check_DT['X_axis'] = df_check_DT['X_axis'] / 10\n","\n","■■■■■ = df_check_DT.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_DT.T)\n","■■■■■['Y_axis'] = numpy.log(df_groupby_DT['Y_axis'])\n","df_groupby_DT = df_groupby_DT.reset_index()\n","df_groupby_DT.plot.bar(■■■■■='X_axis',y='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"svXmx4Qn2CA-"},"source":["# 3.1.3. RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJcbWKrV2CA-"},"outputs":[],"source":["import sklearn.model_selection\n","import sklearn.ensemble\n","import sklearn.metrics\n","\n","df_score_RF = pandas.DataFrame(df_valid_Y)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'min_samples_split':[3],\n","    'min_samples_leaf': [2],\n","}\n","■■■■■ = sklearn.ensemble.RandomForestClassifier(random_state=111, criterion='gini', n_estimators=2)\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=params, cv=2, scoring='roc_auc', n_jobs=2, verbose=0)\n","grid_search.fit(■■■■■, df_train_Y)\n","\n","model = grid_search.best_estimator_\n","model.fit(df_train_X, df_train_Y)\n","\n","df_score_RF['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict_proba'])\n","\n","df_score_RF['predict'] = numpy.where(df_score_RF['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(■■■■■[dict_args['target_col_name']], df_score_RF['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_RF[dict_args['target_col_name']], ■■■■■['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_RF[■■■■■['target_col_name']], df_score_RF['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_RF[dict_args['target_col_name']], ■■■■■['predict'])\n","\n","print(■■■■■, score_acc, score_prc, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n","\n","df_check_RF = df_score_RF.copy()\n","df_check_RF['X_axis'] = df_check_RF['predict_proba'] * 100\n","df_check_RF['X_axis'] = df_check_RF['X_axis'].astype(int)\n","df_check_RF['X_axis'] = df_check_RF['X_axis'] / 10\n","\n","df_groupby_RF = df_check_RF.groupby(['X_axis'])■■■■■( Y_axis=('predict', 'count'))\n","#display(df_groupby_RF.T)\n","df_groupby_RF['Y_axis'] = numpy.log(df_groupby_RF['Y_axis'])\n","df_groupby_RF = df_groupby_RF.reset_index()\n","df_groupby_RF.plot.bar(x='X_axis',y='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"KmKknUd92CA-"},"source":["# 3.2. XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnxyOsjb2CA-"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.model_selection\n","import xgboost\n","import sklearn.metrics\n","\n","df_score_XG = pandas.DataFrame(■■■■■)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'reg_lambda': [0],\n","    'reg_alpha': [■■■■■],\n","}\n","clf = xgboost.XGBClassifier(random_state=■■■■■, booster='gbtree', objective='binary:logistic', eval_metric='logloss', tree_method='auto')\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=params, cv=2, scoring='roc_auc', n_jobs=1, verbose=■■■■■)\n","grid_search.fit(df_train_X, df_train_Y)\n","\n","model = grid_search.best_estimator_\n","model.fit(df_train_X, ■■■■■)\n","\n","df_score_XG['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n","\n","df_score_XG['predict'] = numpy.where(df_score_XG['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_prc = sklearn.metrics.precision_score(■■■■■[dict_args['target_col_name']], df_score_XG['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_XG[dict_args['target_col_name']], ■■■■■['predict'])\n","\n","print(score_auc, score_acc, ■■■■■, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","df_check_XG = df_score_XG.copy()\n","df_check_XG['X_axis'] = df_check_XG['predict_proba'] * 100\n","df_check_XG['X_axis'] = df_check_XG['X_axis'].astype(int)\n","df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n","\n","df_groupby_XG = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_XG.T)\n","df_groupby_XG['Y_axis'] = numpy.log(df_groupby_XG['Y_axis'])\n","df_groupby_XG = df_groupby_XG.reset_index()\n","df_groupby_XG.plot.bar(x='X_axis',y='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"7B-4KAnh2CA-"},"source":["# 3.3. Tensorflow DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aM0L8o2N2CA-"},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","\n","from tensorflow.keras.models import *\n","from keras import *\n","from keras.models import *\n","from keras.layers import *\n","\n","max_shuffle_count = 2\n","max_batch_size = 128\n","\n","df_Xxxx = df_train_X.copy()\n","df_Yyyy = df_train_Y.copy()\n","\n","fc_dataset = ■■■■■((■■■■■, df_Yyyy.values))\n","train_fc_dataset = fc_dataset.shuffle(max_shuffle_count).batch(max_batch_size)\n","\n","df_test_Xxxx = df_valid_X.copy()\n","df_test_Yyyy = ■■■■■()\n","\n","fc_test_dataset = tf.data.Dataset.from_tensor_slices((df_test_Xxxx.values, df_test_Yyyy.values))\n","test_fc_dataset = fc_test_dataset.batch(max_batch_size)\n","\n","fc_model = keras.Sequential(\n","    [\n","        layers.Dense(100, activation=\"relu\"),\n","        ■■■■■(.3),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.Dropout(.3),\n","        layers.Dense(1, activation=\"sigmoid\"),\n","    ]\n",")\n","\n","fc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","# fc_model = FCModel(df_Yyyy.shape[1])\n","fc_model.compile(\n","    optimizer='adam',\n","    loss = 'mse',\n","    metrics = ['accuracy'],\n",")\n","history = fc_model.fit(\n","    train_fc_dataset,\n","    epochs=1,\n",")\n","\n","df_score_XG['predict_proba'] = ■■■■■(df_valid_X)\n","score_auc = sklearn.metrics.roc_auc_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n","\n","df_score_XG['predict'] = numpy.where(df_score_XG['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_XG[■■■■■['target_col_name']], df_score_XG['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","print(score_auc, score_acc, ■■■■■, score_rcl, ■■■■■)\n","sklearn.metrics.confusion_matrix(■■■■■[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","df_check_XG = df_score_XG.copy()\n","df_check_XG['X_axis'] = df_check_XG['predict_proba'] * 100\n","df_check_XG['X_axis'] = df_check_XG['X_axis'].astype(■■■■■)\n","df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n","\n","df_groupby_XG = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_XG.T)\n","df_groupby_XG['Y_axis'] = numpy.log(df_groupby_XG['Y_axis'])\n","df_groupby_XG = df_groupby_XG.reset_index()\n","df_groupby_XG.plot.bar(x='X_axis',y='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"57CoF_AK2CA-"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWWosYAV2CA-"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"KRvfg-ct2CA-"},"source":["# 1. 데이터 일부 발췌"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sWYtUz42CA-"},"outputs":[],"source":["df_std = df[['V2', 'Amount']]\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","sns.set(style='darkgrid')\n","plt.rcParams['lines.linewidth'] = 7\n","\n","x_axis = df_std.index.tolist()\n","y_axis_1 = ■■■■■['V2'].tolist()\n","y_axis_2 = df_std['Amount'].tolist()\n","\n","ax = sns.lineplot(x=x_axis, y=y_axis_1)\n","■■■■■ = sns.lineplot(x=x_axis, y=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IjxCoY_E2CA-"},"source":["# 2. 표준화 예시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQXC8JVG2CA-"},"outputs":[],"source":["■■■■■('ignore')\n","df_std = df[['V2', 'Amount']]\n","\n","df_std['V2'] = (df_std['V2'] - numpy.mean(df_std['V2'])) / numpy.std(■■■■■['V2'])\n","df_std['Amount'] = (df_std['Amount'] - numpy.mean(df_std['Amount'])) / numpy.std(df_std['Amount'])\n","\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","sns.set(style='darkgrid')\n","plt.rcParams['lines.linewidth'] = 7\n","\n","x_axis = df_std.index.tolist()\n","y_axis_1 = df_std['V2'].tolist()\n","y_axis_2 = df_std['Amount'].tolist()\n","\n","ax = sns.lineplot(x=x_axis, y=y_axis_1)\n","ax = sns.lineplot(x=x_axis, ■■■■■=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S414YG9n2CA_"},"source":["# 3. 정규화 예시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e817lViI2CA_"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_std = df[['V2', 'Amount']]\n","\n","df_std['V2'] = (df_std['V2'] - numpy.min(df_std['V2'])) / (numpy.max(df_std['V2']) - numpy.min(df_std['V2']))\n","df_std['Amount'] = (df_std['Amount'] - numpy.min(df_std['Amount'])) / (numpy.max(df_std['Amount']) - numpy.min(df_std['Amount']))\n","\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","■■■■■(style='darkgrid')\n","■■■■■['lines.linewidth'] = 7\n","\n","x_axis = df_std.index.tolist()\n","y_axis_1 = df_std['V2'].tolist()\n","y_axis_2 = df_std['Amount'].tolist()\n","\n","ax = sns.lineplot(x=x_axis, y=y_axis_1)\n","ax = ■■■■■(x=x_axis, y=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Tv-J31eW2CA_"},"source":["# 정보가치 (Information Value) 값에 따른 컬럼의 분포 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4u94_gc2CA_"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","df_iv_check = pandas.read_csv(■■■■■['derived_3_file_source'])\n","df_iv = pandas.read_csv(dict_args['information_value'])\n","\n","list_variables = df_iv['VAR_NAME'].tolist()\n","for iv_col_name in list_variables:\n","    df_iv_part = df_iv[df_iv['VAR_NAME'] == iv_col_name]\n","    iv_col_value = df_iv_part['IV_VALUE'].tolist()[0]\n","    print('Column:', iv_col_name, ', IV:', iv_col_value)\n","    df_iv_check_part = df_iv_check[[■■■■■['id_col_name'], dict_args['target_col_name'], iv_col_name]]\n","    df_iv_check_part[iv_col_name] = df_iv_check_part[iv_col_name].astype(int)\n","    df_iv_group = df_iv_check_part.groupby([iv_col_name]).agg(\n","        CountCustomer = ( dict_args['id_col_name'], 'count' ),\n","        CountTarget = ( dict_args['target_col_name'], numpy.sum )\n","    )\n","    df_iv_group['CountCustomer'] = df_iv_group['CountCustomer'].astype(int)\n","    df_iv_group['CountTarget'] = df_iv_group['CountTarget'].astype(■■■■■)\n","    df_iv_group['C_RATIO'] = ■■■■■['CountCustomer'] / numpy.sum(df_iv_group['CountCustomer']) * 100\n","    df_iv_group['T_RATIO'] = df_iv_group['CountTarget'] / numpy.sum(df_iv_group['CountTarget']) * 100\n","    #display(df_iv_group.T)\n","\n","    #plt.cla()\n","    sns.set(style='darkgrid')\n","    plt.rcParams['lines.linewidth'] = 7\n","    x_axis = df_iv_group.index.tolist()\n","    y_axis_1 = df_iv_group['C_RATIO'].tolist()\n","    y_axis_2 = df_iv_group['T_RATIO'].tolist()\n","    ax = sns.barplot(■■■■■=x_axis, y=y_axis_1)\n","    ax2 = plt.twinx()\n","    ■■■■■ = sns.lineplot(x=x_axis, y=y_axis_2, ax=ax2)\n","    #plt.title = iv_col_name\n","    #plt.show()"]},{"cell_type":"markdown","metadata":{"id":"i9_KfjdZ2CA_"},"source":["# 7. 지도 학습 기초 - II. 회귀"]},{"cell_type":"markdown","metadata":{"id":"EKvge-BF2CA_"},"source":["# 사용 함수 명\n","\n","- 상관관계 분석 : get_corr\n","- ADF Test : get_adf_test\n","- Shapiro-wilk Test : get_shapiro_wilk\n","- Feature Importance : get_feature_importance"]},{"cell_type":"markdown","metadata":{"id":"rgzEBwl12CA_"},"source":["# 이용 파일 명\n","data_X.csv\n","\n","# 이용 컬럼 명\n","['eod_date', '5380-price_close']"]},{"cell_type":"markdown","metadata":{"id":"NMUPNZUf2CA_"},"source":["# 0. 환경 설정"]},{"cell_type":"markdown","metadata":{"id":"luo_R1Dc2CA_"},"source":["# 0.1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnnqOhu92CA_"},"outputs":[],"source":["import numpy\n","import os\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wx_Nbyf2CA_"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams[\"figure.figsize\"] = (40, ■■■■■)\n","plt.rcParams['lines.linewidth'] = 2\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-z6sxDVG2CA_"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html th { text-align: left; }\n","</■■■■■>"]},{"cell_type":"markdown","metadata":{"id":"TZHIYcTP2CA_"},"source":["# 0.2. 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUnVoOpG2CBA"},"outputs":[],"source":["# ================================================================================================================================\n","# 개인별 환경에 맞게 조정하실 것\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args = dict()\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 입력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args['base_file'] = ■■■■■(\"./Data\") + '/data_X.csv'\n","# 기준일자\n","dict_args['date_col_name'] = 'eod_date'\n","# 현대차 일별 종가 \n","dict_args['target_col_name'] = '5380-price_close'\n","# 학습 구간 (window size)\n","dict_args['window_size'] = 10\n","# 테스트 크기\n","dict_args['test_size'] = 2\n","# 테스트 시작 기간\n","dict_args['test_date_start'] = 20140101\n","# 테스트 종료 기간\n","dict_args['test_date_finish'] = 20140410\n","# 모델 파일 저장 패턴\n","dict_args['model_file'] = 'zulu_train_model_MODELNAME.h5'\n","\n","dir_work = 'work/'\n","■■■■■ = 'refs/'\n","\n","if not os.path.exists(dir_work):\n","    os.mkdir(dir_work)\n","if not os.path.exists(dir_refs):\n","    os.mkdir(dir_refs)"]},{"cell_type":"markdown","metadata":{"id":"vcR2Wjqv2CBA"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbuuTMsx2CBA"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","df = df.set_index([dict_args['date_col_name']])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"hi-exiFt2CBA"},"source":["# 0.4. 함수 선언"]},{"cell_type":"markdown","metadata":{"id":"I_juMIGf2CBA"},"source":["## 0.4.1. 함수 선언 - 01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUSa5h4j2CBA"},"outputs":[],"source":["# data load\n","def data_load(path,file_name, Date_col):\n","    import pandas\n","    ■■■■■ = ■■■■■(path + file_name, index_col = False, low_memory = False)\n","    df[Date_col] = pandas.to_datetime(df[Date_col], format='%Y%m%d')\n","    df = df.set_index(Date_col)\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"7JhHmCJe2CBA"},"source":["# 1. 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"TG_xPNEn2CBA"},"source":["# 1.1. 예측 대상 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ES-Y_1FI2CBA"},"outputs":[],"source":["df_Y = df[[dict_args['target_col_name']]]\n","df_Y = df_Y[df_Y.index <= dict_args['test_date_finish']]\n","df_Y = df_Y[df_Y[dict_args['target_col_name']] > 0]\n","df_Y['target'] = df_Y[dict_args['target_col_name']].shift(-1)\n","df_Y = df_Y.dropna(axis='rows')\n","del df_Y[■■■■■['target_col_name']]\n","#display(df_Y)\n","df_Y.to_csv('data_Y_source.csv')\n","df_plot_Y = ■■■■■()\n","df_plot_Y['datetime'] = pandas.to_datetime(df_plot_Y.index, format='%Y%m%d')\n","df_plot_Y = df_plot_Y.set_index(['datetime'])\n","# df_plot_Y.plot()"]},{"cell_type":"markdown","metadata":{"id":"TL5yxjzH2CBA"},"source":["# 1.2. 데이터 처리 속도 향상을 위한 종가 데이터 발췌"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVmKVwl_2CBA"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_X = pandas.DataFrame()\n","for col_name in df.columns:\n","    if 'close' in col_name:\n","        if df_X.shape[■■■■■] == 0:\n","            df_X = df[[col_name]]\n","        else:\n","            df_X[col_name] = df[col_name]\n","#display(df_X)\n","■■■■■('data_X_source.csv')\n","\n","df_source = df_Y.join(df_X, how='inner')\n","df_source = df_source.fillna(1 + ■■■■■-7)\n","#display(df_source)\n","for col_name in df_source.columns:\n","    if numpy.min(df_source[col_name]) == numpy.max(df_source[col_name]):\n","        del df_source[col_name]\n","#display(df_source)"]},{"cell_type":"markdown","metadata":{"id":"zQMXOR-42CBA"},"source":["# 2. 피처 엔지니어링"]},{"cell_type":"markdown","metadata":{"id":"wZWFZRN72CBB"},"source":["# 2.1. 정규화 / 표준화 / 로그 데이터 생성"]},{"cell_type":"markdown","metadata":{"id":"8D-0Oe4G2CBB"},"source":["# 2.1.1. 정규화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9YH3Brk2CBB"},"outputs":[],"source":["df_norm = df_source.copy()\n","list_min_max = list()\n","for col_name in df_norm.columns:\n","    val_min = numpy.min(df_norm[col_name])\n","    val_max = numpy.max(df_norm[col_name])\n","    if val_min == val_max:\n","        del df_norm[col_name]\n","        continue\n","    list_min_max.append({\n","        'col': col_name,\n","        'min': val_min,\n","        'max': val_max\n","    })\n","    df_norm[col_name] = (df_norm[col_name] - ■■■■■) / (■■■■■ - ■■■■■)\n","df_min_max = pandas.DataFrame(list_min_max)\n","df_min_max.to_csv(dir_refs + 'ref_min_max.csv', index=None)\n","#display(df_norm)"]},{"cell_type":"markdown","metadata":{"id":"o383ckXf2CBB"},"source":["# 2.1.2. 표준화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFInkBb12CBB"},"outputs":[],"source":["df_std = df_source.copy()\n","for col_name in df_std.columns:\n","    val_std = ■■■■■(df_std[col_name])\n","    ■■■■■ = numpy.mean(df_std[col_name])\n","    if val_std == 0:\n","        del df_std[col_name]\n","        continue\n","    df_std[col_name] = (df_std[col_name] - val_avg) / val_std\n","#display(df_std)"]},{"cell_type":"markdown","metadata":{"id":"ce5F6hoY2CBB"},"source":["# 2.1.3. 로그 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWkBOCx42CBB"},"outputs":[],"source":["df_log = df_source.copy()\n","df_log = numpy.log(df_log)\n","#display(df_log)"]},{"cell_type":"markdown","metadata":{"id":"0w-DJUX-2CBB"},"source":["# 2.2. 상관관계 분석"]},{"cell_type":"markdown","metadata":{"id":"5PrB5nGv2CBB"},"source":["# 2.2.1. 원천 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SiB1Nmj2CBB"},"outputs":[],"source":["# 상관관계 분석\n","def get_corr(p_source, p_target):\n","    import numpy\n","    import pandas\n","\n","    list_corr = list()\n","    for col_name in p_source.columns:\n","        if col_name == 'target':\n","            continue\n","        if col_name == p_target:\n","            ■■■■■\n","\n","        df_corr_check = p_source[['target', col_name]].corr(method='pearson')\n","\n","        list_corr.append({\n","            'VAR_NAME': col_name,\n","            'CORR': df_corr_check.head(1)[col_name].tolist()[0],\n","            'CORR-ABS': numpy.abs(df_corr_check.head(1)[col_name].tolist()[0]),\n","        })\n","    df_corr = pandas.DataFrame(list_corr)\n","    df_corr = ■■■■■(by=['CORR-ABS'], ascending=False)\n","\n","    ■■■■■ df_corr\n","\n","df_corr_src = df_source.fillna(-■■■■■)\n","df_corr_source = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_source.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_source.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"BgCNqFpF2CBC"},"source":["# 2.2.2. 표준화 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj1T0L5v2CBC"},"outputs":[],"source":["df_corr_src = df_std.copy()\n","df_corr_std = ■■■■■(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_std.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_std.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"1e1AdtgA2CBC"},"source":["# 2.2.3. 정규화 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDtJJoFo2CBC"},"outputs":[],"source":["df_corr_src = df_norm.copy()\n","df_corr_norm = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_norm.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_norm.head(1)['VAR_NAME'].tolist()[■■■■■]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"-El3fEUc2CBC"},"source":["# 2.2.4. 로그 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XwhQGDl2CBC"},"outputs":[],"source":["df_corr_src = df_log.copy()\n","df_corr_log = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_log.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_log.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, ■■■■■='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"Q3UjMbvo2CBC"},"source":["# 2.3. Feature Importance"]},{"cell_type":"markdown","metadata":{"id":"70u7qztN2CBC"},"source":["# 2.3.1. 원천 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdE3q0BH2CBE"},"outputs":[],"source":["# Feature Importance\n","def get_feature_importance(■■■■■, p_target):\n","    import numpy\n","    import pandas\n","    import sklearn.ensemble\n","\n","    l_data_y = l_data_x['target']\n","    del l_data_x['target']\n","    del l_data_x[p_target]\n","\n","    forest = sklearn.ensemble.RandomForestRegressor(\n","        max_depth = 50,\n","        min_samples_split = 5,\n","        min_samples_leaf = 3,\n","        ■■■■■ = 0,\n","        ■■■■■ = 100,\n","    )\n","    forest.fit(l_data_x, ■■■■■)\n","    l_importance = forest.feature_importances_\n","\n","    indices = numpy.argsort(l_importance)\n","\n","    columns = ■■■■■()\n","    column_count = len(columns)\n","\n","    list_var = list()\n","    for idx in ■■■■■(column_count):\n","        list_var.append({\n","            'feature_name': columns[indices[idx]],\n","            'feature_value': l_importance[indices[idx]]\n","        })\n","\n","    df_fi_source = pandas.DataFrame(list_var)\n","    df_fi_source = df_fi_source.sort_values(by='feature_value', ascending=False)\n","\n","    return df_fi_source\n","\n","df_src = df_source.fillna(-1)\n","df_fi_source = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_source.T)\n","warnings.filterwarnings('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_source.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"DPiONsko2CBF"},"source":["# 2.3.2. 표준화 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qruokI442CBF"},"outputs":[],"source":["df_src = ■■■■■(-1)\n","df_fi_std = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_std.T)\n","warnings.filterwarnings('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_std.head(■■■■■)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"seehYpyW2CBF"},"source":["# 2.3.3. 정규화 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqxLOW3E2CBF"},"outputs":[],"source":["df_src = df_norm.fillna(-■■■■■)\n","■■■■■ = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_norm.T)\n","warnings.filterwarnings('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_norm.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"WLP4GVfD2CBF"},"source":["# 2.3.4. 로그 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRC64aJP2CBF"},"outputs":[],"source":["df_src = df_log.fillna(-1)\n","■■■■■ = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_log.T)\n","warnings.filterwarnings('ignore')\n","■■■■■ = df_src[['target', df_fi_log.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"-LKSosrf2CBF"},"source":["# 2.4. 공적분 검정 - ADF Test"]},{"cell_type":"markdown","metadata":{"id":"ukUSaPkm2CBF"},"source":["# 2.4.1. 원천 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE9q2rko2CBF"},"outputs":[],"source":["# ADF Test\n","def get_adf_test(p_source, p_col_1, p_col_2):\n","    import statsmodels.tsa.stattools\n","\n","    p_source['spread'] = p_source[p_col_1] - p_source[p_col_2]\n","    return statsmodels.tsa.stattools.adfuller(p_source['spread'])\n","\n","warnings.filterwarnings('ignore')\n","df_src = df_source.fillna(-1)\n","list_result = list()\n","for col_name in ■■■■■:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_src = pandas.DataFrame(list_result)\n","df_adf_src = ■■■■■(by=['adf p-value'], ascending=True)\n","#display(df_adf_src.head(7))\n","selected_feature = df_adf_src.head(■■■■■)['feature_name']■■■■■()[■■■■■]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","# df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = df_plot_adf_src[['spread']]\n","# df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"-4NGOoon2CBF"},"source":["# 2.4.2. 표준화 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kao9WaoF2CBF"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_std.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[■■■■■]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': ■■■■■,\n","    })\n","df_adf_std = pandas.DataFrame(list_result)\n","df_adf_std = df_adf_std.sort_values(by=['adf p-value'], ascending=■■■■■)\n","#display(df_adf_std.head(7))\n","selected_feature = df_adf_src.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(■■■■■, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","# df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = df_plot_adf_src[['spread']]\n","# df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"zI9-jyKM2CBF"},"source":["# 2.4.3. 정규화 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpkkxwO42CBG"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_norm.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_norm = pandas.DataFrame(list_result)\n","df_adf_norm = df_adf_norm.sort_values(by=['adf p-value'], ascending=True)\n","#display(df_adf_norm.head(7))\n","selected_feature = df_adf_norm.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = ■■■■■(df_plot_adf_src.index, format='%Y%m%d')\n","■■■■■ = ■■■■■(['datetime'])\n","■■■■■ = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[selected_feature])]\n","df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[■■■■■]\n","df_plot = df_plot_adf_src[['spread']]\n","#df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"Pa2JWvBg2CBG"},"source":["# 2.4.4. 로그 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"324Zw_Ky2CBG"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_log.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        ■■■■■\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': ■■■■■,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_log = pandas.DataFrame(list_result)\n","df_adf_log = ■■■■■(by=['adf p-value'], ascending=True)\n","#display(df_adf_log.head(7))\n","selected_feature = df_adf_log.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","df_plot_adf_src = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[selected_feature])]\n","df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = ■■■■■[['spread']]\n","■■■■■()"]},{"cell_type":"markdown","metadata":{"id":"jI6p_lwC2CBG"},"source":["# 2.5. 공적분 검정 - Shapiro-wilk Test"]},{"cell_type":"markdown","metadata":{"id":"rBAFPpSt2CBG"},"source":["# 2.5.1. 원천 데이터에 대해 shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvCn2pDy2CBG"},"outputs":[],"source":["# Shapiro wilk Test\n","def get_shapiro_wilk(p_source, p_col_1, p_col_2):\n","    import scipy.stats\n","\n","    p_source['spread'] = p_source[p_col_1] - p_source[p_col_2]\n","    return ■■■■■(p_source['spread'])\n","\n","warnings.filterwarnings('ignore')\n","df_src = ■■■■■(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_src = pandas.DataFrame(list_result)\n","df_shapiro_src = df_shapiro_src.sort_values(by=['shapiro-wilk'], ascending=■■■■■)\n","#display(df_shapiro_src)\n","selected_feature = df_shapiro_src.head(■■■■■)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","■■■■■()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"oQSPXQKb2CBG"},"source":["# 2.5.2. 표준화 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN7lDRfF2CBG"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_std.fillna(-1)\n","list_result = ■■■■■()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(■■■■■, 'target', col_name)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_std = pandas.DataFrame(list_result)\n","df_shapiro_std = df_shapiro_std.sort_values(by=['shapiro-wilk'], ■■■■■=False)\n","#display(df_shapiro_std)\n","selected_feature = df_shapiro_std.head(1)['feature_name']■■■■■()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"AJryfM4m2CBG"},"source":["# 2.5.3. 정규화 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-1YpLhA2CBG"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = ■■■■■(-1)\n","list_result = list()\n","for ■■■■■ in df_src.columns:\n","    if ■■■■■ == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n","    ■■■■■ = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_norm = pandas.DataFrame(list_result)\n","df_shapiro_norm = df_shapiro_norm.sort_values(by=['shapiro-wilk'], ascending=False)\n","#display(df_shapiro_norm)\n","selected_feature = df_shapiro_norm.head(1)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"j7s1GwII2CBG"},"source":["# 2.5.4. 로그 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRH0Cro02CBH"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_log.fillna(-1)\n","list_result = list()\n","for ■■■■■ in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if ■■■■■ == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', ■■■■■)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_log = pandas.DataFrame(list_result)\n","df_shapiro_log = df_shapiro_log.sort_values(by=['shapiro-wilk'], ascending=False)\n","#display(df_shapiro_log)\n","selected_feature = df_shapiro_log.head(1)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = ■■■■■[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"fUDuKjOl2CBH"},"source":["# 2.6. 변수 취합"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl0ekXl62CBH"},"outputs":[],"source":["list_features = ['target']\n","list_features.extend(df_corr_source.head(20)['VAR_NAME'].tolist())\n","list_features.extend(df_corr_std.head(20)['VAR_NAME'].tolist())\n","■■■■■(df_corr_norm.head(20)['VAR_NAME'].tolist())\n","list_features.extend(df_corr_log.head(■■■■■)['VAR_NAME'].tolist())\n","\n","list_features.extend(df_fi_source.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_std.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_norm.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_log.head(20)['feature_name'].tolist())\n","\n","list_features.extend(df_adf_src.head(20)['feature_name'].tolist())\n","list_features.extend(df_adf_std.head(■■■■■)['feature_name']■■■■■())\n","list_features.extend(df_adf_norm.head(20)['feature_name'].tolist())\n","list_features.extend(df_adf_log.head(20)['feature_name'].tolist())\n","\n","list_features.extend(df_shapiro_src.head(20)['feature_name']■■■■■())\n","list_features.extend(df_shapiro_std.head(20)['feature_name'].tolist())\n","list_features.extend(df_shapiro_norm.head(20)['feature_name'].tolist())\n","list_features.extend(df_shapiro_log.head(20)['feature_name'].tolist())\n","\n","list_features = list(set(list_features))\n","# print(list_features)\n","\n","df_train = df_source.fillna(1 + 1e-7)\n","df_train = ■■■■■[list_features]\n","#display(df_train)\n","df_train.to_csv('data_modeling.csv')\n","df_plot = df_train[['target']]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"dLAEX7D42CBH"},"source":["# 3. 모델링"]},{"cell_type":"markdown","metadata":{"id":"YiMBAaNl2CBH"},"source":["# 3.1. linear_model.OrthogonalMatchingPursuitCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v84PUfot2CBH"},"outputs":[],"source":["import sklearn.linear_model\n","\n","model = sklearn.linear_model.OrthogonalMatchingPursuitCV(cv=3)\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del ■■■■■['target']\n","\n","    model.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / ■■■■■ * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = ■■■■■(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = ■■■■■(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","■■■■■()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"piPi1X-z2CBH"},"source":["# 3.2. tree.DecisionTreeRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CN4zJuc92CBH"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.tree\n","\n","model = sklearn.tree.DecisionTreeRegressor(criterion='squared_error', random_state=111)\n","\n","hyper_parameter = {\n","    'max_features': ['auto'],\n","    'max_depth': [3],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [1],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + ■■■■■ + dict_args['test_size'])■■■■■(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","■■■■■ = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = ■■■■■\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = ■■■■■(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(■■■■■, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    ■■■■■ = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - ■■■■■) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"Ki5lAkJY2CBH"},"source":["# 3.3. ensemble.RandomForestRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94M5-68P2CBI"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.ensemble\n","\n","model = sklearn.ensemble.RandomForestRegressor(criterion='squared_error', random_state=111, n_estimators=1)\n","\n","hyper_parameter = {\n","    'max_features': ['auto'],\n","    'max_depth': [2],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [2],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(■■■■■['test_size']):\n","    df_train_X = df_train.tail(■■■■■['window_size'] + idx + 1).head(■■■■■['window_size'])\n","    ■■■■■ = df_train_X.tail(1)\n","    ■■■■■(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = ■■■■■['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - ■■■■■) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = ■■■■■[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"L5ZFskZl2CBI"},"source":["# 3.4. xgboost.XGBRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfaSNpdC2CBI"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import xgboost\n","\n","model = xgboost.XGBRegressor(booster='gbtree', objective='reg:squarederror', n_estimators=2)\n","\n","hyper_parameter = {\n","    'reg_lambda': [1.0, 0.0, 0.1],\n","    'reg_alpha': [0.0, 0.001, 0.01],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(■■■■■['window_size'])\n","■■■■■ = df_tune_X['target']\n","del ■■■■■['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","■■■■■ idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = ■■■■■(df_plot.index, format='%Y%m%d')\n","df_plot = ■■■■■(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = ■■■■■[['PercentError']]\n","■■■■■()"]},{"cell_type":"markdown","metadata":{"id":"ihZZtVrF2CBI"},"source":["# 3.5. LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15PDFs3n2CBI"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import lightgbm\n","\n","model = lightgbm.LGBMRegressor(random_state=0, objective='regression', ■■■■■=-1, max_depth=-1, subsample=0.8, min_child_weight=3)\n","\n","■■■■■ = {\n","    'learning_rate': [0.1],\n","    'num_leaves': [2],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","■■■■■ = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, ■■■■■=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx ■■■■■ range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    ■■■■■ = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(■■■■■, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / ■■■■■ * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"yWNrolTx2CBI"},"source":["# 3.6. keras.LSTM"]},{"cell_type":"markdown","metadata":{"id":"OE1dg9_r2CBI"},"source":["# 3.6.1. LSTM Style #1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEMzlkaV2CBI"},"outputs":[],"source":["# window 기간에 과거 데이터를 통한 예측 데이터 생성 함수\n","def make_dataset(data, label, window_size):\n","    import numpy\n","    feature_list = ■■■■■()\n","    label_list = list()\n","    for i in range(len(data) - ■■■■■):\n","        feature_list.append(numpy.array(data.iloc[i:i+window_size]))\n","        label_list.append(numpy.array(label.iloc[i+window_size]))\n","    return numpy.array(feature_list), numpy.array(label_list)\n","\n","def split_data(DataFrame, test_size, target_col, ■■■■■):\n","    import pandas\n","    # train_test 분리\n","    train = DataFrame[:-test_size]\n","    test = DataFrame[-test_size:]\n","\n","    # feature, target 분리\n","    feature_cols = list(DataFrame.columns)\n","    feature_cols.remove(target_col)\n","\n","    # train_data\n","    train_feature = train[feature_cols]\n","    train_label = ■■■■■(train[target_col])\n","\n","    # test_data\n","    test_feature = ■■■■■[feature_cols]\n","    test_label = pandas.DataFrame(test[target_col])\n","\n","    # 예측 주기에 따른 timestep 생성\n","    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n","    test_feature, test_label = make_dataset(test_feature, test_label, window_size)\n","\n","    # print('split_data', train_feature.shape, train_label.shape)\n","    # print('split_data', test_feature.shape, test_label.shape)\n","\n","    return train_feature, train_label, test_feature, test_label\n","\n","def ■■■■■(train_feature, train_label, test_size, units=4, ■■■■■=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=200, batch_size=16):\n","    \n","    import sklearn\n","    import tensorflow.keras.models\n","    import tensorflow.keras.layers\n","    import tensorflow.keras.callbacks\n","\n","    # train_valid 분할\n","    x_train, x_valid, y_train, ■■■■■ = sklearn.model_selection.train_test_split(train_feature, train_label, test_size=test_size)\n","\n","    # print('LSTM_MODEL', x_train.shape, x_valid.shape)\n","    # print('LSTM_MODEL', y_train.shape, y_valid.shape)\n","\n","    # Sequential 모델 레이어 생성\n","    model = tensorflow.keras.models.Sequential()\n","    # LSTM 레이어 추가\n","    model.add(tensorflow.keras.layers.LSTM(\n","        units,\n","        input_shape=(train_feature.shape[1], train_feature.shape[2]),\n","        activation='relu',\n","        return_sequences=False\n","    ))\n","    # Dense 레이어 추가\n","    model.add(tensorflow.keras.layers.Dense(32))\n","    model.add(tensorflow.keras.layers.Dense(1))\n","    # print(model.summary())\n","    # 모델 컴파일 활성함수 loss_function 지정 , 최적화함수 지정\n","    ■■■■■(loss=loss, optimizer=optimizer)\n","    # 얼리스탑핑 ( val_loss의 변화를 비교, 같은 값 5번 일시 중지)\n","    early_stop = tensorflow.keras.callbacks.EarlyStopping(monitor=monitor, patience=20, baseline=0.4, mode ='min')\n","    # 모형 가중치 저장 파일 경로 선언\n","    # 모형 가중치 판단 및 모델 저장\n","    ■■■■■ = 'refs/model_checkpoint.h5'\n","    checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filename, monitor=monitor, ■■■■■=1, save_best_only=True, mode='min')\n","\n","    # 모형 학습\n","    model.fit(x_train, y_train,\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              validation_data=(x_valid, y_valid),\n","              callbacks=[early_stop, checkpoint],\n","              verbose = 1\n","              )\n","\n","    return model, filename\n","\n","\n","■■■■■('ignore')\n","\n","import sklearn\n","import tensorflow.keras.models\n","import tensorflow.keras.layers\n","import tensorflow.keras.callbacks\n","\n","df_test_scaled = df_source.fillna(1 + 1e-7)\n","list_min_max = list()\n","for col_name in df_source.columns:\n","    val_min = numpy.min(df_test_scaled[col_name])\n","    val_max = ■■■■■(df_test_scaled[col_name])\n","    if val_min == val_max:\n","        del df_test_scaled[col_name]\n","        continue\n","    list_min_max.append({\n","        'col': col_name,\n","        'min': val_min,\n","        'max': val_max\n","    })\n","    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n","df_min_max = pandas.DataFrame(list_min_max)\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","\n","    df_train_sub = df_test_scaled.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    predict_date = df_train_sub.tail(1).index.tolist()[0]\n","    df_train_X, df_train_Y, df_tests_X, df_tests_Y = ■■■■■(df_train_sub, ■■■■■, 'target', 1)\n","\n","    model_instance, filename = LSTM_MODEL(df_train_X, ■■■■■, 0.2, units=2, loss=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=2, batch_size=128)\n","    model_instance.fit(■■■■■, df_train_Y)\n","\n","    real_value = df_tests_Y.tolist()[0][0]\n","    predict_value = model_instance.predict(df_tests_X)[0][0]\n","    print(predict_date, ■■■■■, predict_value)\n","    list_predict.append({\n","        'date': predict_date,\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (■■■■■ - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = ■■■■■(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"IPTLlfq12CBI"},"source":["# 3.6.2. LSTM Style #2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9e0InrT2CBJ"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn\n","import tensorflow.keras.models\n","import tensorflow.keras.layers\n","import tensorflow.keras.callbacks\n","import tensorflow.keras.optimizers\n","import datetime\n","import sys\n","\n","def line_logging(*messages):\n","    import datetime\n","    import sys\n","    today = datetime.datetime.today()\n","    log_time = today.strftime('[%Y/%m/%d %H:%M:%S]')\n","    log = []\n","    for message in messages:\n","        log.append(str(message))\n","    print(log_time + '::' + ' '.join(log) + '')\n","    sys.stdout.flush()\n","\n","\n","df_test_scaled = ■■■■■(1 + 1e-7)\n","list_min_max = list()\n","for col_name in ■■■■■:\n","    val_min = numpy.min(df_test_scaled[col_name])\n","    val_max = numpy.max(df_test_scaled[col_name])\n","    if val_min == val_max:\n","        del df_test_scaled[■■■■■]\n","        ■■■■■\n","    list_min_max.append({\n","        'col': col_name,\n","        'min': val_min,\n","        'max': val_max\n","    })\n","    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n","df_min_max = pandas.DataFrame(list_min_max)\n","\n","size_row = df_test_scaled.shape[0]\n","size_col = df_test_scaled.shape[1]\n","size_win = dict_args['window_size']\n","loop_count = size_row - size_win + 1\n","line_logging(■■■■■, size_col, size_win, loop_count)\n","\n","for col_name in df_test_scaled.columns:\n","    # print(col_name, numpy.min(df_test_scaled[col_name]), numpy.max(df_test_scaled[col_name]))\n","    pass\n","    if numpy.min(df_test_scaled[col_name]) != 0.0:\n","        print(col_name, 'min', numpy.min(■■■■■[col_name]))\n","    if numpy.max(df_test_scaled[col_name]) != 1.0:\n","        print(col_name, 'max', numpy.max(df_test_scaled[col_name]))\n","line_logging('check min/max value')\n","\n","list_train_X = ■■■■■()\n","list_train_Y = list()\n","list_tests_X = list()\n","list_tests_Y = list()\n","■■■■■ idx in range(loop_count):\n","    df_train_sub_X = df_test_scaled.tail(size_win + idx + ■■■■■).head(size_win)\n","    df_train_sub_Y = df_train_sub_X.tail(1)['target']\n","    del df_train_sub_X['target']\n","    if idx >= loop_count - 50:\n","        list_tests_X.append(■■■■■())\n","        ■■■■■(df_train_sub_Y.to_numpy())\n","    else:\n","        list_train_X.append(■■■■■())\n","        list_train_Y.append(df_train_sub_Y.to_numpy())\n","line_logging('build data layer')\n","\n","train_X = numpy.array(list_train_X)\n","train_Y = numpy.array(list_train_Y)\n","tests_X = numpy.array(list_tests_X)\n","value_real = numpy.array(list_tests_Y)[0][0]\n","line_logging(train_X.shape, train_Y.shape, tests_X.shape, value_real.shape)\n","\n","units = 2\n","loss = 'mean_squared_error'\n","optimizer = tensorflow.keras.optimizers.Adam(clipvalue=0.5)\n","epochs = 2\n","batch_size = 128\n","monitor = 'val_loss'\n","model = tensorflow.keras.models.Sequential()\n","# LSTM 레이어 추가\n","# input_shape: 첫번째는 윈도우 크기, 두번째는 컬럼수\n","model.add(tensorflow.keras.layers.LSTM(units, input_shape=(size_win, size_col - 1), return_sequences=False))\n","# Dense 레이어 추가\n","model.add(tensorflow.keras.layers.Dense(32, activation='relu'))\n","model.add(tensorflow.keras.layers.Dense(1, activation='sigmoid'))\n","■■■■■('define model')\n","# 모델 컴파일 활성함수 loss_function 지정 , 최적화함수 지정\n","model.compile(loss=loss, ■■■■■=optimizer)\n","line_logging('compile model')\n","# 모형 학습\n","model.fit(train_X, train_Y, epochs=epochs, batch_size=■■■■■, verbose=1)\n","model.summary()\n","value_pred = model.predict(tests_X)[0][0]\n","line_logging(value_pred)\n","line_logging(value_real)\n","\n","# #display(df_min_max)\n","target_min = df_min_max[df_min_max['col'] == 'target']['min'].tolist()[0]\n","target_max = df_min_max[df_min_max['col'] == 'target']['max'].tolist()[0]\n","\n","denrm_pred =  target_min + (■■■■■ - target_min) * ■■■■■\n","denrm_real =  target_min + (target_max - target_min) * value_real\n","\n","line_logging('PRED', value_pred, denrm_pred)\n","line_logging('REAL', value_real, denrm_real)\n","line_logging('ERROR Normalize:', int((value_pred - value_real) / value_real * 10000) / 100, '%')\n","line_logging('ERROR De-normalize: ', int((denrm_pred - denrm_real) / denrm_real * ■■■■■) / 100, '%')"]},{"cell_type":"markdown","metadata":{"id":"rC0eYh5R2CBJ"},"source":["# 3.7. MLPRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXOdppb22CBJ"},"outputs":[],"source":["■■■■■('ignore')\n","\n","import sklearn.neural_network\n","\n","model = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=2, activation='relu', solver='adam', ■■■■■='constant')\n","\n","hyper_parameter = {\n","    'hidden_layer_sizes': [(2,), (3,)],\n","    'alpha': [0.001],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=■■■■■, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(■■■■■)■■■■■, inplace=True)\n","\n","    df_train_Y = ■■■■■['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = ■■■■■(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(■■■■■)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","# df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","# df_plot_error.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhOiC7h02CBJ"},"outputs":[],"source":["df_tune = df_std.copy()\n","#display(df_tune)\n","list_features = ['target']\n","list_features.extend(df_corr_std.head(20)['VAR_NAME'].tolist())\n","\n","df_tune = df_tune[list_features]\n","■■■■■['datetime'] = pandas.to_datetime(df_tune.index, format='%Y%m%d')\n","df_tune = df_tune.set_index(['datetime'])\n","#display(df_tune)\n","# df_tune.plot()\n","df_tune.to_csv('data_std.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('test')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"7f404382b6ee812ff854d886c5a2fbb987a19b2f69a33528c318b78ba24eea58"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}