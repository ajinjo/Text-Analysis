{"cells":[{"cell_type":"markdown","metadata":{"id":"mj4NViaAGTG0"},"source":["# 1. 파이썬 기초"]},{"cell_type":"markdown","metadata":{"id":"IFKcipfYGTG1"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASJcxP4PGTG2"},"outputs":[],"source":["# 병렬 실행은 같은 주피터 파일에 있으면 실행되지 않으므로 별도의 파일로 처리\n","from Functions import *\n","\n","import multiprocessing\n","import os\n","import pandas\n","import sqlite3"]},{"cell_type":"markdown","metadata":{"id":"HHvkU7KTGTG3"},"source":["# 주석 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ppSSauDVGTG3"},"outputs":[],"source":["# 한 줄 주석\n","'''\n","여러 줄 주석은 이렇게 쓰거나\n","또는....\n","'''\n","\"\"\"\n","이렇게 작성합니다.\n","예시로 작성했습니다.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"pBjosGD0GTG3"},"source":["# 데이터 유형"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQWppgkVGTG3"},"outputs":[],"source":["# =====================================================================================================================================\n","# 예제로 사용할 값입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","value1 = 1.1\n","value2 = '1.2'\n","value3 = 1.3\n","value4 = 1\n","value5 = 0\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 정수/실수/참거짓/문자열 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('정수 유형')\n","print('실제 값', value1, '(', type(value1), ')을 정수로 표현하면', int(value1), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('실수 유형')\n","print('실제 값', value2, '(', type(value2), ')을 실수로 표현하면', float(value2), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('참/거짓 유형')\n","print('실제 값', value4, '(', type(value4), ')을 참/거짓으로 표현하면', bool(value4), '입니다.')\n","print('실제 값', value5, '(', type(value5), ')을 참/거짓으로 표현하면', bool(value5), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","print('문자 유형')\n","print('실제 값', value3, '(', type(value3), ')을 문자열로 표현하면', str(value3), '입니다.')\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# set 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","set_sample = set()\n","set_sample.add(1)\n","set_sample.add(2)\n","set_sample.add(3)\n","set_sample.add(4)\n","set_sample.add(5)\n","■■■■■(1)\n","print('----------------------------------------------------------------------------------------------------')\n","print('set 유형 (값이 중복되지 않습니다.)')\n","print(set_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# list 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","list_sample = list()\n","list_sample.append(■■■■■)\n","list_sample.append(2)\n","list_sample.append(3)\n","list_sample.append(4)\n","list_sample.append(5)\n","list_sample.append(1)\n","print('----------------------------------------------------------------------------------------------------')\n","print('list 유형 (중복을 허용합니다.)')\n","print(list_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# dict 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","■■■■■ = dict()\n","dict_sample['key:1'] = 1\n","dict_sample['key:2'] = 2\n","dict_sample['key:3'] = 3\n","dict_sample['key:4'] = ■■■■■\n","dict_sample['key:5'] = 5\n","dict_sample['key:1'] = 6\n","print('----------------------------------------------------------------------------------------------------')\n","print('dict 유형 (key 에서는 중복을 허용하지 않고, value에서는 중복을 허용합니다.)')\n","print(dict_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# DataFrame 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","df_sample = ■■■■■([\n","    {'C1':1, 'C2':■■■■■},\n","    {'C1':3, 'C2':4},\n","    {'C1':5, 'C2':6},\n","    {'C1':■■■■■, 'C2':8},\n","])\n","print('----------------------------------------------------------------------------------------------------')\n","print('DataFrame 유형 (jupyter에서는 display를 사용하면 조금 더 확인이 쉽습니다.')\n","###display(df_sample)\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# Series 유형 예시입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('Series 유형 (DataFrame의 부분 집합입니다.)')\n","print(type(df_sample['C1']), df_sample['C1'])\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"ovAQmSiFGTG4"},"source":["# 조건문"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrHGd_VpGTG4"},"outputs":[],"source":["value1 = 1\n","value2 = 3.2\n","value3 = True\n","value4 = 'XX'\n","\n","# =====================================================================================================================================\n","# 정수 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 2\n","if value1 > ■■■■■:\n","    print('value1 은 ', condition_value, '보다 큽니다.')\n","elif value1 == condition_value:\n","    print('value1 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value1 은 ', ■■■■■, '보다 작습니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 실수 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 3\n","if value2 > condition_value:\n","    print('value2 은 ', condition_value, '보다 큽니다.')\n","elif value2 == condition_value:\n","    print('value2 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value2 은 ', condition_value, '보다 작습니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 참거짓 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","if value3:\n","    print('value3 은 ', value3, '참입니다.')\n","else:\n","    print('value3 은 ', value3, '거짓입니다.')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 문자열 값을 비교합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","condition_value = 'YY'\n","if value4 > condition_value:\n","    print('value4 은 ', condition_value, '보다 큽니다.')\n","elif value4 == ■■■■■:\n","    print('value4 은 ', condition_value, '과 같습니다.')\n","else:\n","    print('value4 은 ', condition_value, '보다 작습니다.')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"jlVW5qjnGTG5"},"source":["# 반복문"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHip5KYHGTG5"},"outputs":[],"source":["# =====================================================================================================================================\n","# For 구문입니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = list()\n","for index in range(10):\n","    list_to_log.append(index)\n","print(list_to_log)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# While 구문 입니다. (무한 반복을 주의합니다.)\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = list()\n","MAX_LOOP_COUNT = 10\n","flag_loop = True\n","loop_count = 0\n","while flag_loop:\n","    list_to_log.append(loop_count)\n","    loop_count += ■■■■■\n","    if loop_count > MAX_LOOP_COUNT:\n","        flag_loop = False\n","print(list_to_log)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# Do-While 구문 입니다. (파이썬에는 없습니다.)\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","list_to_log = list()\n","MAX_LOOP_COUNT = 10\n","flag_loop = True\n","loop_count = 0\n","list_to_log.append(loop_count)\n","■■■■■ += 1\n","while flag_loop:\n","    list_to_log.append(■■■■■)\n","    ■■■■■ += 1\n","    if loop_count > MAX_LOOP_COUNT:\n","        flag_loop = False\n","print(list_to_log)\n","# =====================================================================================================================================\n","print('----------------------------------------------------------------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"mREbgh_lGTG5"},"source":["# 예외 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdFTgeCjGTG6"},"outputs":[],"source":["value1 = '1.2'\n","try:\n","    print('문자열( 값:', value1, ')을 정수 유형으로 변경합니다.')\n","    print('값을 출력합니다. [', int(■■■■■), ']')\n","    print('첫 시도에서 데이터 유형 변경에 성공하였습니다.')\n","except:\n","    print('데이터 유형 변환에 실패하여, 실수형으로 먼저 변경한 후 정수형으로 변경합니다.')\n","    print('값을 출력합니다. [', int(float(value1)), ']')\n","    print('예외 처리로 데이터 유형 변경에 성공하였습니다.')"]},{"cell_type":"markdown","metadata":{"id":"v5WpGrjzGTG6"},"source":["# 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmH_Vyd6GTG6"},"outputs":[],"source":["def function_sample(p_args):\n","    print(type(p_args), p_args)\n","\n","value1 = 1\n","value2 = 1.2\n","value3 = '2.3'\n","value4 = {'key': 'value'}\n","value5 = ['element']\n","\n","function_sample(value1)\n","function_sample(value2)\n","function_sample(value3)\n","function_sample(value4)\n","function_sample(value5)"]},{"cell_type":"markdown","metadata":{"id":"vOnZEOQNGTG6"},"source":["# 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPd7WZ69GTG6"},"outputs":[],"source":["class class_sample():\n","    def method_sample(self, p_args):\n","        print(type(p_args), p_args)\n","\n","value1 = 1\n","value2 = 1.2\n","value3 = '2.3'\n","value4 = {'key': 'value'}\n","value5 = ['element']\n","\n","sample = class_sample()\n","sample.method_sample(value1)\n","sample.method_sample(value2)\n","sample.method_sample(value3)\n","sample.method_sample(value4)\n","sample.method_sample(value5)"]},{"cell_type":"markdown","metadata":{"id":"DOA-807gGTG6"},"source":["# 파일 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFPLrPgRGTG7"},"outputs":[],"source":["# =====================================================================================================================================\n","# 파일에 내용을 기록합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('파일에 내용을 기록합니다.')\n","sample_file_name = 'sample.csv'\n","with open(sample_file_name, 'w') as f:\n","    f.write('C1,C2\\n')\n","    f.write('1,2\\n')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 파일 안의 내용을 확인합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('파일 안의 내용을 확인합니다.')\n","###display(pandas.read_csv(sample_file_name))\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","# 파일의 내용을 읽습니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('파일의 내용을 읽습니다.')\n","with open(sample_file_name, 'r') as f:\n","    print(f.read())\n","print('----------------------------------------------------------------------------------------------------')\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 사용한 파일을 삭제합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('사용한 파일을 삭제합니다.')\n","os.remove(sample_file_name)\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"7pqlQCj7GTG7"},"source":["# 데이터베이스 처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQBbV4hkGTG7"},"outputs":[],"source":["# =====================================================================================================================================\n","# 데이터베이스와 연결합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('connect to database')\n","conn = sqlite3.connect(':memory:')\n","curs = conn.cursor()\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 테이블을 생성합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('create table')\n","try:\n","    curs.execute('drop table sample')\n","    conn.commit()\n","except:\n","    pass\n","curs.execute('''\n","create table sample (\n","    c1      int,\n","    c2      float,\n","    c3      text\n",")\n","''')\n","conn.commit()\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 데이터를 입력합니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('insert data to table')\n","df = pandas.DataFrame([\n","    {\n","        'c1': 1,\n","        'c2': 1.1,\n","        'c3': '1.1.1',\n","    },\n","    {\n","        'c1': 2,\n","        'c2': 2.2,\n","        'c3': '2.2.2',\n","    },\n","])\n","###display(df)\n","df_to_sql('sample', conn, if_exists='append', index=None)\n","# df_to_sql('sample', conn, if_exists='append', index=None)\n","\n","print('----------------------------------------------------------------------------------------------------')\n","print('check data in table')\n","df2 = pandas.read_sql('select * from sample', conn)\n","###display(df2)\n","# =====================================================================================================================================\n","\n","# =====================================================================================================================================\n","# 데이터를 읽습니다.\n","# -------------------------------------------------------------------------------------------------------------------------------------\n","print('----------------------------------------------------------------------------------------------------')\n","print('read data in table')\n","curs.execute('select * from sample')\n","rs = curs.fetchall()\n","for row in rs:\n","    print('c1:[', row[0], '], c2:[', row[1], '], c3:[', row[2], ']')\n","print('----------------------------------------------------------------------------------------------------')\n","# ====================================================================================================================================="]},{"cell_type":"markdown","metadata":{"id":"UiqRjoJGGTG7"},"source":["# 병렬처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWxm388zGTG7"},"outputs":[],"source":["list_param = list()\n","for idx in range(10):\n","    dict_param = dict()\n","    dict_param['index'] = idx\n","    list_param.append(dict_param)\n","p = multiprocessing.Pool(3)\n","# parallel_print 함수에 대한 정의는 .py 파일에 별도로 저장되어 있어야 합니다.\n","\n","p.map(parallel_print, list_param)"]},{"cell_type":"markdown","metadata":{"id":"huCviAy2GTG7"},"source":["# 전처리 기초"]},{"cell_type":"markdown","metadata":{"id":"1vsfDIeZGTG8"},"source":["# 사전 준비"]},{"cell_type":"markdown","metadata":{"id":"Kf3RE8chGTG8"},"source":["# 1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcHjKkJ7GTG8"},"outputs":[],"source":["import pandas\n","import sqlite3"]},{"cell_type":"markdown","metadata":{"id":"l8N1DxN2GTG8"},"source":["# 2. 예제 데이터 준비"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_XYPYm1GTG8"},"outputs":[],"source":["t1 = pandas.DataFrame([\n","    { 'c1': 11, 'c2': 111, 'c3': 1111, },\n","    { 'c1': 12, 'c2': 121, 'c3': ■■■■■, },\n","    { 'c1': 13, 'c2': 131, 'c3': 1311, },\n","    { 'c1': 14, 'c2': 141, 'c3': 1411, },\n","    { 'c1': 15, 'c2': 151, 'c3': 1511, },\n","    { 'c1': 16, 'c2': ■■■■■, 'c3': 1611, },\n","])\n","t2 = ■■■■■([\n","    { 'c1': 11, 'c2': 111, 'c3': 1111, 'c4': 21111, 'c5': 211111, },\n","    { 'c1': 12, 'c2': 121, 'c3': 1211, 'c4': 22111, 'c5': 221111, },\n","    { 'c1': 13, 'c2': 131, 'c3': 1311, 'c4': 23111, 'c5': 231111, },\n","    { 'c1': 21, 'c2': 211, 'c3': 2111, 'c4': ■■■■■, 'c5': 211111, },\n","    { 'c1': 22, 'c2': 221, 'c3': 2211, 'c4': 22111, 'c5': 221111, },\n","    { 'c1': 23, 'c2': 231, 'c3': 2311, 'c4': 23111, 'c5': 231111, },\n","    { 'c1': 24, 'c2': ■■■■■, 'c3': 2411, 'c4': 24111, 'c5': 241111, },\n","    { 'c1': 25, 'c2': 251, 'c3': 2511, 'c4': 25111, 'c5': 251111, },\n","    { 'c1': 26, 'c2': 261, 'c3': 2611, 'c4': 26111, 'c5': 261111, },\n","])\n","\n","db_conn = sqlite3.connect(':memory:')\n","t1.to_sql('t1', db_conn, if_exists='replace')\n","t2.to_sql('t2', db_conn, if_exists='replace')"]},{"cell_type":"markdown","metadata":{"id":"oFxc7JBQGTG8"},"source":["# UNION"]},{"cell_type":"markdown","metadata":{"id":"xhYuTX5GGTG8"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkLKCJZxGTG8"},"outputs":[],"source":["df = pandas.concat([t1, t2], sort=False)\n","df = df[['c1', 'c2', 'c3']]\n","df = df.drop_duplicates() \n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"vAQ_d2vxGTG9"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBofM5pzGTG9"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," union\n","select c1, c2, c3\n","  from t2\n","'''\n","\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","#print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"pu2F8q3pGTG9"},"source":["# UNION ALL"]},{"cell_type":"markdown","metadata":{"id":"Fjujugc7GTG9"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8673S8_tGTG9"},"outputs":[],"source":["df = pandas.concat([t1, t2], sort=False)\n","df = df[['c1', 'c2', 'c3']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"pdYxESmZGTG9"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAQUMX27GTG9"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," union all\n","select c1, c2, c3\n","  from t2\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"dU1iE3ZeGTG9"},"source":["# RELATIVE COMPLEMENT"]},{"cell_type":"markdown","metadata":{"id":"K_6Lw_-QGTG9"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRe1yoxLGTG-"},"outputs":[],"source":["set_t2 = set(t2['c1'].tolist())\n","df = t1[~t1['c1'].isin(set_t2)]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"rp44CcM6GTG-"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a25BZIfkGTG-"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t1\n"," where not exists (select 1\n","                     from t2\n","                    where t1.c1 = t2.c1)\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","#display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"b9PR4HaIGTG-"},"source":["# INNER JOIN"]},{"cell_type":"markdown","metadata":{"id":"F5RcXAesGTG-"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LShGOh9oGTG-"},"outputs":[],"source":["df_t1 = t1.set_index(['c1'])\n","df_t2 = t2.set_index(['c1'])\n","df = df_t1.join(df_t2, how='inner', lsuffix='', rsuffix='_DEL')\n","df = df.reset_index()\n","df = df[['c1', 'c2', 'c3', 'c4']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"b8e_c5SNGTG-"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxqKufc7GTG-"},"outputs":[],"source":["sql_select = '''\n","select t1.c1, t1.c2, t1.c3, t2.c4\n","  from t1\n"," inner join t2\n","         on t1.c1 = t2.c1\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"_J2T9vkvGTG-"},"source":["# LEFT OUTER JOIN"]},{"cell_type":"markdown","metadata":{"id":"M68MK8O1GTG_"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAl3j5DpGTG_"},"outputs":[],"source":["df_t1 = t1.set_index(['c1'])\n","df_t2 = t2.set_index(['c1'])\n","df = df_t1.join(df_t2, how='left', lsuffix='', rsuffix='_DEL')\n","df = df.reset_index()\n","df = df[['c1', 'c2', 'c3', 'c4']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"LQ_SZvBJGTG_"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYGn8EYcGTG_"},"outputs":[],"source":["sql_select = '''\n","select t1.c1, t1.c2, t1.c3, t2.c4\n","  from t1\n","  left outer join t2\n","               on t1.c1 = t2.c1\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"6dIVI0lOGTG_"},"source":["# GROUP BY"]},{"cell_type":"markdown","metadata":{"id":"pmBoDym2GTG_"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m48Flhw1GTG_"},"outputs":[],"source":["df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ( 'c3', 'count')).reset_index()\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"hY2sVpzhGTG_"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiW8-ylRGTG_"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, count(c3) as cnt_c3\n","  from t1\n"," group by c1, c2\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"ospmecgoGTHA"},"source":["# WHERE"]},{"cell_type":"markdown","metadata":{"id":"qnKAcqEwGTHA"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3I4tw2egGTHA"},"outputs":[],"source":["df = t2[t2['c4'] == 22111]\n","df = df[['c1', 'c2', 'c3']]\n","print(df.shape)\n","###display(df)"]},{"cell_type":"markdown","metadata":{"id":"rhhJTp4PGTHB"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhmqciAmGTHB"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, c3\n","  from t2\n"," where c4 = 22111\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"RFSv9alEGTHB"},"source":["# HAVING"]},{"cell_type":"markdown","metadata":{"id":"EPiN3fPqGTHB"},"source":["# 1. Python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5NExwnhGTHB"},"outputs":[],"source":["import numpy\n","df = t1.groupby(['c1', 'c2']).agg(cnt_c3 = ( 'c3', numpy.sum)).reset_index()\n","df = df[df['cnt_c3'] > 1311]\n","print(df.shape)\n","##display(df)"]},{"cell_type":"markdown","metadata":{"id":"Bwpwkap3GTHB"},"source":["# 2. SQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1EgkNZ-GTHB"},"outputs":[],"source":["sql_select = '''\n","select c1, c2, sum(c3) as cnt_c3\n","  from t1\n"," group by c1, c2\n","having sum(c3) > 1311\n","'''\n","df_from_db = pandas.read_sql(sql_select, db_conn)\n","print(df_from_db.shape)\n","###display(df_from_db)"]},{"cell_type":"markdown","metadata":{"id":"k54gF6X-GTHC"},"source":["# 전처리를 통한 모델 데이터셋 생성하기"]},{"cell_type":"markdown","metadata":{"id":"MRwAUkX7GTHC"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJ-mufadGTHC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"PqkNgPB5GTHC"},"source":["### 데이터 설명\n","\n","#### 파일 및 설명\n","\n","- sales.csv - 2017~2019 3개년치 판매데이터 중 2019년 데이터만 활용\n","- product_hierarchy.csv - 대중소 등 5단계별 상품그룹코드 및 상품 사이즈\n","- store_cities.csv - 매장도시, 유형, 면적\n","\n","#### 컬럼(변수) 설명\n","\n","- store_id - 매장코드\n","- product_id - 상품코드\n","- date - 판매일자\n","- sales - 일별 판매량\n","- revenue - 일별 이익액\n","- stock - 일별 재고량\n","- price - 판매가/가격\n","- promotype1 - 판매채널1의 프로모션1 유형\n","- promobin1 - 판매채널1 프로모션1의 프로모션 binning 적용율\n","- promotype2 - 판매채널2의 프로모션2 유형\n","- promobin2 - 판매채널2 프로모션2의 프로모션 binning 적용율\n","- promodiscount2 - 프로모션2의 적용된 할인율\n","- promodiscounttype_2 - 프모로션2 할인율 유형\n","- product_length - 제품 가로사이즈\n","- product_depth - 제품 세로사이즈\n","- product_width - 제품 높이사이즈\n","- hierarchy1_id - 그룹코드1\n","- hierarchy2_id - 그룹코드2\n","- hierarchy3_id - 그룹코드3\n","- hierarchy4_id - 그룹코드4\n","- hierarchy5_id - 그룹코드5\n","- storetype_id - 매장유형코드\n","- store_size - 매장면적\n","- city_id - 도시코드"]},{"cell_type":"markdown","metadata":{"id":"bBW30VxXGTHC"},"source":["#### 결측치 처리 및 모델링 변수 활용 방법 \n","#### 범주형(명목형 또는 순서형) - 각 조건별로 One-hot encoding\n","#### 연속형(수치형) - 구간 설정 후 각 구간별로 One-hot encoding - EDA를 통해 구간 정의 필요\n","#### 또는 연속데이터로 활용\n","\n","#### Shift 및 Rolling을 활용한 시계열 데이터 생성"]},{"cell_type":"markdown","metadata":{"id":"PAfNkHM4GTHC"},"source":["# 데이터 불러오기 (csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDHqXEPHGTHC"},"outputs":[],"source":["#파일 경로 설정\n","PATH = os.path.abspath(\"./Data\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENt8g9sgGTHC"},"outputs":[],"source":["# sales_df = pd.read_csv(PATH+'sales.csv') #2019년도 데이터만 추출\n","sales_df = pd.read_csv(PATH+'/sales_2019.csv')\n","store_df = pd.read_csv(PATH+'/store_cities.csv')\n","product_df = pd.read_csv(PATH+'/product_hierarchy.csv')"]},{"cell_type":"markdown","metadata":{"id":"VE95ifiqGTHD"},"source":["# 데이터 불러오기(csv) -> 쓰기(json) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XNcZkxlGTHD"},"outputs":[],"source":["file_handler = open(PATH+'/sample_json.csv', 'r')\n","# file_handler.readlines()\n","lines = file_handler.readlines()\n","# print(lines)\n","previous_data = None\n","# previous_data = None\n","json_dictionary = {}\n","for line in lines:\n","    line_data = [data.strip() for data in line.split(',')]\n","    \n","    # 이전 줄이 있을 때에만 수행\n","    if previous_data is not None:\n","      \n","        p1, p2, p3 = previous_data[0], previous_data[1], previous_data[2]\n","        c1, c2, c3 = line_data[0], line_data[1]], line_data[2]\n","        \n","        # level1이 다른 경우 새로운 level1, level2 키를 만들어서 값 하나를 저장\n","        if p1 != c1: \n","            json_dictionary[c1] = {}\n","            json_dictionary[c1][c2]=c3\n","        else:\n","            # level2가 다른 경우 새로운 level2 키를 만들어서 값 하나를 저장\n","            if p2 != c2:\n","                json_dictionary[c1][c2]=c3\n","                # level1, level2 가 이전과 같으므로 level3의 값만 추가하여 갱신\n","            else:\n","                previous_value = json_dictionary[c1][c2]\n","                json_dictionary[c1][c2]= f'{previous_value}:{c3}'        \n","        \n","    previous_data = line_data\n","\n","file_handler.close()\n","\n","json_data = json.dumps(json_dictionary)\n","print(json_data)"]},{"cell_type":"markdown","metadata":{"id":"XXU75AkKGTHD"},"source":["# 데이터 불러오기 (json) -> 쓰기(csv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yV66KIdSGTHD"},"outputs":[],"source":["json_parsed = ■■■■■(json_data)\n","columns = ['C1', 'C2', 'C3']\n","csv_string = ','.join(columns)\n","\n","first_keys = ■■■■■(list(json_parsed.keys()))\n","for key1 in first_keys:\n","    second_keys = sorted(list(json_parsed[key1].keys()))\n","    for key2 in second_keys:\n","        third_keys = ■■■■■[key1][key2].split(':')\n","        for key3 in third_keys:\n","            csv_string = csv_string + f'\\n{key1},{key2},{key3}'\n","            \n","file_handler = open('json_to_csv.csv', 'w')\n","file_handler.write(csv_string)\n","file_handler.close()"]},{"cell_type":"markdown","metadata":{"id":"JQ1I-3_1GTHD"},"source":["# 필요한 데이터 유지 (필요없는 변수 삭제)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVVxJ-mGGTHD"},"outputs":[],"source":["#### drop 함수 사용하여 'year' 변수 삭제\n","sales_df.drop('year', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drcEvnucGTHD"},"outputs":[],"source":["#### 필요 변수만 유지 - 'year' 변수만 제외하고 유지\n","columns = ['product_id','store_id','date','sales','revenue','stock','price','promo_type_1','promo_bin_1','promo_type_2','promo_bin_2','promo_discount_2','promo_discount_type_2']\n","sales_df = sales_df[columns]"]},{"cell_type":"markdown","metadata":{"id":"t9EHFLvNGTHD"},"source":["# 결측치 변수 삭제"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJhZVDQlGTHD"},"outputs":[],"source":["#결측치여부 - 각 변수별 결측치 개수 확인\n","print(sales_df.isna().sum())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IK5okMAGTHE"},"outputs":[],"source":["# \"sales\" 변수 기준으로 결측치 행 삭제\n","#방법 1 - 결측치를 제외한 인덱스 유지\n","\n","sales_df = sales_df[sales_df['sales'].isna()!=True].reset_index(drop=True)\n","#방법 2 - 결측치 인덱스를 드롭한 남은 인덱스 추출\n","#sales_df = sales_df.iloc[sales_df['sales'].dropna().index].reset_index(drop=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o3AdtXpGTHE"},"outputs":[],"source":["#판매수량 1이상인 데이터만 유지\n","#sales_df = sales_df[sales_df.sales>0]\n","#인덱스 수정\n","#sales_df.reset_index(drop=True, inplace=True)\n","\n","#또는 \n","sales_df = sales_df[sales_df.sales>0].reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"4eZTj45WGTHE"},"source":["# 결측치 채우기"]},{"cell_type":"markdown","metadata":{"id":"fPu2gzGaGTHE"},"source":["# 문자열 변수의 결측치는 ''으로 변경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2vuwsGhGTHE"},"outputs":[],"source":["#결측치 수정\n","sales_df.promo_bin_1.fillna('', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhSORhRvGTHE"},"outputs":[],"source":["#결측치 수정\n","sales_df.promo_bin_2.fillna('', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nY-AKCj1GTHE"},"outputs":[],"source":["sales_df.promo_discount_type_2.fillna('', inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"rY4iR2F0GTHE"},"source":["# 가격 결측치 채우기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32d1uOOMGTHE"},"outputs":[],"source":["#가격 데이터와 결측치 데이터 분리\n","#가격정보 유 데이터 df1 으로 생성\n","df1 = sales_df[sales_df.price.isna()!=True]\n","#가격정보 무 데이터 df0 으로 생성\n","df0 = sales_df[sales_df.price.isna()==True]\n","\n","#가격정보 없는 매장의 상품 정보가 가격정보가 있는 데이터에서 존재하는지 확인\n","#상품코드/매장코드 기준으로 병합\n","#날짜별 가격 정보 포함\n","#df1의 상품코드/매장코드/가격/날짜 와 df0의 상품코드/매장코드로 상품코드/매장코드 기준으로 병합 후 dfz로 생성\n","dfz = pd.merge(df1[['product_id', 'store_id', 'price', 'date']], df0[['product_id', 'store_id']], on=['product_id', 'store_id'], how='inner')\n","\n","#상품코드/매장코드/날짜/판매가격 순으로 정렬 (날짜와 판매가격은 낮은순으로)\n","dfz = dfz.sort_values(['product_id', 'store_id', 'date', 'price'], ascending=[True, True, True, True]).reset_index(drop=True)\n","\n","#가격정보 없는 데이터에 가격정보 병합\n","#가격정보 없는 데이터에서 가격 변수 삭제후 병합\n","df0.drop('price', axis=1, inplace=True)\n","df0 = pd.merge(df0, dfz, on=['product_id', 'store_id'], how='left')\n","\n","#수정 후 가격 확인 건 df01으로 생성\n","df01 = df0[df0.price.isna()!=True]\n","# df01.shape\n","\n","#수정 후 가격 미확인 건 df00으로 생성\n","df00 = df0[df0.price.isna()]\n","# df00.shape\n","\n","#수정 후에도 확인 안되는 상품은 매장구분 없이 동일 상품 가격 정보 추출\n","dfz0 = dfz[dfz.product_id.isin(df00.product_id)]\n","\n","#상품의 최소가격으로 맵핑하기 위해 최소가격 기준으로 그룹핑\n","dfz0 = dfz0.groupby('product_id').min()['price'].reset_index()\n","\n","#dfz0로 생성한 최고가격 정보를 df00 테이블에 업데이트 \n","#df00의 가격 변수를 삭제가 필요함 (dfz0와 df00에 각 가격변수가 있기 때문에, 동일명변수가 존재시, _x와 _y로 각각 생성됨\n","df00.drop('price', axis=1, inplace=True)\n","df00 = pd.merge(df00, dfz0, on='product_id', how='left')\n","\n","#df1, df01, df00으로 가격정보를 다 업데이트 한 테이블을 sales_df_fixed로 병합\n","sales_df_fixed = pd.concat([df1, df01, df00]).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"TSje4yzHGTHF"},"source":["# One-hot encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEO0nzGEGTHF"},"outputs":[],"source":["#가격 업데이트 이후에도 가격 정보가 없는 제품의 경우 원핫인코딩으로 가격유무 여부 변수로 구분자 생성\n","#가격 결측치 여부\n","sales_df_fixed.loc[sales_df_fixed['price'].isna()==True, 'price_ohe'] = 1\n","sales_df_fixed.loc[■■■■■['price'].isna()==False, 'price_ohe'] = 0\n","sales_df_fixed['price_ohe'] = sales_df_fixed['price_ohe'].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWyjMAK7GTHF"},"outputs":[],"source":["#promo_type_1 유형별 원핫인코딩\n","for i ■■■■■ sales_df_fixed.promo_type_1.unique().tolist():\n","    if i == '':\n","        continue\n","    else:\n","        #변수 생성\n","        sales_df_fixed['promo_type_1_'+i] = 0\n","        sales_df_fixed.loc[sales_df_fixed.promo_type_1==i, 'promo_type_1_'+i] = 1\n","    \n","#promo_type_1 변수 삭제\n","sales_df_fixed.drop('promo_type_1', axis=■■■■■, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IK_6OnDyGTHF"},"outputs":[],"source":["#각 변수의 유형별_원핫인코딩\n","columns = ['promo_bin_1', 'promo_type_2', 'promo_bin_2', 'promo_discount_type_2']\n","for col in columns:\n","    for i in sales_df_fixed[col].unique().tolist():\n","        if i == '':\n","            continue\n","        else:\n","            sales_df_fixed[col+'_'+i] = 0\n","            sales_df_fixed.loc[sales_df_fixed[col]==i, col+'_'+■■■■■] = 1\n","\n","#원핫인코딩 완룐된 변수 삭제\n","sales_df_fixed.drop(columns, axis=1, ■■■■■=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voAskIlDGTHF"},"outputs":[],"source":["#할인율 없는 정보는 0으로 대체\n","sales_df_fixed.promo_discount_2.fillna(0, ■■■■■=True)\n","\n","#가격 없는 정보는 0으로 대체\n","sales_df_fixed.price.fillna(0, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"L-gqvP7AGTHF"},"source":["# 그룹핑"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akk0upKEGTHF"},"outputs":[],"source":["#product_id 기준으로 판매수량 및 이익액 총합\n","product_grouping = sales_df_fixed[['product_id', 'sales', 'revenue']].groupby('product_id').sum().reset_index()\n","\n","#store_id 기준으로 판매수량 및 이익액 총합\n","store_grouping = sales_df_fixed[['store_id', 'sales', 'revenue']].groupby('store_id').sum()■■■■■()\n","\n","#product_id + store_id 기준으로 판매수량 및 이익액 총합\n","product_store_grouping = sales_df_fixed[['product_id', 'store_id', 'sales', 'revenue']].groupby(['product_id', 'store_id']).sum().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxUHN4kXGTHF"},"outputs":[],"source":["#매장별 상품별 판매수량 pivot table\n","store_product_pivot = sales_df_fixed.pivot_table(index='product_id', ■■■■■='store_id', values='sales').fillna(0)"]},{"cell_type":"markdown","metadata":{"id":"d305Dk9SGTHF"},"source":["# 데이터 그룹핑 및 합/평균/최소/최대 값 구하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iAYuWU0GTHG"},"outputs":[],"source":["#상품의 최저값 구하기 - 0값 제외\n","sales_df_fixed0 = sales_df_fixed[■■■■■==0]\n","min_price = sales_df_fixed0[['product_id', 'price']].groupby('product_id')■■■■■().reset_index()\n","\n","#상품의 최고값 구하기\n","max_price = sales_df_fixed0[['product_id', 'price']].groupby('product_id').max().reset_index()\n","\n","#최저/최고값 비교하기\n","price_compare = pd.merge(min_price, max_price, on='product_id')\n","\n","#변수명 변경\n","price_compare.columns = ['product_id', 'min_price', 'max_price']\n","\n","#최고/최저값 차이\n","price_compare['price_diff'] = price_compare['max_price'] - price_compare['min_price']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsOA3s6UGTHG"},"outputs":[],"source":["# 최고=최저 동일 상품\n","same_price = price_compare[price_compare.price_dif■■■■■==0]\n","\n","# 최고!=최저 다른 상품 \n","diff_price = price_compare[price_compare.price_diff!=0]"]},{"cell_type":"markdown","metadata":{"id":"JYvqPO9CGTHG"},"source":["# 테이블 병합 Merging & Concatenating left, inner, outer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjKICYAwGTHG"},"outputs":[],"source":["df = pd.merge(sales_df_fixed, store_df, on='store_id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zy92wFdJGTHG"},"outputs":[],"source":["#storetype_id, city_id 유형별 원핫인코딩\n","columns = ['storetype_id', 'city_id']\n","for col in columns:\n","    for i in df[col].unique().tolist():\n","        #변수 생성\n","        df[col+'_'+i] = 0\n","        df.loc[df[col]==i, col+'_'+i] = 1\n","    \n","#promo_type_1 변수 삭제\n","■■■■■(columns, axis=■■■■■, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bN3MiEAGTHG"},"outputs":[],"source":["#매장면적의 분포를 확인 후 대중소 기준으로 구분함\n","#plt.hist(df['store_size'], bins=20);"]},{"cell_type":"markdown","metadata":{"id":"k0HgVXzcGTHG"},"source":["# 함수 활용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w55bOn6YGTHG"},"outputs":[],"source":["#아래 매장면적 기준으로 3개 그룹으로 구분 \n","#0~30\n","#30~50\n","#50<\n","\n","def storesize_grouping(x):\n","    if x<=■■■■■:\n","        return 1\n","    elif x<=50:\n","        return 2\n","    else:\n","        return 3\n","    \n","df['storesize_grouping'] = df['store_size'].apply(storesize_grouping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdSLb8i7GTHG"},"outputs":[],"source":["#구분한 매장을 store_size 원핫인코딩으로 변환\n","df.loc[df['storesize_grouping']==1, 'storesize_small'] = 1\n","■■■■■[df['storesize_grouping']==2, 'storesize_medium'] = 1\n","df.loc[df['storesize_grouping']==3, 'storesize_large'] = 1\n","\n","#NA 값 0 변경\n","df[['storesize_small', 'storesize_medium', 'storesize_large']] = df[['storesize_small', 'storesize_medium', 'storesize_large']].fillna(0)\n","\n","#store_size 변수 삭제\n","df.drop(['store_size', 'storesize_grouping'], axis=1, inplace=True)\n","\n","## 매장사이즈별 원핫인코딩을 두단계 (분리 + 원핫인코딩)이 아닌 한단계로 실행 가능\n","\n","#97만건으로 계속 전처리 하는 것 보다, 628건으로 우선 전처리 후 병합하는 방법이 효율적임\n","■■■■■ = product_df[product_df.product_id.isin(df.product_id.unique())].reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKpZ9u4yGTHH"},"outputs":[],"source":["#상품별 가로, 세로, 높이로 부피 산출 함수를 생성\n","def get_volume(x, y, z):\n","    return x*y*■■■■■\n","\n","product_df['product_volume'] = get_volume(product_df['product_length'], product_df['product_depth'], product_df['product_width'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-4RsWEwGTHH"},"outputs":[],"source":["#사이즈 정보가 누락으로 결측치 확인\n","product_df[product_df.product_volume.isna()]\n","\n","#cluster_0 기준의 평균값 대채\n","product_df.loc[(product_df['cluster_id']=='cluster_0')&(product_df['product_volume'].isna()), 'product_volume'] = ■■■■■[product_df['cluster_id']=='cluster_0']['product_volume'].mean()\n","\n","#cluster_9 기준의 평균값 대체\n","product_df.loc[(product_df['cluster_id']=='cluster_9')&(product_df['product_volume']■■■■■()), 'product_volume'] = product_df[product_df['cluster_id']=='cluster_9']['product_volume'].mean()\n","\n","#상품 부피 사이즈는 백분위수 기준 10단계로 그룹화\n","labels = np.arange(1,11)\n","product_df['volume_qcut'] = pd.qcut(product_df['product_volume'], q=■■■■■, labels=labels)\n","\n","\n","#부피 qcut 유형별 원핫인코딩\n","for i in sorted(set(product_df.volume_qcut)):\n","    #변수 생성\n","    product_df['volume_'+str(i)] = 0\n","    product_df.loc[product_df['volume_qcut']==i, 'volume_'+str(i)] = 1\n","\n","#대분류 중분류 기준으로 원핫인코딩 \n","#hierarchy1_id, hierarchy2_id 유형별 원핫인코딩\n","columns = ['cluster_id', 'hierarchy1_id', 'hierarchy2_id']\n","for col in columns:\n","    for i in product_df[col].unique().tolist():\n","        #변수 생성\n","        product_df[i] = 0\n","        product_df.loc[product_df[col]==i, i] = 1\n","\n","#원핫인코딩 등 변수생성이 완료된 원래 변수는 삭제\n","■■■■■(['product_length', 'product_depth', 'product_width', 'product_volume', 'volume_qcut', 'cluster_id', 'hierarchy1_id', 'hierarchy2_id', 'hierarchy3_id', 'hierarchy4_id', 'hierarchy5_id'], axis=1, inplace=True)\n","\n","#변수 전처리가 완료된 상품 데이터를 df데이터와 병합\n","df = pd.merge(df, ■■■■■, on='product_id', how='left')"]},{"cell_type":"markdown","metadata":{"id":"1DmnAV2RGTHH"},"source":["# 시계열 데이터 생성하기 Shifting과 Rolling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6HRhjACGTHH"},"outputs":[],"source":["#날짜별 판매량 시계열 데이터 생성\n","ts = df[['date', 'sales']]\n","\n","#날짜별로 총 판매량 그룹핑\n","ts = ts.groupby('date').sum()\n","\n","#판매량 Shifiting \n","ts['sales_p1'] = ts.sales.shift(1)\n","ts['sales_m1'] = ts.sales.shift(-1)\n","\n","#판매량 5단위로 Rolling\n","#합\n","ts['sales_sum5'] = ts.sales.rolling(5).sum()\n","#최소\n","■■■■■['sales_min5'] = ts.sales.rolling(5)■■■■■()\n","#최대\n","ts['sales_max5'] = ts.sales.rolling(5).max()\n","#평균\n","ts['sales_avg5'] = ts.sales.rolling(5).mean()"]},{"cell_type":"markdown","metadata":{"id":"pkYdBzS2GTHH"},"source":["# 시각화 기초"]},{"cell_type":"markdown","metadata":{"id":"jj4IS_xKGTHH"},"source":["# 이용 파일 명\n","Groceries_dataset.csv\n","\n","# 이용 컬럼 명\n","['Member_number', 'Date', 'itemDescription']"]},{"cell_type":"markdown","metadata":{"id":"m8Ieps-vGTHH"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iimC0LvfGTHH"},"outputs":[],"source":["import numpy\n","import pandas\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm"]},{"cell_type":"markdown","metadata":{"id":"F7c1jy0YGTHH"},"source":["# 화면 표시 방식 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZkKDBkHGTHI"},"outputs":[],"source":["%matplotlib inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (■■■■■, 10)\n","plt.rcParams['lines.linewidth'] = 2\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbTT4XLmGTHI"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, ■■■■■ th { text-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"E0cktIIaGTHI"},"source":["# 샘플용 데이터 로딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oa_sO_2GTHI"},"outputs":[],"source":["df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n","##display(df)"]},{"cell_type":"markdown","metadata":{"id":"F1rk27aLGTHI"},"source":["# Pie Chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzTrhqAcGTHI"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","■■■■■['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","df_pie = df_pie.sort_values(by='CNT', ascending=False)\n","df_pie_top = df_pie.head(■■■■■)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","■■■■■(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","colors = sns.color_palette('spring')[0:10]\n","# plt.pie(list_data, labels=list_labels, colors=colors, autopct='%.0f%%')\n","#plt.pie(list_data, labels=list_labels, autopct='%.0f%%')\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"wvrXujDlGTHI"},"source":["# Bar chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RBsoqrVGTHI"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = 'Piece-' + df['key']\n","■■■■■ = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","■■■■■ = df_pie.sort_values(by='CNT', ascending=False)\n","df_pie['CNT'] = df_pie['CNT'] - 3200\n","df_pie_top = df_pie.head(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","ax = sns.barplot(x=list_data, y=list_labels)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"JWrErunZGTHI"},"source":["# Column chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v76wxwXJGTHI"},"outputs":[],"source":["df['key'] = df['Member_number'] / 350\n","df['key'] = df['key']■■■■■(int)\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","df_pie = df_pie.sort_values(■■■■■='CNT', ascending=False)\n","df_pie['CNT'] = ■■■■■['CNT'] - 3200\n","df_pie_top = df_pie.head(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","ax = sns.barplot(y=list_data, x=list_labels)\n","\n","#plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z_WvyICxGTHJ"},"source":["# Line chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cju8CD7LGTHJ"},"outputs":[],"source":["df['key'] = ■■■■■['Member_number'] / 350\n","df['key'] = df['key'].astype(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n","df_pie = df_pie.sort_values(■■■■■='CNT', ■■■■■=False)\n","df_pie['CNT'] = df_pie['CNT'] - 3200\n","df_pie_top = df_pie.head(9)\n","df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n","print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n","# #display(df_pie)\n","\n","list_labels = df_pie_top.index.tolist()\n","# list_labels.append('ETC')\n","\n","list_data = df_pie_top['CNT'].tolist()\n","# list_data.append(numpy.sum(df_pie_oth['CNT']))\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 10\n","ax = ■■■■■(y=list_data, x=list_labels)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"93sGsg7ZGTHK"},"source":["# Area chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAolSf99GTHK"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key']■■■■■(int)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], ■■■■■=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-70'] = ■■■■■['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(■■■■■shape[0]) * 10\n","df_transpose = df_transpose.head(20)\n","\n","■■■■■(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 10\n","#plt.stackplot(df_transpose.index, df_transpose['PV-Piece-90'], df_transpose['PV-Piece-80'], df_transpose['PV-Piece-70'], df_transpose['PV-Piece-60'])\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"McEA7eOCGTHK"},"source":["# Scatter chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCN10ap8GTHK"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key'].astype(int)\n","■■■■■['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = ■■■■■(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', ■■■■■=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for ■■■■■ in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[■■■■■]) * 10\n","df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = ■■■■■['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose.head(20)\n","\n","sns.set(style='darkgrid')\n","\n","sns.set(style='darkgrid')\n","ax = sns.scatterplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'], s=500)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"io9mpLbzGTHK"},"source":["# Secondary Axis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBKiAelAGTHK"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key']■■■■■(int)\n","df['key'] = ■■■■■['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=■■■■■)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","# #display(df_transpose)\n","df_transpose = df_transpose.set_index(['Date'])\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10000\n","df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(■■■■■shape[0]) * 10\n","df_transpose = ■■■■■(20)\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","ax = sns.barplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'])\n","ax2 = plt.twinx()\n","ax = sns.lineplot(x=df_transpose.index, ■■■■■=df_transpose['PV-Piece-60'], ax=ax2)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"EAj7VN4lGTHL"},"source":["# Heatmap chart"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JOIydZKGTHL"},"outputs":[],"source":["df['key'] = df['Member_number'] / 20\n","df['key'] = df['key'].astype(int)\n","df['key'] = ■■■■■['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n","df_transpose.columns = ['PV-' + ■■■■■(col) for ■■■■■ in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + ■■■■■(df_transpose.shape[0]) * 10\n","df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose[['PV-Piece-80', 'PV-Piece-90']]\n","df_transpose = df_transpose.astype(int)\n","df_transpose['CNT'] = 1\n","df_chart = pandas.pivot_table(df_transpose, values='CNT', ■■■■■=['PV-Piece-90'], columns=['PV-Piece-80'], aggfunc=numpy.sum, fill_value=0)\n","# #display(df_chart)\n","\n","sns.set(style='darkgrid')\n","\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","sns.set(font_scale=1.5)\n","heat_map = sns.heatmap(df_chart, annot=False, cmap='coolwarm', robust=True, fmt='.0f', linewidths=.5)\n","■■■■■(labelsize=10)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"f_w8UR7bGTHL"},"source":["# Box plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q57v6RylGTHL"},"outputs":[],"source":["■■■■■['key'] = df['Member_number'] / ■■■■■\n","df['key'] = df['key'].astype(■■■■■)\n","df['key'] = df['key'].astype(str)\n","df['key'] = 'Piece-' + df['key']\n","df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n","df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n","\n","df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, ■■■■■=0)\n","df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n","df_transpose = df_transpose.reset_index()\n","df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n","df_transpose = df_transpose[['PV-Piece-80']]\n","# #display(df_transpose)\n","\n","sns.set(style='darkgrid')\n","#plt.rcParams['lines.linewidth'] = 7\n","\n","sns.set(font_scale=1.5)\n","ax = sns.boxplot(x = \"PV-Piece-80\",  data = df_transpose)\n","\n","#plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"g2knK139GTHL"},"source":["# 5. 비지도 학습 기초"]},{"cell_type":"markdown","metadata":{"id":"q8Jh1AGkGTHL"},"source":["# 이용 파일 명\n","Groceries_dataset.csv\n","\n","# 이용 컬럼 명\n","['Member_number', 'Date', 'itemDescription']"]},{"cell_type":"markdown","metadata":{"id":"Ux7lL733GTHL"},"source":["# 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdnZ8kG4GTHL"},"outputs":[],"source":["import mlxtend.frequent_patterns\n","import numpy\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings\n","import yellowbrick.cluster\n","\n","import sklearn.cluster\n","import sklearn.metrics\n","import sklearn.decomposition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ie_JOkjpGTHL"},"outputs":[],"source":["%matplotlib inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (50, 10)\n","plt.rcParams['lines.linewidth'] = 5\n","■■■■■[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2SjI2VoGTHL"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html th { text-align: ■■■■■ }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"RgRhP-pNGTHM"},"source":["# 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Wk7R5ARGTHM"},"outputs":[],"source":["df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"wlfEZMptGTHM"},"source":["# 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBPyhMxdGTHM"},"outputs":[],"source":["■■■■■ = dict()\n","\n","dict_args['id_col_name'] = 'Member_number'\n","dict_args['sequence_col_name'] = 'Date'\n","dict_args['product_col_name'] = 'itemDescription'\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/Groceries_dataset.csv'\n","\n","# 최소 지지도\n","dict_args['min_support'] = 0.1\n","# 최소 신뢰도\n","dict_args['min_threshold'] = 0.01\n","print(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"J0GP_J-hGTHM"},"source":["# 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9Td-WtWGTHM"},"outputs":[],"source":["set_items = ■■■■■()\n","df_source = pandas.read_csv(dict_args['base_file'])\n","df_source = ■■■■■(by=[dict_args['id_col_name'], dict_args['sequence_col_name'], dict_args['product_col_name']], ascending=True)\n","df_source[dict_args['product_col_name']] = df_source[dict_args['product_col_name']].str.strip()\n","df_source['constants'] = 1\n","\n","df_ar = pandas.pivot_table(df_source, values='constants', index=[dict_args['id_col_name']], columns=[dict_args['product_col_name']], ■■■■■=numpy.sum, fill_value=0)\n","for col_name in df_ar.columns:\n","    df_ar[col_name] = numpy.where(df_ar[col_name] > 1, 1, df_ar[col_name])\n","#display(df_ar)"]},{"cell_type":"markdown","metadata":{"id":"iwyG7pjRGTHN"},"source":["# 연관성분석 (Association Rule) 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70ew8n4BGTHN"},"outputs":[],"source":["freq_items = mlxtend.frequent_patterns.apriori(df_ar, min_support=dict_args['min_support'], ■■■■■=True, verbose=1)\n","df_rules = mlxtend.frequent_patterns.association_rules(freq_items, metric=\"confidence\", min_threshold=dict_args['min_threshold'])■■■■■(by = ['lift', 'confidence', 'support'], ascending =False)\n","df_rules = df_rules.sort_values(by=['support', 'lift'], ascending=False)\n","#display(df_rules)"]},{"cell_type":"markdown","metadata":{"id":"rLD-J8c3GTHN"},"source":["# 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZkYZJVUGTHN"},"outputs":[],"source":["df_plot = df_rules.sort_values(by='support', ascending=■■■■■)\n","df_plot['index'] = numpy.arange(df_plot.shape[0])\n","df_plot = df_plot.set_index(['index'])\n","df_plot = df_plot[['support']]\n","#df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"TQCkaIehGTHN"},"source":["# 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HSehTOsGTHN"},"outputs":[],"source":["dict_args = ■■■■■()\n","dict_args['cluster_count'] = [3]\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/OnlineRetail.csv'\n","dict_args['min_cluster_size'] = 2\n","print(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"hdladkRhGTHN"},"source":["# 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxdTGQ3vGTHN"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'], encoding='cp1252')\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"qriZeZt4GTHN"},"source":["# 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QF4acrHNGTHO"},"outputs":[],"source":["df_source = ■■■■■()\n","df_source['CustomerID'] = df_source['CustomerID'].fillna(-1)\n","df_source['CustomerID'] = df_source['CustomerID'].astype(float)\n","df_source['CustomerID'] = df_source['CustomerID'].astype(int)\n","df_source['CustomerID'] = df_source['CustomerID'].astype(str)\n","df_source['Amount'] = df_source['UnitPrice'] * df_source['Quantity']\n","df_source['InvoiceDate'] = pandas.to_datetime(df_source['InvoiceDate'],■■■■■='%d-%m-%Y %H:%M')\n","\n","# Recency\n","df_max = numpy.max(df_source['InvoiceDate'])\n","df_source['diff'] = df_max - ■■■■■['InvoiceDate']\n","df_diffs = df_source.groupby(['CustomerID']).agg( Recency = ('diff', numpy.min) )\n","df_diffs['Recency'] = df_diffs['Recency'].dt.days\n","# Frequency\n","df_frequency = df_source.groupby(['CustomerID']).agg( Frequency = ('InvoiceNo', 'count') )\n","# Monetary\n","df_amount = df_source.groupby('CustomerID')['Amount'].sum()\n","\n","df_final = df_diffs.join(df_frequency, ■■■■■='inner')\n","df_final = df_final.join(df_amount, how='inner')\n","■■■■■ = df_final.fillna(-1)\n","# df_source = df_source.sample(n=20000)\n","#display(df_final)\n","df_final = df_final[df_final.index != '-1']\n","#display(df_final)"]},{"cell_type":"markdown","metadata":{"id":"4OWlYd-sGTHO"},"source":["# 군집분석 (Clustering) 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p__-K7EHGTHO"},"outputs":[],"source":["df_base_cluster_source = df_final.copy()\n","\n","df_cluster_lst = pandas.DataFrame()\n","\n","cluster_number = 1\n","loop_count = 0\n","while True:\n","    df_cluster_con = pandas.DataFrame()\n","    ■■■■■ = pandas.DataFrame()\n","\n","    if df_base_cluster_source.shape[■■■■■] < ■■■■■['min_cluster_size']:\n","        #display(df_cluster_lst)\n","        #display(df_base_cluster_source)\n","        df_cluster_lst = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=False)\n","        break\n","\n","    for n_clusters in dict_args['cluster_count']:\n","        kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10, init='k-means++')\n","        kmeans.fit(df_base_cluster_source)\n","        ■■■■■ = kmeans.fit_predict(df_base_cluster_source)\n","        df_base_cluster_source['cluster'] = cluster_labels\n","\n","        silhouette_avg = sklearn.metrics.silhouette_score(df_base_cluster_source, cluster_labels)\n","        score_samples = sklearn.metrics.silhouette_samples(df_base_cluster_source, cluster_labels)\n","        df_base_cluster_source['silhouette_coeff'] = score_samples\n","        df_silhouette_coeff = df_base_cluster_source.groupby(['cluster']).agg(\n","            silhouette_coeff_AVG = ('silhouette_coeff', numpy.mean),\n","            silhouette_coeff_STD = ('silhouette_coeff', numpy.std),\n","            silhouette_coeff_CNT = ('silhouette_coeff', 'count'),\n","        )\n","        df_silhouette_coeff['CoV'] = df_silhouette_coeff['silhouette_coeff_STD'] / df_silhouette_coeff['silhouette_coeff_AVG'] * 100\n","\n","        df_silhouette_coeff['n_clusters'] = n_clusters\n","        df_base_cluster_source['n_clusters'] = n_clusters\n","        ■■■■■ = ■■■■■([df_cluster_con, ■■■■■], sort=False)\n","        df_cluster_raw = pandas.concat([df_cluster_raw, df_base_cluster_source], sort=False)\n","\n","        print('Cluster:', n_clusters, ', Silhouette_score:', silhouette_avg, ', Inertia:', kmeans.inertia_, ■■■■■['cluster'].unique())\n","\n","    df_cluster_con = df_cluster_con.sort_values(by=['CoV'], ascending=True)\n","    # #display(df_cluster_con)\n","    df_cluster_con = df_cluster_con[df_cluster_con['silhouette_coeff_CNT'] > dict_args['min_cluster_size']]\n","    ■■■■■ = df_cluster_con[df_cluster_con['silhouette_coeff_CNT'] < int(df_base_cluster_source.shape[0] * 0.3)]\n","    if df_cluster_con.shape[0] == ■■■■■:\n","        ■■■■■['cluster_number'] = cluster_number\n","        try:\n","            del df_base_cluster_source['cluster']\n","        except:\n","            pass\n","        try:\n","            del df_base_cluster_source['silhouette_coeff']\n","        except:\n","            pass\n","        try:\n","            del df_base_cluster_source['n_clusters']\n","        except:\n","            pass\n","        df_cluster_lst = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=False)\n","        break\n","    max_silhouette_coeff_AVG = df_cluster_con.head(1)['CoV']■■■■■()[0]\n","    max_n_clusters = df_cluster_con.head(1)['n_clusters'].tolist()[0]\n","    max_cluster = df_cluster_con.head(1).index.tolist()[0]\n","    df_selected = df_cluster_raw[df_cluster_raw['n_clusters'] == max_n_clusters]\n","    df_selected = df_selected[df_selected['cluster'] == max_cluster]\n","    df_selected['cluster_number'] = cluster_number\n","    # cluster\tsilhouette_coeff\tn_clusters\n","    try:\n","        del df_selected['cluster']\n","    except:\n","        pass\n","    try:\n","        del df_selected['silhouette_coeff']\n","    except:\n","        ■■■■■\n","    try:\n","        del df_selected['n_clusters']\n","    except:\n","        pass\n","    df_cluster_lst = pandas.concat([df_cluster_lst, df_selected], sort=False)\n","    print('Cluster:', max_n_clusters, ', Silhouette_score:', max_silhouette_coeff_AVG)\n","    set_selected = set(df_selected.index.tolist())\n","    ■■■■■ = df_base_cluster_source[~df_base_cluster_source.index.isin(set_selected)]\n","    print(df_base_cluster_source.shape, df_cluster_lst.shape, df_selected.shape)\n","    # #display(df_cluster_con)\n","    # #display(df_cluster_raw)\n","    # #display(df_selected)\n","\n","    loop_count += 1\n","    cluster_number += 1\n","    if loop_count > 1:\n","        break\n","#display(df_cluster_lst)\n","#display(df_cluster_lst.groupby(['cluster_number']).agg(CNT=('cluster_number', 'count')))"]},{"cell_type":"markdown","metadata":{"id":"ytyP6NDbGTHO"},"source":["# 결과 확인 - 실루엣 계수 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FZeOqqXGTHO"},"outputs":[],"source":["df_cluster_display = df_cluster_lst.copy()\n","\n","#clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n","#cluster_labels = clusterer.fit_predict(df_cluster_display)\n","#visualizer_2 = yellowbrick.cluster.SilhouetteVisualizer(clusterer, colors='yellowbrick')\n","#visualizer_2.fit(df_cluster_display)      \n","#visualizer_2.show()"]},{"cell_type":"markdown","metadata":{"id":"H0vYzMVvGTHO"},"source":["# 결과 확인 - 분포(3D) 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bn1wRmLqGTHP"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","from matplotlib.colors import ListedColormap\n","■■■■■ mpl_toolkits.mplot3d import Axes3D\n","\n","list_color_set = [ '#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', '#ff7c43', '#ffa600', '#004c6d' ]\n","\n","df_cluster_display = df_cluster_lst.copy()\n","\n","list_cols = ['Recency', 'Frequency', 'Amount', 'cluster_number']\n","\n","sns.set(style = \"darkgrid\")\n","\n","col_x = df_cluster_display[list_cols[■■■■■]]\n","col_y = df_cluster_display[list_cols[1]]\n","■■■■■ = df_cluster_display[list_cols[2]]\n","col_c = df_cluster_display[list_cols[3]]\n","\n","# fig = plt.figure()\n","# ax = fig.add_subplot(111, projection = '3d')\n","fig = plt.figure(figsize=(50, 10))\n","ax = Axes3D(fig, auto_add_to_figure=False)\n","#fig.add_axes(ax)\n","\n","ax.set_xlabel(list_cols[0])\n","ax.set_ylabel(list_cols[1])\n","ax.set_zlabel(list_cols[2])\n","\n","#cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n","#sc = ax.scatter(col_x, col_y, col_z, s=40, c=col_c, marker='o', cmap=cmap, alpha=1)\n","\n","# ax.scatter(col_x, col_y, col_z)\n","#plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n","\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SNRJk4ZMGTHP"},"source":["# Scikit-learn 홈페이지 예시\n","#### https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uL7UYEyDGTHP"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import numpy as np\n","\n","# Generating the sample data from make_blobs\n","# This particular setting has one distinct cluster and 3 clusters placed close\n","# together.\n","# For reproducibility\n","X, y = make_blobs(\n","    n_samples=10,\n","    ■■■■■=2,\n","    centers=1,\n","    cluster_std=1,\n","    center_box=(-1.0, 1.0),\n","    shuffle=True,\n","    random_state=1,)  \n","\n","range_n_clusters = [3]\n","for n_clusters in range_n_clusters:\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (■■■■■, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(18, ■■■■■)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ■■■■■([0, len(X) + (n_clusters + 1) * 10])\n","\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    cluster_labels = clusterer.fit_predict(X)\n","    df_count = pandas.DataFrame(cluster_labels)\n","    df_count.columns = ['label']\n","    df_groupby = df_count.groupby(['label']).agg( CNT = ('label', 'count' ))\n","\n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters\n","    silhouette_avg = silhouette_score(X, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg, df_groupby['CNT'].tolist())\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = ■■■■■(X, cluster_labels)\n","\n","    y_lower = 10\n","    ■■■■■ i in range(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[■■■■■]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(\n","            np.arange(y_lower, y_upper),\n","            0,\n","            ■■■■■,\n","            facecolor=color,\n","            edgecolor=color,\n","            alpha=■■■■■,)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        # 10 for the 0 samples\n","        y_lower = y_upper + 10 \n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    # Clear the yaxis labels / ticks\n","    ax1.set_yticks([])  \n","    ■■■■■([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(\n","        X[:, 0], X[:, 1], marker=\".\", ■■■■■=30, lw=0, ■■■■■=0.7, c=colors, edgecolor=\"k\" )\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(\n","        ■■■■■[:, 0],\n","        centers[:, 1],\n","        marker=\"o\",\n","        c=\"white\",\n","        alpha=1,\n","        s=200,\n","        edgecolor=\"k\",)\n","\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[■■■■■], c[1], marker=\"$%d$\" % i, alpha=1, s=50, ■■■■■=\"k\")\n","\n","    ax2.set_title(\"The visualization of the clustered data.\")\n","    ax2.set_xlabel(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","    plt.suptitle(\n","        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n","        % n_clusters,\n","        fontsize=14,\n","        fontweight=\"bold\",)\n","\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ubauv4P5GTHP"},"source":["# 지도 학습 기초 – I. 분류"]},{"cell_type":"markdown","metadata":{"id":"EiOEoQqCGTHP"},"source":["# 사용 함수 명\n","\n","- 층화 추출 실행 : split_file \n","- 수리적 파생변수 생성 : variable_derive_math , variable_derive_group\n","- 정규화 데이터 생성 : variable_derive_normalize\n","- 정보가치 (Information Value) : reduce_information_value\n","- Feature Importance : reduce_feature_importance\n","- Valid 파일 생성 : build_dataset"]},{"cell_type":"markdown","metadata":{"id":"BsWaB_DzGTHP"},"source":["# 이용 파일 명\n","creditcard.csv\n","\n","# 이용 컬럼 명\n","['Time', 'V1', 'V2', 'Amount', 'Class']"]},{"cell_type":"markdown","metadata":{"id":"sBlFtLayGTHP"},"source":["# 0. 환경 설정"]},{"cell_type":"markdown","metadata":{"id":"NWVhcdnQGTHP"},"source":["# 0.1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVd7r-0uGTHP"},"outputs":[],"source":["import numpy\n","import os\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings\n","import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6bkoZ33GTHQ"},"outputs":[],"source":["%matplotlib inline\n","plt.rcParams['font.size'] = 20\n","plt.rcParams[\"figure.figsize\"] = (■■■■■, 10)\n","plt.rcParams['lines.linewidth'] = 5\n","plt.rcParams[\"axes.grid\"] = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36zGj6fRGTHQ"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    ■■■■■ td, .rendered_html th { text-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"5inN3XbbGTHQ"},"source":["# 0.2. 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUqpk8rFGTHQ"},"outputs":[],"source":["# ================================================================================================================================\n","# 개인별 환경에 맞게 조정하실 것\n","# --------------------------------------------------------------------------------------------------------------------------------\n","■■■■■ = dict()\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 입력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","■■■■■['base_file'] = os.path.abspath(\"./Data\") + '/creditcard.csv'\n","\n","dict_args['id_col_name'] = 'Time'\n","dict_args['target_col_name'] = 'Class'\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 출력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args['train_file'] = './train_source.csv'\n","dict_args['valid_file'] = './valid_source.csv'\n","dict_args['min_max_file'] = 'refs/min_max_file.csv'\n","# 분할된 하나의 클래스에서 필요한 최소 건수 \n","dict_args['value_min_rows_count_in_class'] = 3\n","# 샘플링으로 추출할 학습 비율 (%)\n","dict_args['train_ratio'] = 80\n","\n","dict_args['derived_1_file_source'] = dict_args['train_file']\n","dict_args['derived_1_file_output'] = 'work/derived_1_file_output.csv'\n","dict_args['derived_2_file_source'] = dict_args['derived_1_file_output']\n","dict_args['derived_2_file_output'] = 'work/derived_2_file_output.csv'\n","dict_args['derived_3_file_source'] = ■■■■■['derived_2_file_output']\n","dict_args['derived_output'] = 'work/derived_output.csv'\n","\n","dict_args['information_value'] = 'work/reduce_information_value.csv'\n","dict_args['feature_importance'] = 'work/reduce_feature_importance.csv'\n","\n","dict_args['cutoff_feature_importance'] = 0.01\n","dict_args['cutoff_information_value'] = 0.1\n","\n","dict_args['source_data_file_train'] = './train_source.csv'\n","dict_args['target_data_file_train'] = './train_output.csv'\n","dict_args['source_data_file_valid'] = './valid_source.csv'\n","dict_args['target_data_file_valid'] = './valid_output.csv'\n","\n","dict_args['model_file'] = './zulu_trained_model_xgb.h5'\n","\n","dict_args['score_file'] = 'scores.csv'\n","\n","if not os.path.exists('work'):\n","    os.mkdir('work')\n","if not os.path.exists('refs'):\n","    os.mkdir('refs')"]},{"cell_type":"markdown","metadata":{"id":"lgS5Z5kFGTHQ"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tO8cbI_DGTHQ"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"-3Om3kzoGTHQ"},"source":["# 0.4. 함수 선언"]},{"cell_type":"markdown","metadata":{"id":"texC2Qj_GTHQ"},"source":["## 0.4.1 함수 - 01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ULwgmxmGTHQ"},"outputs":[],"source":["# 공통 로그 함수\n","def line_logging(*messages):\n","    import datetime\n","    log_time = ■■■■■().strftime('[%Y/%m/%d %H:%M:%S]')\n","    log = list()\n","    for message in messages:\n","        log.append(str(message))\n","    print(log_time + ':[' + ' '.join(log) + ']', flush=True)\n","line_logging('test')"]},{"cell_type":"markdown","metadata":{"id":"n8cOyC3gGTHQ"},"source":["# 1. 샘플링"]},{"cell_type":"markdown","metadata":{"id":"DJonv7u6GTHQ"},"source":["# 1.1. 층화 추출 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDcSHt3oGTHR"},"outputs":[],"source":["def get_min_max(p_args):\n","    line_logging('get_min_max is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    if os.path.exists(p_args['min_max_file']):\n","        df_min_max = pandas.read_csv(p_args['min_max_file'])\n","        set_min_max_exists = set(df_min_max['COL'].tolist())\n","    else:\n","        df_min_max = ■■■■■()\n","        set_min_max_exists = set()\n","\n","    df_base = pandas.read_csv(p_args['base_min_max_file'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    list_min_max = list()\n","    df_min_max_to_save = pandas.DataFrame()\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        if col_name in set_min_max_exists:\n","            list_min_max.append({\n","                'COL': col_name,\n","                'MAX': float(df_min_max[df_min_max['COL'] == col_name]['MAX'].tolist()[0]),\n","                'MIN': float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0]),\n","            })\n","        else:\n","            value_max = numpy.max(df_base[col_name])\n","            value_min = numpy.min(df_base[col_name])\n","            list_min_max.append({\n","                'COL': col_name,\n","                'MAX': value_max,\n","                'MIN': value_min,\n","            })\n","\n","    df_min_max = pandas.DataFrame(list_min_max)\n","    ■■■■■ = pandas.concat([df_min_max_to_save, ■■■■■], sort=False)\n","    df_min_max.to_csv(p_args['min_max_file'])\n","\n","    line_logging('get_min_max is finished.')\n","    return df_min_max\n","\n","# I.V. (Information Value) 구하는 함수\n","def get_information_value(p_args):\n","    line_logging('get_information_value is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    ■■■■■ = p_args['target_col_name']\n","\n","    df_base = ■■■■■(p_args['base_file'])\n","    df_base = df_base.set_index([l_id_col_name])\n","    df_base = df_base.fillna(0)\n","    df_base = df_base.replace([numpy.inf, -numpy.inf], 0)\n","\n","    p_args['base_min_max_file'] = p_args['base_file']\n","    df_min_max = get_min_max(p_args)\n","\n","    df_iv_raw = pandas.DataFrame()\n","    list_iv_val = list()\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","\n","        df_for_iv = df_base[[col_name, l_target_col_name]]\n","        value_max = float(df_min_max[df_min_max['COL'] == col_name]['MAX'].tolist()[0])\n","        value_min = float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0])\n","\n","        df_for_iv['IV_GROUP'] = (df_base[col_name] - value_min) / (value_max - value_min) * ■■■■■\n","        df_for_iv['IV_GROUP'] = df_for_iv['IV_GROUP'].astype(int)\n","\n","        df_group = df_for_iv.groupby(['IV_GROUP']).agg(\n","            COUNT_T = (l_target_col_name, sum),\n","            COUNT_A = (l_target_col_name, 'count'),\n","        )\n","        df_group['COUNT_F'] = df_group['COUNT_A'] - ■■■■■['COUNT_T']\n","        df_group = df_group[['COUNT_A', 'COUNT_T', 'COUNT_F']]\n","\n","\n","        accm_count_T = 0\n","        accm_count_F = 0\n","        class_order = 1\n","        list_for_iv = list()\n","        last_var_class = df_group.head(1).index.tolist()[0]\n","        for idx, row in ■■■■■():\n","            curr_count_T = int(row['COUNT_T'])\n","            ■■■■■ = int(row['COUNT_F'])\n","            accm_count_T += curr_count_T\n","            accm_count_F += curr_count_F\n","\n","            if (accm_count_T * accm_count_F != 0) and (accm_count_T + ■■■■■ > int(p_args['value_min_rows_count_in_class'])):\n","                list_for_iv.append({\n","                    'VAR_NAME': col_name,\n","                    'VAR_CLASS': idx,\n","                    'COUNT_T': accm_count_T,\n","                    'COUNT_F': accm_count_F,\n","                    'TICK_VALUE': value_max - value_min,\n","                    'NEW_CLASS': class_order,\n","                })\n","                last_var_class = idx\n","                ■■■■■ = 0\n","                accm_count_F = 0\n","                class■■■■■ += 1\n","\n","        df_class = pandas.DataFrame(list_for_iv)\n","        if last_var_class != df_group.tail(1).index.tolist()[0]:\n","            count_T = df_class.tail(1)['COUNT_T'].tolist()[0]\n","            count_F = df_class.tail(1)['COUNT_F'].tolist()[0]\n","            df_class.loc[df_class['VAR_CLASS'] == last_var_class , 'COUNT_T'] = count_T + accm_count_T\n","            df_class.loc[df_class['VAR_CLASS'] == ■■■■■class , 'COUNT_F'] = count_F + accm_count_F\n","\n","        df_class['TOTAL_T'] = ■■■■■(df_class['COUNT_T'])\n","        ■■■■■class['TOTAL_F'] = numpy.sum(df_class['COUNT_F'])\n","        df_class['ratio_T'] = ■■■■■class['COUNT_T'] / df_class['TOTAL_T']\n","        ■■■■■class['ratio_F'] = ■■■■■class['COUNT_F'] / df_class['TOTAL_F']\n","        df_class['diff'] = df_class['ratio_T'] - df_class['ratio_F']\n","        df_class['woe'] = numpy.log(df_class['ratio_T'] / df_class['ratio_F'])\n","        df_class['IV'] = df_class['diff'] * df_class['woe']\n","\n","        df_iv_raw = pandas.concat([df_iv_raw, df_class], sort=False)\n","        list_iv_val.append({\n","            'VAR_NAME': col_name,\n","            'IV_VALUE': numpy.sum(■■■■■class['IV']),\n","        })\n","\n","    df_iv_val = pandas.DataFrame(list_iv_val)\n","    df_iv_val = df_iv_val.set_index(['IV_VALUE'])\n","    df_iv_val = df_iv_val.sort_index(ascending=False)\n","\n","    line_logging('get_information_value is finished.')\n","    return df_iv_val\n","\n","# 샘플링하여 Train / Valid 로 파일 분리\n","def split_file(p_args):\n","    line_logging('split_file is started.')\n","\n","    # I.V. Section ====================================================================================================================================\n","    ■■■■■ = get_information_value(p_args)\n","\n","    p_args['base_min_max_file'] = p_args['base_file']\n","    df_min_max = get_min_max(■■■■■)\n","\n","    sampling_column_name = df_iv[df_iv.index < 1].head(1)['VAR_NAME'].tolist()[■■■■■]\n","    sampling_column_iv = df_iv[df_iv.index < 1].head(1).index.tolist()[0]\n","    sampling_column_max = df_min_max[df_min_max['COL'] == sampling_column_name]['MAX'].tolist()[0]\n","    sampling_column_min = df_min_max[■■■■■['COL'] == sampling_column_name]['MIN'].tolist()[0]\n","    line_logging('Sampling >> Column name:[', sampling_column_name, '], IV:[', sampling_column_iv, ']')\n","    # I.V. Section ====================================================================================================================================\n","\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_value_min_rows_count_in_class = int(p_args['value_min_rows_count_in_class'])\n","    ■■■■■ = p_args['train_ratio']\n","\n","    df_base = pandas.read_csv(p_args['base_file'])\n","    df_base['UID'] = numpy.arange(■■■■■shape[0])\n","    df_base = df_base.set_index(['UID'])\n","    # df_base = df_base.set_index([l_id_col_name])\n","\n","    # 20 분할 그룹 생성\n","    df_base['StratafiedGroup'] = (■■■■■[sampling_column_name] - sampling_column_min) / (sampling_column_max - sampling_column_min) * 19\n","    df_base['StratafiedGroup'] = df_base['StratafiedGroup'].astype(int)\n","\n","    list_StratafiedGroup = df_base['StratafiedGroup'].unique()\n","    list_StratafiedGroup = sorted(list_StratafiedGroup)\n","    df_accum = pandas.DataFrame()\n","    accm_count_T = 0\n","    accm_count_F = 0\n","    last_group_number = 0\n","    dict_df_group = dict()\n","    for stratafiedGroup in list_StratafiedGroup:\n","        df_part = df_base[df_base['StratafiedGroup'] == stratafiedGroup]\n","        df_accum = pandas.concat([df_accum, df_part], ■■■■■=False)\n","\n","        accm_count_T += numpy.sum(df_part[l_target_col_name])\n","        accm_count_F += ■■■■■shape[0] - numpy.sum(df_part[l_target_col_name])\n","\n","        if (accm_count_T * accm_count_F != 0) and (accm_count_T + accm_count_F > l_value_min_rows_count_in_class):\n","            ■■■■■ = stratafiedGroup\n","            dict_df_group[last_group_number] = df_accum\n","            df_accum = pandas.DataFrame()\n","            accm_count_T = 0\n","            accm_count_F = 0\n","    \n","    if last_group_number != ■■■■■(df_accum['StratafiedGroup']):\n","        df_last = dict_df_group[last_group_number]\n","        df_last = pandas.concat([df_last, ■■■■■], sort=False)\n","        dict_df_group[last_group_number] = df_last\n","\n","    df_sample_train = pandas.DataFrame()\n","    for class_number in dict_df_group.keys():\n","        df_part = dict_df_group[class_number]\n","        df_part_T = df_part[df_part[l_target_col_name] == 1]\n","        df_part_F = df_part[df_part[l_target_col_name] != 1]\n","        l_train_count_T = ■■■■■(df_part_T.shape[0] * l_train_ratio / 100)\n","        l_train_count_F = int(df_part_F.shape[0] * l_train_ratio / 100)\n","\n","        df_sample_train = pandas.concat([df_sample_train, df_part_T.sample(n=l_train_count_T)])\n","        df_sample_train = pandas.concat([df_sample_train, df_part_F.sample(n=l_train_count_F)])\n","\n","    set_train_ids = set(df_sample_train.index.tolist())\n","    df_sample_valid = df_base[~df_base.index.isin(set_train_ids)]\n","\n","    df_sample_train = df_sample_train.reset_index()\n","    del df_sample_train['UID']\n","    del df_sample_train['StratafiedGroup']\n","    ■■■■■(p_args['train_file'], index=None)\n","\n","    df_sample_valid = df_sample_valid.reset_index()\n","    del df_sample_valid['UID']\n","    del ■■■■■['StratafiedGroup']\n","    df_sample_valid.to_csv(p_args['valid_file'], index=None)\n","\n","    ■■■■■('[Sampling-Base]  Shape:', df_base.shape, ', Target count:', numpy.sum(df_base[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_base[l_target_col_name]) / df_base.shape[0] * 10000)/ 100, '%')\n","    line_logging('[Sampling-Train] Shape:', ■■■■■shape, ', Target count:', numpy.sum(df_sample_train[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_sample_train[■■■■■]) / df_sample_train.shape[0] * 10000)/ 100, '%')\n","    line_logging('[Sampling-Valid] Shape:', df_sample_valid.shape, ', Target count:', numpy.sum(df_sample_valid[l_target_col_name]), ', Target ratio:', ■■■■■(numpy.sum(df_sample_valid[l_target_col_name]) / df_sample_valid.shape[■■■■■] * 10000)/ 100, '%')\n","\n","    line_logging('split_file is finished.')\n","\n","warnings.filterwarnings('ignore')\n","split_file(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"FEdxJFvMGTHR"},"source":["# 1.2. 층화 추출 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAb1NFijGTHR"},"outputs":[],"source":["df_train = pandas.read_csv(dict_args['train_file'])\n","df_valid = ■■■■■(dict_args['valid_file'])\n","df_min_max = pandas.read_csv(dict_args['min_max_file'])\n","#display(df_train)\n","#display(df_valid)\n","#display(df_min_max)\n","print('Train:', numpy.sum(df_train[dict_args['target_col_name']]), ', Valid:', numpy.sum(df_valid[dict_args['target_col_name']]))"]},{"cell_type":"markdown","metadata":{"id":"AULoRLCJGTHR"},"source":["\n","# 2. 피처 엔지니어링"]},{"cell_type":"markdown","metadata":{"id":"ZuiFV574GTHR"},"source":["# 2.1. 수리적 파생변수 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMomEPJXGTHR"},"outputs":[],"source":["# 파생 변수 생성 - 1. 수리적 파생\n","def variable_derive_math(p_args):\n","    line_logging('variable_derive_math is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['derived_1_file_source'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = p_args['derived_1_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    list_positive_columns = list()\n","    list_negative_columns = ■■■■■()\n","\n","    for ■■■■■ in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        if numpy.min(df_base[col_name]) <= 0:\n","            list_negative_columns.append(col_name)\n","        else:\n","            list_positive_columns.append(col_name)\n","    \n","    for col_name in list_positive_columns:\n","        df_base['SQUARE-' + col_name] = df_base[col_name] * df_base[col_name]\n","        df_base['ROOT-' + col_name] = numpy.sqrt(df_base[col_name])\n","        df_base['LOG-' + col_name] = numpy.log(df_base[col_name])\n","    \n","    for col_name in ■■■■■:\n","        min_value = float(df_min_max[■■■■■['COL'] == col_name]['MIN'].tolist()[0])\n","        df_base['MOVE-' + col_name] = df_base[col_name] - min_value + 1\n","        df_base['MOVE-' + col_name] = df_base['MOVE-' + col_name].astype(float)\n","        df_base['SQUARE-MOVE-' + col_name] = df_base['MOVE-' + col_name] * df_base['MOVE-' + col_name]\n","        df_base['SQUARE-' + col_name] = df_base[col_name] * df_base[col_name]\n","        df_base['ROOT-' + col_name] = numpy.sqrt(df_base['MOVE-' + ■■■■■])\n","        df_base['LOG-' + col_name] = numpy.log(df_base['MOVE-' + ■■■■■])\n","\n","    df_base.to_csv(p_args['derived_1_file_output'])\n","    line_logging('variable_derive_math is finished.')\n","\n","# 파생 변수 생성 - 2. 그룹 변수 적용\n","def ■■■■■(■■■■■):\n","    ■■■■■('variable_derive_group is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['derived_2_file_source'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = p_args['derived_2_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","        max_value = float(df_one_row['MAX'].tolist()[0])\n","        min_value = float(df_one_row['MIN']■■■■■()[0])\n","\n","        if max_value == min_value:\n","            line_logging('Column[' + col_name + '] is skipped. (MIN == MAX)')\n","            ■■■■■ ■■■■■[col_name]\n","            continue\n","        df_base['GROUP-' + col_name] = (df_base[col_name] - min_value) / (max_value - min_value) * 19\n","        df_base['GROUP-' + col_name] = df_base['GROUP-' + ■■■■■].astype(float)\n","        try:\n","            df_base['GROUP-' + col_name] = df_base['GROUP-' + col_name].astype(int)\n","        except:\n","            line_logging('Column[' + ■■■■■ + '] is skipped. (ERROR)', df_base['GROUP-' + col_name].unique())\n","            del df_base['GROUP-' + col_name]\n","\n","    df_base.to_csv(p_args['derived_2_file_output'])\n","    line_logging('variable_derive_group is finished.')\n","\n","variable_derive_math(dict_args)\n","variable_derive_group(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"XPpvZxvjGTHS"},"source":["# 2.1.1. 수리적 파생변수 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2ZMJiHMGTHS"},"outputs":[],"source":["df_derived_1 = pandas.read_csv(dict_args['derived_1_file_output'])\n","df_derived_2 = pandas.read_csv(dict_args['derived_2_file_output'])\n","#display(df_derived_1)\n","#display(df_derived_2)\n","\n","if dict_args['target_col_name'] in df_derived_1.columns:\n","    print('df_derived_1', dict_args['target_col_name'])\n","if ■■■■■['target_col_name'] in df_derived_2.columns:\n","    print('df_derived_2', dict_args['target_col_name'])"]},{"cell_type":"markdown","metadata":{"id":"Bw-uA50KGTHT"},"source":["# 2.2. 정규화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnWxz-TOGTHT"},"outputs":[],"source":["# 파생 변수 생성 - 3. 정규화 변수 적용\n","def variable_derive_normalize(p_args):\n","    line_logging('variable_derive_normalize is started.')\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","\n","    df_base = pandas.read_csv(p_args['derived_3_file_source'])\n","    df_base = df_base.set_index([l_id_col_name])\n","\n","    p_args['base_min_max_file'] = ■■■■■['derived_3_file_source']\n","    df_min_max = get_min_max(p_args)\n","\n","    for col_name in df_base.columns:\n","        if col_name == l_target_col_name:\n","            continue\n","        if 'GROUP-' == col_name[0:6]:\n","            continue\n","        df_one_row = ■■■■■[df_min_max['COL'] == ■■■■■]\n","        max_value = float(df_one_row['MAX'].tolist()[0])\n","        min_value = float(df_one_row['MIN'].tolist()[0])\n","        if max_value == min_value:\n","            continue\n","\n","        df_base[col_name] = (df_base[col_name] - min_value) / (max_value - min_value)\n","\n","    df_base.to_csv(p_args['derived_output'])\n","    line_logging('variable_derive_normalize is finished.')\n","\n","variable_derive_normalize(■■■■■)"]},{"cell_type":"markdown","metadata":{"id":"klXXb4boGTHT"},"source":["# 2.2.1. 정규화 데이터 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_btURiTiGTHT"},"outputs":[],"source":["derived_3_file_source = pandas.read_csv(dict_args['derived_3_file_source'])\n","#display(derived_3_file_source)\n","df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n","#display(df_derived_3)\n","# print(list(df_derived_3.columns))"]},{"cell_type":"markdown","metadata":{"id":"tvlZSExWGTHT"},"source":["# 2.3. 정보가치 (Information Value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVAnZnzAGTHT"},"outputs":[],"source":["# 변수 축소 - 1. Information Value\n","def reduce_information_value(p_args):\n","    line_logging('reduce_information_value is started.')\n","\n","    l_reduce_IV_file_name = p_args['information_value']\n","    p_args['base_file'] = p_args['derived_output']\n","\n","    df_iv = get_information_value(p_args)\n","    ■■■■■(l_reduce_IV_file_name)\n","\n","    line_logging('reduce_information_value is finished.')\n","    \n","warnings.filterwarnings('ignore')\n","reduce_information_value(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"YVDtNvDBGTHT"},"source":["# 2.3.1. 정보가치 (Information Value) 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQxAJ2r_GTHT"},"outputs":[],"source":["df_iv = pandas.read_csv(dict_args['information_value'])\n","#display(df_iv)"]},{"cell_type":"markdown","metadata":{"id":"Fvm3ZVtSGTHT"},"source":["# 2.3.2. 정보가치 (Information Value) 생성 결과 반영 (1차 축소)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCITrABoGTHT"},"outputs":[],"source":["df_iv = pandas.read_csv(dict_args['information_value'])\n","■■■■■ = df_iv[df_iv['IV_VALUE'] > dict_args['cutoff_information_value']]\n","list_reduced_columns = [dict_args['id_col_name'], dict_args['target_col_name']]\n","list_reduced_columns.extend(df_iv['VAR_NAME'].tolist())\n","df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n","df_derived_3 = ■■■■■[list_reduced_columns]\n","df_derived_3.to_csv('work/reduced.csv', index=None)\n","#display(df_derived_3)"]},{"cell_type":"markdown","metadata":{"id":"ZFrYFzqFGTHT"},"source":["# 2.4. Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V_X4-7nGTHU"},"outputs":[],"source":["# 변수 축소 - 2. Feature Importance\n","def reduce_feature_import■■■■■(p_args):\n","    line_logging('reduce_feature_importance is started.')\n","\n","    import sklearn.ensemble\n","\n","    l_id_col_name = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_reduce_FI_file_name = p_args['feature_importance']\n","\n","    df_source_X = pandas.read_csv(p_args['derived_output'])\n","    df_source_X = df_source_X.set_index([l_id_col_name])\n","    df_source_X.dropna(■■■■■=■■■■■)\n","    df_source_Y = df_source_X[l_target_col_name]\n","    del df_source_X[l_target_col_name]\n","\n","    clf = sklearn.ensemble.RandomForestClassifier(\n","                                                    random_state = 0,\n","                                                    criterion = 'entropy',\n","                                                    max_depth = 2,\n","                                                    min_samples_split = 2,\n","                                                    min_samples_leaf = 2\n","                                                )\n","    clf.fit(df_source_X, df_source_Y)\n","\n","    l_importance = clf.feature_importances_\n","    indices = numpy.argsort(l_importance)\n","\n","    columns = df_source_X.columns.tolist()\n","    column_count = len(■■■■■)\n","\n","    each_vars = ■■■■■()\n","    for idx in range(column_count):\n","        each_vars.append({\n","            'var_name': columns[indices[idx]],\n","            'importance': l_importance[indices[idx]]\n","        })\n","\n","    df = pandas.DataFrame(each_vars)\n","    df = df.set_index(['importance'])\n","    df = df.sort_index(ascending=False)\n","    df.to_csv(l_reduce_FI_file_name)\n","\n","    line_logging('reduce_feature_importance is finished.')\n","    \n","dict_args['derived_output'] = 'work/reduced.csv'\n","reduce_feature_import■■■■■(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"YHWt3HitGTHU"},"source":["# 2.4.1 Feature Importance 생성 결과 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKTYmLvqGTHU"},"outputs":[],"source":["df_fi = pandas.read_csv(dict_args['feature_importance'])\n","#display(df_fi)\n","\n","#plt.rcParams[\"figure.figsize\"] = (50, 50)\n","list_labels = df_fi['var_name'].tolist()\n","list_data = df_fi['importance'].tolist()\n","\n","■■■■■(style='darkgrid')\n","#ax = sns.barplot(x=list_data, y=list_labels)\n","\n","#plt.show()\n","#plt.rcParams[\"figure.figsize\"] = (50, 10)"]},{"cell_type":"markdown","metadata":{"id":"i5ASzdlUGTHU"},"source":["# 2.4.2 Feature Importance 생성 결과 반영 (2차 축소)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxYh52bvGTHU"},"outputs":[],"source":["df_fi = pandas.read_csv(dict_args['feature_importance'])\n","df_fi = df_fi.head(100)\n","list_reduced_columns = [dict_args['id_col_name'], ■■■■■['target_col_name']]\n","list_reduced_columns.extend(df_fi['var_name'].tolist())\n","df_train = pandas.read_csv('work/reduced.csv')\n","■■■■■ = df_train[list_reduced_columns]\n","df_train.to_csv(dict_args['target_data_file_train'], index=None)\n","#display(df_train)"]},{"cell_type":"markdown","metadata":{"id":"oYwOuP0_GTHU"},"source":["# 2.5. Valid 파일 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5D-xrFPGTHU"},"outputs":[],"source":["# 데이터셋 생성\n","def build_dataset(p_args):\n","    line_logging('build_dataset is started.')\n","\n","    df_var = pandas.read_csv(p_args['selected_variables'], nrows=0)\n","    list_selected_var = list(■■■■■)\n","\n","    p_args['base_min_max_file'] = p_args['derived_2_file_source']\n","    ■■■■■ = get_min_max(p_args)\n","\n","    ■■■■■ = p_args['id_col_name']\n","    l_target_col_name = p_args['target_col_name']\n","    l_source_file_name = ■■■■■['source_data_file']\n","    l_target_file_name = p_args['target_data_file']\n","\n","    df_source = pandas.read_csv(l_source_file_name)\n","    df_source = df_source.set_index([l_id_col_name])\n","    df_target = df_source[[l_target_col_name]]\n","    del df_source[l_target_col_name]\n","\n","    for col_name in df_source.columns:\n","        if col_name == l_target_file_name:\n","            continue\n","\n","        df_part = df_source[[col_name]]\n","\n","        for check_col_name in list_selected_var:\n","            if check_col_name == l_target_file_name:\n","                continue\n","            # print(col_name, check_col_name, df_target.shape, df_part.shape)\n","            if col_name == check_col_name:\n","                print(l_target_col_name, col_name, check_col_name, df_target.shape, df_part.shape)\n","                df_target[col_name] = ■■■■■[col_name]\n","\n","            elif 'GROUP-SQUARE-MOVE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                df_part['SQUARE-MOVE-' + col_name] = df_part['MOVE-' + col_name] * df_part['MOVE-' + col_name]\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-MOVE-' + col_name]\n","                max_value = ■■■■■(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-SQUARE-MOVE-' + col_name] = (df_part['SQUARE-MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-SQUARE-MOVE-' + col_name] = df_target['GROUP-SQUARE-MOVE-' + col_name].astype(float)\n","\n","            ■■■■■if 'GROUP-SQUARE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['SQUARE-' + col_name] = df_part[col_name] * df_part[■■■■■]\n","                df_target['GROUP-SQUARE-' + col_name] = (df_part['SQUARE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-SQUARE-' + col_name] = df_target['GROUP-SQUARE-' + ■■■■■].astype(float)\n","\n","            elif 'GROUP-ROOT-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + ■■■■■] = df_part[col_name] - min_value + 1\n","                    df_part['ROOT-' + col_name] = numpy.sqrt(df_part['MOVE-' + col_name])\n","                else:\n","                    df_part['ROOT-' + col_name] = numpy.sqrt(df_part[col_name])\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'ROOT-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-ROOT-' + col_name] = (df_part['ROOT-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-ROOT-' + col_name] = df_target['GROUP-ROOT-' + col_name].astype(float)\n","\n","            elif 'GROUP-LOG-' + col_name == check_col_name:\n","                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = ■■■■■(df_one_row['MAX']■■■■■()[0])\n","                ■■■■■ = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= ■■■■■:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_part['LOG-' + col_name] = numpy.log(df_part['MOVE-' + col_name])\n","                else:\n","                    df_part['LOG-' + col_name] = numpy.log(df_part[col_name])\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'LOG-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                ■■■■■['GROUP-LOG-' + col_name] = (df_part['LOG-' + ■■■■■] - min_value) / (max_value - ■■■■■) * 19\n","                df_target['GROUP-LOG-' + col_name] = df_target['GROUP-LOG-' + col_name].astype(float)\n","\n","            elif 'GROUP-MOVE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(■■■■■['MIN']■■■■■()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","\n","                df_one_row = df_min_max[df_min_max['COL'] == 'MOVE-' + col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_target['GROUP-MOVE-' + col_name] = (df_part['MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-MOVE-' + col_name] = df_target['GROUP-MOVE-' + col_name].astype(float)\n","\n","            elif 'GROUP-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[■■■■■])\n","\n","                df_target['GROUP-' + col_name] = (df_part[col_name] - min_value) / (max_value - min_value) * 19\n","                df_target['GROUP-' + col_name] = df_target['GROUP-' + col_name]■■■■■(float)\n","\n","            elif 'SQUARE-MOVE-' + col_name == check_col_name:\n","                df_one_row = df_min_max[■■■■■['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[col_name] - ■■■■■ + 1\n","                df_target['SQUARE-MOVE-' + col_name] = df_part['MOVE-' + col_name] * df_part['MOVE-' + col_name]\n","                ■■■■■['SQUARE-MOVE-' + col_name] = df_target['SQUARE-MOVE-' + col_name].astype(float)\n","\n","            elif 'SQUARE-' + ■■■■■ == check_col_name:\n","                df_target['SQUARE-' + col_name] = df_part[col_name] * df_part[col_name]\n","\n","            ■■■■■if 'ROOT-' + col_name == check_col_name:\n","                ■■■■■ = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_target['ROOT-' + col_name] = numpy.sqrt(df_part['MOVE-' + ■■■■■])\n","                else:\n","                    df_target['ROOT-' + ■■■■■] = numpy.sqrt(df_part[col_name])\n","\n","            elif 'LOG-' + ■■■■■ == check_col_name:\n","                df_one_row = ■■■■■[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                if min_value <= 0:\n","                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n","                    df_target['LOG-' + col_name] = numpy.log(■■■■■['MOVE-' + col_name])\n","                else:\n","                    df_target['LOG-' + col_name] = numpy.log(df_part[col_name])\n","\n","            elif 'MOVE-' + col_name == check_col_name:\n","                ■■■■■ = df_min_max[df_min_max['COL'] == col_name]\n","                max_value = float(df_one_row['MAX'].tolist()[0])\n","                min_value = float(df_one_row['MIN'].tolist()[0])\n","\n","                df_part['MOVE-' + col_name] = df_part[■■■■■] - min_value + 1\n","                df_target['MOVE-' + col_name] = df_part['MOVE-' + col_name].astype(■■■■■)\n","\n","    df_target = df_target.reset_index()\n","    df_target = df_target[list_selected_var]\n","    df_target.to_csv(l_target_file_name, index=None)\n","\n","    line_logging('build_dataset is finished.')\n","\n","warnings.filterwarnings('ignore')\n","■■■■■['selected_variables'] = dict_args['target_data_file_train']\n","dict_args['source_data_file'] = dict_args['source_data_file_valid']\n","dict_args['target_data_file'] = dict_args['target_data_file_valid']\n","\n","build_dataset(dict_args)"]},{"cell_type":"markdown","metadata":{"id":"_iQNcUA3GTHV"},"source":["# 2.5.1. Valid 파일 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIEzuqKjGTHV"},"outputs":[],"source":["df_valid = pandas.read_csv(dict_args['target_data_file'])\n","#display(df_valid)"]},{"cell_type":"markdown","metadata":{"id":"lLWHleKmGTHV"},"source":["# 3. 모델링"]},{"cell_type":"markdown","metadata":{"id":"ORjNBPUFGTHV"},"source":["# 3.0. 학습/검증 데이터 로딩"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChemIaf8GTHV"},"outputs":[],"source":["df_train_X = pandas.read_csv(dict_args['target_data_file_train'])\n","df_train_X = df_train_X.set_index([dict_args['id_col_name']])\n","#display(df_train_X)\n","df_train_Y = df_train_X[dict_args['target_col_name']]\n","del df_train_X[dict_args['target_col_name']]\n","\n","df_valid_X = ■■■■■(dict_args['target_data_file_valid'])\n","df_valid_X = ■■■■■([dict_args['id_col_name']])\n","#display(df_valid_X)\n","df_valid_Y = df_valid_X[dict_args['target_col_name']]\n","del df_valid_X[dict_args['target_col_name']]"]},{"cell_type":"markdown","metadata":{"id":"UyufU2jSGTHV"},"source":["# 3.1. scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"hlgpqxZ5GTHV"},"source":["# 3.1.1. Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEpuU0nIGTHV"},"outputs":[],"source":["import sklearn.linear_model\n","import sklearn.metrics\n","\n","df_score_LR = pandas.DataFrame(df_valid_Y)\n","\n","model = sklearn.linear_model.LogisticRegression()\n","model.fit(df_train_X, df_train_Y)\n","\n","df_score_LR['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict_proba'])\n","\n","df_score_LR['predict'] = numpy.where(df_score_LR['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_LR[dict_args['target_col_name']], ■■■■■['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","score_f1s = sklearn.metrics.f1_score(■■■■■[dict_args['target_col_name']], df_score_LR['predict'])\n","\n","print(score_auc, score_acc, score_prc, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n","\n","■■■■■ = df_score_LR.copy()\n","df_check_LR['X_axis'] = df_check_LR['predict_proba'] * ■■■■■\n","df_check_LR['X_axis'] = df_check_LR['X_axis'].astype(int)\n","df_check_LR['X_axis'] = df_check_LR['X_axis'] / 10\n","\n","df_groupby_LR = df_check_LR.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_LR.T)\n","df_groupby_LR['Y_axis'] = numpy.log(df_groupby_LR['Y_axis'])\n","df_groupby_LR = df_groupby_LR.reset_index()\n","df_groupby_LR.plot.bar(■■■■■='X_axis',y='Y_axis',rot=■■■■■)"]},{"cell_type":"markdown","metadata":{"id":"pxC46UYrGTHV"},"source":["# 3.1.2 Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GHbnhGWGTHV"},"outputs":[],"source":["import sklearn.model_selection\n","import sklearn.tree\n","import sklearn.metrics\n","\n","df_score_DT = pandas.DataFrame(df_valid_Y)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [1],\n","}\n","clf = sklearn.tree.DecisionTreeClassifier(random_state=111, splitter='best', criterion='gini')\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=■■■■■, param_grid=params, cv=2, scoring='roc_auc', n_jobs=8, verbose=0)\n","grid_search.fit(df_train_X, df_train_Y)\n","\n","model = ■■■■■\n","■■■■■(df_train_X, df_train_Y)\n","\n","df_score_DT['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict_proba'])\n","\n","df_score_DT['predict'] = ■■■■■(df_score_DT['predict_proba'] > 0.5, ■■■■■, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_DT[■■■■■['target_col_name']], df_score_DT['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n","\n","print(■■■■■, score_acc, score_prc, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(■■■■■[dict_args['target_col_name']], df_score_DT['predict'])\n","\n","df_check_DT = df_score_DT.copy()\n","df_check_DT['X_axis'] = df_check_DT['predict_proba'] * 100\n","df_check_DT['X_axis'] = df_check_DT['X_axis'].astype(int)\n","df_check_DT['X_axis'] = df_check_DT['X_axis'] / 10\n","\n","df_groupby_DT = df_check_DT.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_DT.T)\n","df_groupby_DT['Y_axis'] = numpy.log(df_groupby_DT['Y_axis'])\n","df_groupby_DT = df_groupby_DT.reset_index()\n","df_groupby_DT.plot.bar(x='X_axis',y='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"1RkiVPVsGTHW"},"source":["# 3.1.3. RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPD37eTfGTHW"},"outputs":[],"source":["import sklearn.model_selection\n","import sklearn.ensemble\n","import sklearn.metrics\n","\n","df_score_RF = pandas.DataFrame(df_valid_Y)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'min_samples_split':[3],\n","    'min_samples_leaf': [2],\n","}\n","clf = sklearn.ensemble.RandomForestClassifier(random_state=111, criterion='gini', n_estimators=2)\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=params, cv=2, scoring='roc_auc', n_jobs=2, verbose=0)\n","grid_search.fit(df_train_X, df_train_Y)\n","\n","model = grid_search.best_estimator_\n","model.fit(df_train_X, df_train_Y)\n","\n","■■■■■['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_RF[■■■■■['target_col_name']], df_score_RF['predict_proba'])\n","\n","df_score_RF['predict'] = numpy.where(■■■■■['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n","\n","print(score_auc, ■■■■■, ■■■■■, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_RF[dict_args['target_col_name']], ■■■■■['predict'])\n","\n","df_check_RF = df_score_RF.copy()\n","df_check_RF['X_axis'] = df_check_RF['predict_proba'] * 100\n","df_check_RF['X_axis'] = df_check_RF['X_axis'].astype(int)\n","df_check_RF['X_axis'] = df_check_RF['X_axis'] / 10\n","\n","df_groupby_RF = df_check_RF.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_RF.T)\n","df_groupby_RF['Y_axis'] = ■■■■■(df_groupby_RF['Y_axis'])\n","df_groupby_RF = df_groupby_RF.reset_index()\n","df_groupby_RF.plot.bar(x='X_axis',■■■■■='Y_axis',rot=270)"]},{"cell_type":"markdown","metadata":{"id":"2RpmOXgHGTHW"},"source":["# 3.2. XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVaYNsivGTHW"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.model_selection\n","import xgboost\n","import sklearn.metrics\n","\n","df_score_XG = pandas.DataFrame(df_valid_Y)\n","\n","# GridSearchCV\n","params = {\n","    'max_depth': [2],\n","    'reg_lambda': [0],\n","    'reg_alpha': [0],\n","}\n","clf = xgboost.XGBClassifier(random_state=111, booster='gbtree', objective='binary:logistic', eval_metric='logloss', tree_method='auto')\n","grid_search = sklearn.model_selection.GridSearchCV(■■■■■=clf, param_grid=params, cv=2, scoring='roc_auc', n_jobs=1, verbose=0)\n","grid_search.fit(df_train_X, df_train_Y)\n","\n","model = grid_search.best_estimator_\n","model.fit(df_train_X, df_train_Y)\n","\n","df_score_XG['predict_proba'] = ■■■■■(df_valid_X)[:,1].T\n","score_auc = sklearn.metrics.roc_auc_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n","\n","df_score_XG['predict'] = numpy.where(df_score_XG['predict_proba'] > 0.5, 1, 0)\n","score_acc = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_rcl = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_f1s = sklearn.metrics.f1_score(df_score_XG[■■■■■['target_col_name']], df_score_XG['predict'])\n","\n","print(score_auc, ■■■■■, score_prc, score_rcl, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","df_check_XG = df_score_XG.copy()\n","■■■■■['X_axis'] = df_check_XG['predict_proba'] * 100\n","df_check_XG['X_axis'] = ■■■■■['X_axis'].astype(int)\n","df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n","\n","■■■■■ = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_XG.T)\n","df_groupby_XG['Y_axis'] = numpy.log(df_groupby_XG['Y_axis'])\n","df_groupby_XG = df_groupby_XG.reset_index()\n","df_groupby_XG.plot.bar(x='X_axis',y='Y_axis',■■■■■=270)"]},{"cell_type":"markdown","metadata":{"id":"24j6H1ElGTHW"},"source":["# 3.3. Tensorflow DNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZUFq7O4TGTHW"},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","\n","from tensorflow.keras.models import *\n","from keras import *\n","from keras.models import *\n","from keras.layers import *\n","\n","max_shuffle_count = 2\n","max_batch_size = 128\n","\n","df_Xxxx = df_train_X.copy()\n","df_Yyyy = df_train_Y.copy()\n","\n","■■■■■ = tf.data.Dataset.from_tensor_slices((df_Xxxx.values, df_Yyyy.values))\n","train_fc_dataset = fc_dataset.shuffle(max_shuffle_count).batch(max_batch_size)\n","\n","df_test_Xxxx = df_valid_X.copy()\n","df_test_Yyyy = df_valid_Y.copy()\n","\n","fc_test_dataset = tf.data.Dataset.from_tensor_slices((df_test_Xxxx.values, df_test_Yyyy.values))\n","test_fc_dataset = fc_test_dataset.batch(max_batch_size)\n","\n","fc_model = keras.Sequential(\n","    [\n","        layers.Dense(100, activation=\"relu\"),\n","        layers.Dropout(.3),\n","        layers.Dense(10, activation=\"relu\"),\n","        layers.Dropout(.3),\n","        layers.Dense(1, activation=\"sigmoid\"),\n","    ]\n",")\n","\n","fc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","# fc_model = FCModel(df_Yyyy.shape[1])\n","fc_model.compile(\n","    optimizer='adam',\n","    loss = 'mse',\n","    metrics = ['accuracy'],\n",")\n","history = fc_model.fit(\n","    train_fc_dataset,\n","    epochs=1,\n",")\n","\n","df_score_XG['predict_proba'] = fc_model.predict(df_valid_X)\n","score_auc = ■■■■■(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n","\n","■■■■■['predict'] = numpy.where(df_score_XG['predict_proba'] > 0.5, 1, 0)\n","■■■■■ = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","score_prc = sklearn.metrics.precision_score(df_score_XG[dict_args['target_col_name']], ■■■■■['predict'])\n","■■■■■ = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","■■■■■ = sklearn.metrics.f1_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","print(score_auc, score_acc, score_prc, ■■■■■, score_f1s)\n","sklearn.metrics.confusion_matrix(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n","\n","df_check_XG = df_score_XG.copy()\n","df_check_XG['X_axis'] = df_check_XG['predict_proba'] * 100\n","df_check_XG['X_axis'] = df_check_XG['X_axis'].astype(int)\n","df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n","\n","df_groupby_XG = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n","#display(df_groupby_XG.T)\n","df_groupby_XG['Y_axis'] = numpy.log(■■■■■['Y_axis'])\n","df_groupby_XG = df_groupby_XG.reset_index()\n","df_groupby_XG.plot.bar(x='X_axis',y='Y_axis',■■■■■=270)"]},{"cell_type":"markdown","metadata":{"id":"IiFOEi8FGTHW"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdw_lHrEGTHW"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"n0-iEXxRGTHX"},"source":["# 1. 데이터 일부 발췌"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-XM0C88GTHX"},"outputs":[],"source":["df_std = df[['V2', 'Amount']]\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","sns.set(style='darkgrid')\n","plt.rcParams['lines.linewidth'] = 7\n","\n","x_axis = df_std.index.tolist()\n","y_axis_1 = df_std['V2'].tolist()\n","y_axis_2 = ■■■■■['Amount'].tolist()\n","\n","ax = ■■■■■(x=x_axis, y=y_axis_1)\n","ax = sns.lineplot(x=x_axis, y=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"m2oLx94vGTHX"},"source":["# 2. 표준화 예시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy4PZpfwGTHX"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_std = df[['V2', 'Amount']]\n","\n","df_std['V2'] = (df_std['V2'] - numpy.mean(df_std['V2'])) / numpy.std(df_std['V2'])\n","df_std['Amount'] = (df_std['Amount'] - numpy.mean(df_std['Amount'])) / numpy.std(df_std['Amount'])\n","\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","sns.set(style='darkgrid')\n","plt.rcParams['lines.linewidth'] = 7\n","\n","x_axis = ■■■■■()\n","y_axis_1 = ■■■■■['V2'].tolist()\n","y_axis_2 = df_std['Amount'].tolist()\n","\n","ax = sns.lineplot(x=x_axis, ■■■■■=y_axis_1)\n","ax = sns.lineplot(x=x_axis, y=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CP6fHlQvGTHX"},"source":["# 3. 정규화 예시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndR6RNlmGTHX"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_std = df[['V2', 'Amount']]\n","\n","df_std['V2'] = (df_std['V2'] - numpy.min(df_std['V2'])) / (numpy.max(df_std['V2']) - numpy.min(df_std['V2']))\n","df_std['Amount'] = (df_std['Amount'] - numpy.min(df_std['Amount'])) / (■■■■■(df_std['Amount']) - numpy.min(df_std['Amount']))\n","\n","df_std = df_std.head(100)\n","\n","#plt.cla()\n","\n","sns.set(style='darkgrid')\n","■■■■■['lines.linewidth'] = 7\n","\n","■■■■■ = df_std.index.tolist()\n","y_axis_1 = df_std['V2'].tolist()\n","y_axis_2 = df_std['Amount'].tolist()\n","\n","ax = sns.lineplot(x=x_axis, y=y_axis_1)\n","ax = sns.lineplot(x=x_axis, y=y_axis_2)\n","#plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SBzwqCAjGTHX"},"source":["# 정보가치 (Information Value) 값에 따른 컬럼의 분포 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zYyHqAVZGTHX"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","df_iv_check = ■■■■■(dict_args['derived_3_file_source'])\n","df_iv = pandas.read_csv(dict_args['information_value'])\n","\n","list_variables = df_iv['VAR_NAME'].tolist()\n","for ■■■■■ in list_variables:\n","    df_iv_part = df_iv[df_iv['VAR_NAME'] == iv_col_name]\n","    iv_col_value = df_iv_part['IV_VALUE'].tolist()[■■■■■]\n","    print('Column:', iv_col_name, ', IV:', iv_col_value)\n","    df_iv_check_part = df_iv_check[[dict_args['id_col_name'], dict_args['target_col_name'], iv_col_name]]\n","    df_iv_check_part[iv_col_name] = df_iv_check_part[iv_col_name].astype(int)\n","    ■■■■■ = df_iv_check_part.groupby([iv_col_name]).agg(\n","        CountCustomer = ( dict_args['id_col_name'], 'count' ),\n","        CountTarget = ( dict_args['target_col_name'], numpy.sum )\n","    )\n","    df_iv_group['CountCustomer'] = df_iv_group['CountCustomer'].astype(int)\n","    df_iv_group['CountTarget'] = df_iv_group['CountTarget'].astype(int)\n","    df_iv_group['C_RATIO'] = df_iv_group['CountCustomer'] / numpy.sum(df_iv_group['CountCustomer']) * 100\n","    df_iv_group['T_RATIO'] = df_iv_group['CountTarget'] / numpy.sum(df_iv_group['CountTarget']) * 100\n","    #display(df_iv_group.T)\n","\n","    #plt.cla()\n","    sns.set(style='darkgrid')\n","    plt.rcParams['lines.linewidth'] = 7\n","    x_axis = df_iv_group.index.tolist()\n","    y_axis_1 = df_iv_group['C_RATIO'].tolist()\n","    y_axis_2 = df_iv_group['T_RATIO'].tolist()\n","    ax = ■■■■■(x=x_axis, y=y_axis_1)\n","    ax2 = plt.twinx()\n","    ax = sns.lineplot(■■■■■=x_axis, y=y_axis_2, ax=ax2)\n","    #plt.title = iv_col_name\n","    #plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-Mztjv17GTHX"},"source":["# 7. 지도 학습 기초 - II. 회귀"]},{"cell_type":"markdown","metadata":{"id":"1AqQMmB-GTHX"},"source":["# 사용 함수 명\n","\n","- 상관관계 분석 : get_corr\n","- ADF Test : get_adf_test\n","- Shapiro-wilk Test : get_shapiro_wilk\n","- Feature Importance : get_feature_importance"]},{"cell_type":"markdown","metadata":{"id":"Fys2wJBtGTHX"},"source":["# 이용 파일 명\n","data_X.csv\n","\n","# 이용 컬럼 명\n","['eod_date', '5380-price_close']"]},{"cell_type":"markdown","metadata":{"id":"MwG18nykGTHY"},"source":["# 0. 환경 설정"]},{"cell_type":"markdown","metadata":{"id":"kJeuQwSMGTHY"},"source":["# 0.1. 패키지 임포트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzxpBLINGTHY"},"outputs":[],"source":["import numpy\n","import os\n","import pandas\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import seaborn as sns\n","import warnings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yW_zE5EpGTHY"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams[\"figure.figsize\"] = (40, 15)\n","plt.rcParams['lines.linewidth'] = 2\n","plt.rcParams[\"axes.grid\"] = ■■■■■"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgL89mhPGTHY"},"outputs":[],"source":["%%html\n","<style>\n","    table { display: inline-block }\n","    .rendered_html td, .rendered_html ■■■■■ { text-align: left; }\n","</style>"]},{"cell_type":"markdown","metadata":{"id":"NT12pRO7GTHY"},"source":["# 0.2. 조건 및 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I15BPDOBGTHY"},"outputs":[],"source":["# ================================================================================================================================\n","# 개인별 환경에 맞게 조정하실 것\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args = dict()\n","# --------------------------------------------------------------------------------------------------------------------------------\n","# 입력 관련 설정\n","# --------------------------------------------------------------------------------------------------------------------------------\n","dict_args['base_file'] = os.path.abspath(\"./Data\") + '/data_X.csv'\n","# 기준일자\n","dict_args['date_col_name'] = 'eod_date'\n","# 현대차 일별 종가 \n","dict_args['target_col_name'] = '5380-price_close'\n","# 학습 구간 (window size)\n","dict_args['window_size'] = 10\n","# 테스트 크기\n","dict_args['test_size'] = 2\n","# 테스트 시작 기간\n","dict_args['test_date_start'] = 20140101\n","# 테스트 종료 기간\n","dict_args['test_date_finish'] = 20140410\n","# 모델 파일 저장 패턴\n","dict_args['model_file'] = 'zulu_train_model_MODELNAME.h5'\n","\n","dir_work = 'work/'\n","dir_refs = 'refs/'\n","\n","if not os.path.exists(dir_work):\n","    os.mkdir(■■■■■)\n","if not os.path.exists(dir_refs):\n","    ■■■■■(dir_refs)"]},{"cell_type":"markdown","metadata":{"id":"xkTzBWuXGTHY"},"source":["# 0.3. 데이터 로딩 및 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGITH1VuGTHY"},"outputs":[],"source":["df = pandas.read_csv(dict_args['base_file'])\n","df = df.set_index([dict_args['date_col_name']])\n","#display(df)"]},{"cell_type":"markdown","metadata":{"id":"Kj8zlfZFGTHY"},"source":["# 0.4. 함수 선언"]},{"cell_type":"markdown","metadata":{"id":"5HTtJpKWGTHZ"},"source":["## 0.4.1. 함수 선언 - 01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2svuMaCGTHZ"},"outputs":[],"source":["# data load\n","def data_load(■■■■■,file_name, Date_col):\n","    import pandas\n","    df = pandas.read_csv(path + file_name, index_col = False, ■■■■■ = False)\n","    df[Date_col] = pandas.to_datetime(df[Date_col], format='%Y%m%d')\n","    df = df.set_index(Date_col)\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"xNZG3G6-GTHZ"},"source":["# 1. 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"f29uKqPwGTHZ"},"source":["# 1.1. 예측 대상 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3K5pkmhFGTHZ"},"outputs":[],"source":["df_Y = df[[dict_args['target_col_name']]]\n","df_Y = df_Y[df_Y.index <= dict_args['test_date_finish']]\n","■■■■■ = df_Y[■■■■■[dict_args['target_col_name']] > 0]\n","df_Y['target'] = df_Y[dict_args['target_col_name']].shift(-1)\n","df_Y = df_Y.dropna(axis='rows')\n","del df_Y[dict_args['target_col_name']]\n","#display(df_Y)\n","df_Y.to_csv('data_Y_source.csv')\n","df_plot_Y = df_Y.copy()\n","df_plot_Y['datetime'] = pandas.to_datetime(df_plot_Y.index, format='%Y%m%d')\n","df_plot_Y = df_plot_Y.set_index(['datetime'])\n","# df_plot_Y.plot()"]},{"cell_type":"markdown","metadata":{"id":"ZzW4ABY6GTHZ"},"source":["# 1.2. 데이터 처리 속도 향상을 위한 종가 데이터 발췌"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUrFerrLGTHZ"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_X = pandas.DataFrame()\n","for col_name in df.columns:\n","    if 'close' in col_name:\n","        if df_X.shape[0] == 0:\n","            df_X = df[[col_name]]\n","        else:\n","            df_X[col_name] = df[col_name]\n","#display(df_X)\n","df_X.to_csv('data_X_source.csv')\n","\n","df_source = df_Y.join(df_X, how='inner')\n","df_source = df_source.fillna(1 + 1e-■■■■■)\n","#display(df_source)\n","for col_name ■■■■■ df_source.columns:\n","    if numpy.min(df_source[col_name]) == numpy.max(■■■■■[col_name]):\n","        del df_source[col_name]\n","#display(df_source)"]},{"cell_type":"markdown","metadata":{"id":"1Uq7-FkSGTHZ"},"source":["# 2. 피처 엔지니어링"]},{"cell_type":"markdown","metadata":{"id":"AYSwtr_zGTHZ"},"source":["# 2.1. 정규화 / 표준화 / 로그 데이터 생성"]},{"cell_type":"markdown","metadata":{"id":"5sNWogPSGTHZ"},"source":["# 2.1.1. 정규화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"881ntKl_GTHZ"},"outputs":[],"source":["df_norm = df_source.copy()\n","list_min_max = list()\n","for col_name in df_norm.columns:\n","    val_min = numpy.min(df_norm[col_name])\n","    val_max = ■■■■■(df_norm[col_name])\n","    if val_min == val_max:\n","        del df_norm[col_name]\n","        ■■■■■\n","    list_min_max.append({\n","        'col': col_name,\n","        'min': val_min,\n","        'max': ■■■■■\n","    })\n","    df_norm[col_name] = (df_norm[col_name] - val_min) / (val_max - val_min)\n","df_min_max = pandas.DataFrame(list_min_max)\n","df_min_max.to_csv(dir_refs + 'ref_min_max.csv', index=None)\n","#display(df_norm)"]},{"cell_type":"markdown","metadata":{"id":"bGyhqn0SGTHZ"},"source":["# 2.1.2. 표준화 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bq-nKQPCGTHa"},"outputs":[],"source":["df_std = df_source.copy()\n","for ■■■■■ in df_std.columns:\n","    val_std = numpy.std(df_std[col_name])\n","    val_avg = numpy.mean(df_std[col_name])\n","    if val_std == ■■■■■:\n","        del df_std[col_name]\n","        continue\n","    df_std[col_name] = (df_std[col_name] - val_avg) / val_std\n","#display(df_std)"]},{"cell_type":"markdown","metadata":{"id":"KTwyvqfeGTHa"},"source":["# 2.1.3. 로그 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8i0RzclXGTHa"},"outputs":[],"source":["df_log = df_source.copy()\n","df_log = numpy.log(df_log)\n","#display(df_log)"]},{"cell_type":"markdown","metadata":{"id":"3NMqIvQhGTHa"},"source":["# 2.2. 상관관계 분석"]},{"cell_type":"markdown","metadata":{"id":"uhbY52F0GTHa"},"source":["# 2.2.1. 원천 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tooz-aBZGTHa"},"outputs":[],"source":["# 상관관계 분석\n","def get_corr(p_source, p_target):\n","    import numpy\n","    import pandas\n","\n","    list_corr = list()\n","    for col_name in p_source.columns:\n","        if col_name == 'target':\n","            continue\n","        if col_name == p_target:\n","            continue\n","\n","        df_corr_check = p_source[['target', ■■■■■]].corr(method='pearson')\n","\n","        list_corr.append({\n","            'VAR_NAME': col_name,\n","            'CORR': ■■■■■(1)[col_name].tolist()[0],\n","            'CORR-ABS': ■■■■■(df_corr_check.head(1)[col_name].tolist()[0]),\n","        })\n","    df_corr = pandas.DataFrame(list_corr)\n","    df_corr = df_corr.sort_values(by=['CORR-ABS'], ascending=False)\n","\n","    return df_corr\n","\n","df_corr_src = df_source.fillna(-1)\n","df_corr_source = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_source.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_source.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(■■■■■, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"xowbPfgxGTHa"},"source":["# 2.2.2. 표준화 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVvzMqouGTHa"},"outputs":[],"source":["df_corr_src = df_std.copy()\n","df_corr_std = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_std.T)\n","warnings.filterwarnings('ignore')\n","■■■■■ = df_corr_src[['target', df_corr_std.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"Z0ZSnpJxGTHa"},"source":["# 2.2.3. 정규화 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slcIfQD7GTHa"},"outputs":[],"source":["df_corr_src = df_norm.copy()\n","df_corr_norm = ■■■■■(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_norm.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_norm.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"noJHXVj_GTHa"},"source":["# 2.2.4. 로그 데이터에 대한 상관관계 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-2n7UatGTHb"},"outputs":[],"source":["df_corr_src = df_log.copy()\n","df_corr_log = get_corr(df_corr_src, dict_args['target_col_name'])\n","#display(df_corr_log.T)\n","warnings.filterwarnings('ignore')\n","df_plot_corr = df_corr_src[['target', df_corr_log.head(1)['VAR_NAME'].tolist()[0]]]\n","df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, ■■■■■='%Y%m%d')\n","df_plot_corr = df_plot_corr.set_index(['datetime'])\n","# df_plot_corr.plot()\n","#display(df_plot_corr)"]},{"cell_type":"markdown","metadata":{"id":"eRM7GSRsGTHb"},"source":["# 2.3. Feature Importance"]},{"cell_type":"markdown","metadata":{"id":"PZkW-I6vGTHb"},"source":["# 2.3.1. 원천 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7acq2sXvGTHb"},"outputs":[],"source":["# Feature Importance\n","def get_feature_importance(l_data_x, p_target):\n","    import numpy\n","    import pandas\n","    import sklearn.ensemble\n","\n","    l_data_y = l_data_x['target']\n","    del l_data_x['target']\n","    ■■■■■ l_data_x[p_target]\n","\n","    forest = sklearn.ensemble.RandomForestRegressor(\n","        max_depth = 50,\n","        ■■■■■ = 5,\n","        min_samples_leaf = 3,\n","        ■■■■■ = 0,\n","        n_estimators = 100,\n","    )\n","    forest.fit(l_data_x, l_data_y)\n","    ■■■■■importance = forest.feature_importances_\n","\n","    indices = numpy.argsort(l_importance)\n","\n","    columns = l_data_x.columns.tolist()\n","    column_count = len(columns)\n","\n","    list_var = list()\n","    for idx in range(column_count):\n","        list_var.append({\n","            'feature_name': columns[indices[idx]],\n","            'feature_value': l_import■■■■■[indices[idx]]\n","        })\n","\n","    df_fi_source = pandas.DataFrame(list_var)\n","    df_fi_source = df_fi_source.sort_values(by='feature_value', ascending=False)\n","\n","    return df_fi_source\n","\n","df_src = df_source.fillna(-1)\n","df_fi_source = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_source.T)\n","■■■■■('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_source.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"L07vzGfEGTHb"},"source":["# 2.3.2. 표준화 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgZFiQZ7GTHb"},"outputs":[],"source":["df_src = df_std.fillna(-1)\n","df_fi_std = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_std.T)\n","warnings.filterwarnings('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_std.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = ■■■■■(df_plot_fi_src.index, format='%Y%m%d')\n","■■■■■ = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"qBNejrXWGTHb"},"source":["# 2.3.3. 정규화 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEmazr56GTHb"},"outputs":[],"source":["df_src = df_norm.fillna(-1)\n","df_fi_norm = get_feature_import■■■■■(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_norm.T)\n","warnings.filterwarnings('ignore')\n","■■■■■ = df_src[['target', df_fi_norm.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"AMoBKIp4GTHb"},"source":["# 2.3.4. 로그 데이터에 대해 Feature Importance 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c84OKVsCGTHc"},"outputs":[],"source":["df_src = ■■■■■(-1)\n","df_fi_log = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n","#display(df_fi_log.T)\n","warnings.filterwarnings('ignore')\n","df_plot_fi_src = df_src[['target', df_fi_log.head(1)['feature_name'].tolist()[0]]]\n","df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n","df_plot_fi_src = ■■■■■(['datetime'])\n","# df_plot_fi_src.plot()\n","#display(df_plot_fi_src)"]},{"cell_type":"markdown","metadata":{"id":"LAP4LC54GTHc"},"source":["# 2.4. 공적분 검정 - ADF Test"]},{"cell_type":"markdown","metadata":{"id":"LBNnYI3aGTHc"},"source":["# 2.4.1. 원천 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWrorIFpGTHc"},"outputs":[],"source":["# ADF Test\n","def get_adf_test(p_source, p_col_1, p_col_2):\n","    import statsmodels.tsa.stattools\n","\n","    p_source['spread'] = p_source[p_col_1] - p_source[■■■■■]\n","    return statsmodels.tsa.stattools.adfuller(p_source['spread'])\n","\n","■■■■■('ignore')\n","df_src = df_source.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if ■■■■■ == 'target':\n","        continue\n","    df_adf = ■■■■■(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': adf_p_value,\n","    })\n","■■■■■ = pandas.DataFrame(list_result)\n","df_adf_src = df_adf_src.sort_values(by=['adf p-value'], ascending=True)\n","#display(df_adf_src.head(7))\n","selected_feature = df_adf_src.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","# df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = df_plot_adf_src[['spread']]\n","# df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"BG-INQpOGTHc"},"source":["# 2.4.2. 표준화 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkxLDcYCGTHc"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_std.fillna(-1)\n","list_result = ■■■■■()\n","for col_name in df_src.columns:\n","    if ■■■■■ == dict_args['target_col_name']:\n","        continue\n","    if ■■■■■ == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_std = pandas.DataFrame(list_result)\n","df_adf_std = df_adf_std.sort_values(by=['adf p-value'], ascending=True)\n","#display(df_adf_std.head(7))\n","■■■■■ = df_adf_src.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","# df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = df_plot_adf_src[['spread']]\n","# df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"J2UqYA_5GTHc"},"source":["# 2.4.3. 정규화 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cm-XP3eKGTHc"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","■■■■■ = df_norm.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', col_name)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_norm = pandas.DataFrame(list_result)\n","df_adf_norm = df_adf_norm.sort_values(by=['adf p-value'], ascending=True)\n","#display(df_adf_norm.head(7))\n","selected_feature = df_adf_norm.head(1)['feature_name'].tolist()[■■■■■]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, ■■■■■='%Y%m%d')\n","■■■■■ = df_plot_adf_src.set_index(['datetime'])\n","df_plot_adf_src = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[selected_feature])]\n","df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = ■■■■■[['spread']]\n","#df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"KuO_QYbLGTHc"},"source":["# 2.4.4. 로그 데이터에 대해 ADF Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GaM9W3sGTHd"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_log.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_adf = get_adf_test(df_src, 'target', ■■■■■)\n","    adf_p_value = df_adf[1]\n","    if adf_p_value == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': ■■■■■,\n","        'adf p-value': adf_p_value,\n","    })\n","df_adf_log = pandas.DataFrame(list_result)\n","df_adf_log = df_adf_log.sort_values(by=['adf p-value'], ascending=True)\n","#display(df_adf_log.head(7))\n","■■■■■ = df_adf_log.head(1)['feature_name'].tolist()[0]\n","df_plot_adf_src = df_src[['target', selected_feature]]\n","df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n","df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n","df_plot_adf_src = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[■■■■■])]\n","df_plot_adf_src.plot()\n","#display(df_plot_adf_src)\n","df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n","df_plot = ■■■■■[['spread']]\n","df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"UGNjN7YoGTHd"},"source":["# 2.5. 공적분 검정 - Shapiro-wilk Test"]},{"cell_type":"markdown","metadata":{"id":"R_v-h0-jGTHd"},"source":["# 2.5.1. 원천 데이터에 대해 shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGbf_vfKGTHd"},"outputs":[],"source":["# Shapiro wilk Test\n","def get_shapiro_wilk(p_source, p_col_1, p_col_2):\n","    import scipy.stats\n","\n","    p_source['spread'] = p_source[p_col_1] - p_source[p_col_2]\n","    return scipy.stats.shapiro(p_source['spread'])\n","\n","warnings.filterwarnings('ignore')\n","df_src = df_source.fillna(-1)\n","list_result = ■■■■■()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', ■■■■■)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': ■■■■■,\n","    })\n","df_shapiro_src = pandas.DataFrame(list_result)\n","df_shapiro_src = df_shapiro_src.sort_values(by=['shapiro-wilk'], ascending=False)\n","#display(df_shapiro_src)\n","selected_feature = ■■■■■(■■■■■)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"prIZ7j-CGTHd"},"source":["# 2.5.2. 표준화 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XplCkmoyGTHd"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_std.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if ■■■■■ == 'target':\n","        continue\n","    df_shapiro = ■■■■■(df_src, 'target', col_name)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_std = pandas.DataFrame(list_result)\n","df_shapiro_std = df_shapiro_std.sort_values(by=['shapiro-wilk'], ascending=■■■■■)\n","#display(df_shapiro_std)\n","selected_feature = df_shapiro_std.head(1)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - ■■■■■[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"_83GgAqwGTHd"},"source":["# 2.5.3. 정규화 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8rXAceCGTHd"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_norm.fillna(-1)\n","list_result = ■■■■■()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == numpy.nan:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': ■■■■■,\n","    })\n","df_shapiro_norm = pandas.DataFrame(list_result)\n","df_shapiro_norm = df_shapiro_norm.sort_values(by=['shapiro-wilk'], ascending=False)\n","#display(df_shapiro_norm)\n","selected_feature = df_shapiro_norm.head(1)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', selected_feature]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = ■■■■■(['datetime'])\n","■■■■■()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"HLcuOUczGTHd"},"source":["# 2.5.4. 로그 데이터에 대해 Shapiro-wilk Test 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WwR7_jkGTHd"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","df_src = df_log.fillna(-1)\n","list_result = list()\n","for col_name in df_src.columns:\n","    if col_name == dict_args['target_col_name']:\n","        continue\n","    if col_name == 'target':\n","        continue\n","    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n","    shapiro_wilk = df_shapiro.statistic\n","    if shapiro_wilk == ■■■■■:\n","        continue\n","    list_result.append({\n","        'feature_name': col_name,\n","        'shapiro-wilk': shapiro_wilk,\n","    })\n","df_shapiro_log = pandas.DataFrame(list_result)\n","df_shapiro_log = df_shapiro_log.sort_values(by=['shapiro-wilk'], ascending=False)\n","#display(df_shapiro_log)\n","selected_feature = ■■■■■(1)['feature_name'].tolist()[0]\n","df_plot = df_src[['target', ■■■■■]]\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","■■■■■ = df_plot.set_index(['datetime'])\n","df_plot.plot()\n","#display(df_plot)\n","df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n","df_plot = df_plot[['spread']]\n","df_plot.plot()\n","#display(df_plot)"]},{"cell_type":"markdown","metadata":{"id":"FddaYicBGTHe"},"source":["# 2.6. 변수 취합"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_5WW54gGTHe"},"outputs":[],"source":["list_features = ['target']\n","list_features.extend(df_corr_source.head(20)['VAR_NAME']■■■■■())\n","list_features.extend(df_corr_std.head(20)['VAR_NAME'].tolist())\n","list_features.extend(df_corr_norm.head(20)['VAR_NAME'].tolist())\n","■■■■■(df_corr_log.head(■■■■■)['VAR_NAME'].tolist())\n","\n","list_features.extend(df_fi_source.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_std.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_norm.head(20)['feature_name'].tolist())\n","list_features.extend(df_fi_log.head(20)['feature_name']■■■■■())\n","\n","list_features.extend(df_adf_src.head(20)['feature_name'].tolist())\n","list_features.extend(df_adf_std.head(20)['feature_name'].tolist())\n","list_features.extend(df_adf_norm.head(20)['feature_name'].tolist())\n","list_features.extend(df_adf_log.head(20)['feature_name'].tolist())\n","\n","list_features.extend(df_shapiro_src.head(20)['feature_name'].tolist())\n","list_features.extend(df_shapiro_std.head(20)['feature_name'].tolist())\n","list_features.extend(df_shapiro_norm.head(20)['feature_name'].tolist())\n","list_features.extend(■■■■■(20)['feature_name'].tolist())\n","\n","list_features = list(set(list_features))\n","# print(list_features)\n","\n","df_train = df_source.fillna(1 + 1e-7)\n","df_train = df_train[list_features]\n","#display(df_train)\n","df_train.to_csv('data_modeling.csv')\n","df_plot = df_train[['target']]\n","df_plot['datetime'] = ■■■■■(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot.plot()"]},{"cell_type":"markdown","metadata":{"id":"zUXirgWNGTHe"},"source":["# 3. 모델링"]},{"cell_type":"markdown","metadata":{"id":"Rdb3zEzmGTHe"},"source":["# 3.1. linear_model.OrthogonalMatchingPursuitCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yz8_293GTHe"},"outputs":[],"source":["import sklearn.linear_model\n","\n","model = sklearn.linear_model.OrthogonalMatchingPursuitCV(cv=3)\n","\n","list_predict = list()\n","for idx ■■■■■ range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","■■■■■ = ■■■■■(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, ■■■■■='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","■■■■■()"]},{"cell_type":"markdown","metadata":{"id":"ks-XQsdJGTHe"},"source":["# 3.2. tree.DecisionTreeRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfoDSDqTGTHe"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.tree\n","\n","model = sklearn.tree.DecisionTreeRegressor(criterion='squared_error', random_state=111)\n","\n","hyper_parameter = {\n","    'max_features': ['auto'],\n","    'max_depth': [3],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [1],\n","}\n","■■■■■ = df_train.tail(dict_args['window_size'] + idx + 1 + ■■■■■['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, ■■■■■=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(■■■■■['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': ■■■■■,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","■■■■■ = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, ■■■■■='%Y%m%d')\n","■■■■■ = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"2zucjO-PGTHe"},"source":["# 3.3. ensemble.RandomForestRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kb8C6sQZGTHe"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.ensemble\n","\n","model = sklearn.ensemble.RandomForestRegressor(criterion='squared_error', random_state=111, n_estimators=1)\n","\n","hyper_parameter = {\n","    'max_features': ['auto'],\n","    'max_depth': [2],\n","    'min_samples_split': [2],\n","    'min_samples_leaf': [2],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size'])■■■■■(■■■■■['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    ■■■■■ = df_train.tail(dict_args['window_size'] + idx + 1)■■■■■(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, ■■■■■)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': ■■■■■()[0],\n","        'real_value': ■■■■■,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = ■■■■■[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"ZVTBrLR6GTHf"},"source":["# 3.4. xgboost.XGBRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ti0X110AGTHf"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import xgboost\n","\n","model = xgboost.XGBRegressor(booster='gbtree', objective='reg:squarederror', n_estimators=2)\n","\n","hyper_parameter = {\n","    'reg_lambda': [1.0, ■■■■■, 0.1],\n","    'reg_alpha': [0.0, 0.001, ■■■■■],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = ■■■■■(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(■■■■■).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    ■■■■■ = ■■■■■()[0]\n","    predict_value = model_instance.predict(df_tests_X)[■■■■■]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': ■■■■■,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"ATSIr8K7GTHf"},"source":["# 3.5. LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hWTNmrLGTHf"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import lightgbm\n","\n","model = ■■■■■(random_state=0, objective='regression', ■■■■■=-1, max_depth=-1, subsample=0.8, ■■■■■=3)\n","\n","hyper_parameter = {\n","    'learning_rate': [0.1],\n","    'num_leaves': [■■■■■],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + ■■■■■ + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del df_tune_X['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', ■■■■■=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(1)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    ■■■■■ = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, ■■■■■)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"RvbCLq6zGTHf"},"source":["# 3.6. keras.LSTM"]},{"cell_type":"markdown","metadata":{"id":"VfCaRE8-GTHf"},"source":["# 3.6.1. LSTM Style #1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDInhoE6GTHf"},"outputs":[],"source":["# window 기간에 과거 데이터를 통한 예측 데이터 생성 함수\n","def make_dataset(data, label, window_size):\n","    import numpy\n","    feature_list = list()\n","    label_list = list()\n","    for i ■■■■■ range(■■■■■(data) - window_size):\n","        feature_list.append(numpy.array(data.iloc[i:i+window_size]))\n","        label_list.append(numpy.array(label.iloc[i+window_size]))\n","    return numpy.array(feature_list), numpy.array(label_list)\n","\n","def split_data(DataFrame, test_size, target_col, window_size):\n","    import pandas\n","    # train_test 분리\n","    train = DataFrame[:-■■■■■]\n","    test = DataFrame[-test_size:]\n","\n","    # feature, target 분리\n","    feature_cols = list(DataFrame.columns)\n","    feature_cols.remove(target_col)\n","\n","    # train_data\n","    train_feature = train[feature_cols]\n","    train_label = pandas.DataFrame(train[target_col])\n","\n","    # test_data\n","    test_feature = test[feature_cols]\n","    test_label = pandas.DataFrame(test[target_col])\n","\n","    # 예측 주기에 따른 timestep 생성\n","    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n","    test_feature, test_label = make_dataset(test_feature, test_label, window_size)\n","\n","    # print('split_data', train_feature.shape, train_label.shape)\n","    # print('split_data', test_feature.shape, test_label.shape)\n","\n","    return train_feature, train_label, test_feature, test_label\n","\n","def LSTM_MODEL(■■■■■, train_label, test_size, units=4, ■■■■■=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=200, batch_size=16):\n","    \n","    import sklearn\n","    import tensorflow.keras.models\n","    import tensorflow.keras.layers\n","    import tensorflow.keras.callbacks\n","\n","    # train_valid 분할\n","    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(train_feature, train_label, test_size=test_size)\n","\n","    # print('LSTM_MODEL', x_train.shape, x_valid.shape)\n","    # print('LSTM_MODEL', y_train.shape, y_valid.shape)\n","\n","    # Sequential 모델 레이어 생성\n","    model = tensorflow.keras.models.Sequential()\n","    # LSTM 레이어 추가\n","    model.add(tensorflow.keras.layers.LSTM(\n","        units,\n","        input_shape=(train_feature.shape[1], train_feature.shape[2]),\n","        activation='relu',\n","        return_sequences=■■■■■\n","    ))\n","    # Dense 레이어 추가\n","    model.add(tensorflow.keras.layers.Dense(32))\n","    model.add(tensorflow.keras.layers.Dense(1))\n","    # print(model.summary())\n","    # 모델 컴파일 활성함수 loss_function 지정 , 최적화함수 지정\n","    ■■■■■(loss=loss, optimizer=optimizer)\n","    # 얼리스탑핑 ( val_loss의 변화를 비교, 같은 값 5번 일시 중지)\n","    early_stop = tensorflow.keras.callbacks.EarlyStopping(monitor=monitor, patience=20, baseline=■■■■■, mode ='min')\n","    # 모형 가중치 저장 파일 경로 선언\n","    # 모형 가중치 판단 및 모델 저장\n","    filename = 'refs/model_checkpoint.h5'\n","    checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filename, monitor=monitor, verbose=1, save_best_only=True, mode='min')\n","\n","    # 모형 학습\n","    model.fit(x_train, y_train,\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              validation_data=(x_valid, y_valid),\n","              callbacks=[early_stop, ■■■■■],\n","              verbose = 1\n","              )\n","\n","    ■■■■■ model, ■■■■■\n","\n","\n","warnings.filterwarnings('ignore')\n","\n","import sklearn\n","import tensorflow.keras.models\n","import tensorflow.keras.layers\n","import tensorflow.keras.callbacks\n","\n","df_test_scaled = df_source.fillna(1 + 1e-7)\n","list_min_max = list()\n","for col_name in df_source.columns:\n","    val_min = numpy.min(df_test_scaled[col_name])\n","    val_max = numpy.max(df_test_scaled[■■■■■])\n","    if val_min == val_max:\n","        del df_test_scaled[col_name]\n","        continue\n","    list_min_max.append({\n","        'col': ■■■■■,\n","        'min': val_min,\n","        'max': val_max\n","    })\n","    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n","df_min_max = pandas.DataFrame(list_min_max)\n","list_predict = ■■■■■()\n","■■■■■ idx in range(dict_args['test_size']):\n","\n","    df_train_sub = df_test_scaled.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    predict_date = df_train_sub.tail(1).index.tolist()[0]\n","    df_train_X, df_train_Y, df_tests_X, df_tests_Y = ■■■■■(df_train_sub, 2, 'target', 1)\n","\n","    model_instance, filename = LSTM_MODEL(df_train_X, df_train_Y, ■■■■■, units=2, loss=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=2, batch_size=128)\n","    model_instance.fit(df_train_X, df_train_Y)\n","\n","    real_value = df_tests_Y.tolist()[0][0]\n","    predict_value = model_instance.predict(df_tests_X)[0][0]\n","    print(predict_date, real_value, predict_value)\n","    list_predict.append({\n","        'date': predict_date,\n","        'real_value': ■■■■■,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = df_predict.set_index(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","■■■■■ = df_plot.set_index(['datetime'])\n","■■■■■ = df_plot[['real_value', 'predict_value']]\n","df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","df_plot_error.plot()"]},{"cell_type":"markdown","metadata":{"id":"6Mh551GmGTHg"},"source":["# 3.6.2. LSTM Style #2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFrSLKNkGTHg"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn\n","import tensorflow.keras.models\n","import tensorflow.keras.layers\n","import tensorflow.keras.callbacks\n","import tensorflow.keras.optimizers\n","import datetime\n","import sys\n","\n","def line_logging(*messages):\n","    import datetime\n","    import sys\n","    today = datetime.datetime.today()\n","    log_time = ■■■■■('[%Y/%m/%d %H:%M:%S]')\n","    log = []\n","    for message in messages:\n","        ■■■■■(str(message))\n","    print(log_time + '::' + ' '■■■■■(log) + '')\n","    sys.stdout.flush()\n","\n","\n","df_test_scaled = df_source.fillna(1 + 1e-7)\n","list_min_max = list()\n","for col_name in df_source.columns:\n","    val_min = numpy.min(df_test_scaled[col_name])\n","    ■■■■■ = ■■■■■(df_test_scaled[col_name])\n","    if val_min == val_max:\n","        del df_test_scaled[col_name]\n","        continue\n","    list_min_max.append({\n","        'col': col_name,\n","        'min': val_min,\n","        'max': val_max\n","    })\n","    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n","df_min_max = pandas.DataFrame(list_min_max)\n","\n","size_row = df_test_scaled.shape[0]\n","size_col = df_test_scaled.shape[1]\n","size_win = dict_args['window_size']\n","loop_count = size_row - size_win + 1\n","line_logging(size_row, ■■■■■, size_win, loop_count)\n","\n","for col_name in df_test_scaled.columns:\n","    # print(col_name, numpy.min(df_test_scaled[col_name]), numpy.max(df_test_scaled[col_name]))\n","    pass\n","    if ■■■■■(df_test_scaled[col_name]) != 0.0:\n","        print(col_name, 'min', ■■■■■(df_test_scaled[col_name]))\n","    if numpy.max(df_test_scaled[col_name]) != ■■■■■:\n","        print(col_name, 'max', numpy.max(df_test_scaled[col_name]))\n","line_logging('check min/max value')\n","\n","list_train_X = list()\n","list_train_Y = list()\n","list_tests_X = list()\n","list_tests_Y = list()\n","■■■■■ idx in range(loop_count):\n","    df_train_sub_X = df_test_scaled.tail(size_win + idx + 1).head(size_win)\n","    df_train_sub_Y = df_train_sub_X.tail(1)['target']\n","    del df_train_sub_X['target']\n","    if idx >= loop_count - 50:\n","        list_tests_X.append(df_train_sub_X.to_numpy())\n","        list_tests_Y.append(df_train_sub_Y.to_numpy())\n","    else:\n","        list_train_X.append(df_train_sub_X.to_numpy())\n","        list_train_Y.append(df_train_sub_Y.to_numpy())\n","line_logging('build data layer')\n","\n","train_X = numpy.array(list_train_X)\n","train_Y = numpy.array(■■■■■)\n","■■■■■ = numpy.array(list_tests_X)\n","value_real = numpy.array(list_tests_Y)[0][0]\n","line_logging(train_X.shape, train_Y.shape, tests_X.shape, value_real.shape)\n","\n","■■■■■ = 2\n","loss = 'mean_squared_error'\n","optimizer = tensorflow.keras.optimizers.Adam(clipvalue=0.5)\n","epochs = 2\n","batch_size = 128\n","monitor = 'val_loss'\n","model = tensorflow.keras.models.Sequential()\n","# LSTM 레이어 추가\n","# input_shape: 첫번째는 윈도우 크기, 두번째는 컬럼수\n","■■■■■(tensorflow.keras.layers.LSTM(units, input_shape=(size_win, size_col - 1), return_sequences=False))\n","# Dense 레이어 추가\n","model.add(tensorflow.keras.layers.Dense(32, activation='relu'))\n","model.add(tensorflow.keras.layers.Dense(1, activation='sigmoid'))\n","line_logging('define model')\n","# 모델 컴파일 활성함수 loss_function 지정 , 최적화함수 지정\n","model.compile(loss=loss, optimizer=optimizer)\n","line_logging('compile model')\n","# 모형 학습\n","model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, verbose=■■■■■)\n","model.summary()\n","■■■■■ = model.predict(tests_X)[0][■■■■■]\n","line_logging(value_pred)\n","line_logging(value_real)\n","\n","# #display(df_min_max)\n","target_min = df_min_max[df_min_max['col'] == 'target']['min'].tolist()[0]\n","target_max = df_min_max[df_min_max['col'] == 'target']['max'].tolist()[0]\n","\n","denrm_pred =  target_min + (target_max - target_min) * value_pred\n","denrm_real =  target_min + (target_max - target_min) * value_real\n","\n","line_logging('PRED', value_pred, denrm_pred)\n","line_logging('REAL', value_real, denrm_real)\n","line_logging('ERROR Normalize:', int((value_pred - value_real) / value_real * 10000) / 100, '%')\n","line_logging('ERROR De-normalize: ', int((denrm_pred - denrm_real) / ■■■■■ * 10000) / 100, '%')"]},{"cell_type":"markdown","metadata":{"id":"t03m9DXWGTHg"},"source":["# 3.7. MLPRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hEz1ANDGTHg"},"outputs":[],"source":["warnings.filterwarnings('ignore')\n","\n","import sklearn.neural_network\n","\n","model = sklearn.neural_network.MLPRegressor(random_state=■■■■■, max_iter=■■■■■, activation='relu', solver='adam', learning_rate='constant')\n","\n","■■■■■ = {\n","    'hidden_layer_sizes': [(2,), (3,)],\n","    'alpha': [0.001],\n","}\n","df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n","df_tune_Y = df_tune_X['target']\n","del ■■■■■['target']\n","grid_search = sklearn.model_selection.GridSearchCV(estimator=model, ■■■■■=hyper_parameter, ■■■■■=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n","grid_search.fit(df_tune_X, df_tune_Y)\n","model_instance = grid_search.best_estimator_\n","\n","list_predict = list()\n","for idx in range(dict_args['test_size']):\n","    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n","    df_tests_X = df_train_X.tail(■■■■■)\n","    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n","\n","    df_train_Y = df_train_X['target']\n","    df_tests_Y = df_tests_X['target']\n","    del df_train_X['target']\n","    del df_tests_X['target']\n","\n","    model_instance.fit(df_train_X, df_train_Y)\n","    real_value = df_tests_Y.values.tolist()[0]\n","    predict_value = model_instance.predict(df_tests_X)[0]\n","    list_predict.append({\n","        'date': df_tests_X.index.tolist()[0],\n","        'real_value': real_value,\n","        'predict_value': predict_value,\n","        'PercentError': (predict_value - real_value) / real_value * 100\n","    })\n","\n","df_predict = pandas.DataFrame(list_predict)\n","# #display(df_predict)\n","\n","df_plot = ■■■■■(['date'])\n","df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n","df_plot = df_plot.set_index(['datetime'])\n","df_plot_value = df_plot[['real_value', 'predict_value']]\n","# df_plot_value.plot()\n","df_plot_error = df_plot[['PercentError']]\n","# df_plot_error.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmmdXsNRGTHg"},"outputs":[],"source":["df_tune = df_std.copy()\n","#display(df_tune)\n","list_features = ['target']\n","list_features.extend(df_corr_std.head(20)['VAR_NAME'].tolist())\n","\n","df_tune = ■■■■■[list_features]\n","df_tune['datetime'] = pandas.to_datetime(df_tune.index, format='%Y%m%d')\n","df_tune = df_tune.set_index(['datetime'])\n","#display(df_tune)\n","# df_tune.plot()\n","df_tune.to_csv('data_std.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('test')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"7f404382b6ee812ff854d886c5a2fbb987a19b2f69a33528c318b78ba24eea58"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}