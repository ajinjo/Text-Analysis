{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. \ud30c\uc774\uc36c \uae30\ucd08"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ubcd1\ub82c \uc2e4\ud589\uc740 \uac19\uc740 \uc8fc\ud53c\ud130 \ud30c\uc77c\uc5d0 \uc788\uc73c\uba74 \uc2e4\ud589\ub418\uc9c0 \uc54a\uc73c\ubbc0\ub85c \ubcc4\ub3c4\uc758 \ud30c\uc77c\ub85c \ucc98\ub9ac\n",
				"from Functions import *\n",
				"\n",
				"import multiprocessing\n",
				"import os\n",
				"import pandas\n",
				"import sqlite3"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc8fc\uc11d \ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ud55c \uc904 \uc8fc\uc11d\n",
				"'''\n",
				"\uc5ec\ub7ec \uc904 \uc8fc\uc11d\uc740 \uc774\ub807\uac8c \uc4f0\uac70\ub098\n",
				"\ub610\ub294....\n",
				"'''\n",
				"\"\"\"\n",
				"\uc774\ub807\uac8c \uc791\uc131\ud569\ub2c8\ub2e4.\n",
				"\uc608\uc2dc\ub85c \uc791\uc131\ud588\uc2b5\ub2c8\ub2e4.\n",
				"\"\"\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \uc720\ud615"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# =====================================================================================================================================\n",
				"# \uc608\uc81c\ub85c \uc0ac\uc6a9\ud560 \uac12\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"value1 = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = '1.2'\n",
				"value3 = 1.3\n",
				"value4 = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"value5 = 0\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \uc815\uc218/\uc2e4\uc218/\ucc38\uac70\uc9d3/\ubb38\uc790\uc5f4 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\uc815\uc218 \uc720\ud615')\n",
				"print('\uc2e4\uc81c \uac12', value1, '(', \u25a0\u25a0\u25a0\u25a0\u25a0(value1), ')\uc744 \uc815\uc218\ub85c \ud45c\ud604\ud558\uba74', int(value1), '\uc785\ub2c8\ub2e4.')\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\uc2e4\uc218 \uc720\ud615')\n",
				"print('\uc2e4\uc81c \uac12', value2, '(', type(value2), ')\uc744 \uc2e4\uc218\ub85c \ud45c\ud604\ud558\uba74', float(value2), '\uc785\ub2c8\ub2e4.')\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\ucc38/\uac70\uc9d3 \uc720\ud615')\n",
				"print('\uc2e4\uc81c \uac12', value4, '(', type(value4), ')\uc744 \ucc38/\uac70\uc9d3\uc73c\ub85c \ud45c\ud604\ud558\uba74', bool(value4), '\uc785\ub2c8\ub2e4.')\n",
				"print('\uc2e4\uc81c \uac12', value5, '(', type(value5), ')\uc744 \ucc38/\uac70\uc9d3\uc73c\ub85c \ud45c\ud604\ud558\uba74', bool(value5), '\uc785\ub2c8\ub2e4.')\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\ubb38\uc790 \uc720\ud615')\n",
				"print('\uc2e4\uc81c \uac12', value3, '(', type(value3), ')\uc744 \ubb38\uc790\uc5f4\ub85c \ud45c\ud604\ud558\uba74', str(value3), '\uc785\ub2c8\ub2e4.')\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# set \uc720\ud615 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"set_sample = set()\n",
				"set_sample.add(1)\n",
				"set_sample.add(2)\n",
				"set_sample.add(3)\n",
				"set_sample.add(4)\n",
				"set_sample.add(5)\n",
				"set_sample.add(1)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('set \uc720\ud615 (\uac12\uc774 \uc911\ubcf5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.)')\n",
				"print(set_sample)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# list \uc720\ud615 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = list()\n",
				"list_sample.append(1)\n",
				"list_sample.append(2)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(3)\n",
				"list_sample.append(4)\n",
				"list_sample.append(5)\n",
				"list_sample.append(1)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('list \uc720\ud615 (\uc911\ubcf5\uc744 \ud5c8\uc6a9\ud569\ub2c8\ub2e4.)')\n",
				"print(list_sample)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# dict \uc720\ud615 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_sample = dict()\n",
				"dict_sample['key:1'] = 1\n",
				"dict_sample['key:2'] = 2\n",
				"dict_sample['key:3'] = 3\n",
				"dict_sample['key:4'] = 4\n",
				"dict_sample['key:5'] = 5\n",
				"dict_sample['key:1'] = 6\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('dict \uc720\ud615 (key \uc5d0\uc11c\ub294 \uc911\ubcf5\uc744 \ud5c8\uc6a9\ud558\uc9c0 \uc54a\uace0, value\uc5d0\uc11c\ub294 \uc911\ubcf5\uc744 \ud5c8\uc6a9\ud569\ub2c8\ub2e4.)')\n",
				"print(dict_sample)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# DataFrame \uc720\ud615 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"df_sample = pandas.DataFrame([\n",
				"    {'C1':1, 'C2':2},\n",
				"    {'C1':3, 'C2':4},\n",
				"    {'C1':\u25a0\u25a0\u25a0\u25a0\u25a0, 'C2':6},\n",
				"    {'C1':7, 'C2':8},\n",
				"])\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('DataFrame \uc720\ud615 (jupyter\uc5d0\uc11c\ub294 display\ub97c \uc0ac\uc6a9\ud558\uba74 \uc870\uae08 \ub354 \ud655\uc778\uc774 \uc27d\uc2b5\ub2c8\ub2e4.')\n",
				"###display(df_sample)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# Series \uc720\ud615 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('Series \uc720\ud615 (DataFrame\uc758 \ubd80\ubd84 \uc9d1\ud569\uc785\ub2c8\ub2e4.)')\n",
				"print(type(df_sample['C1']), df_sample['C1'])\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# ====================================================================================================================================="
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc870\uac74\ubb38"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"value1 = 1\n",
				"value2 = 3.2\n",
				"value3 = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = 'XX'\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \uc815\uc218 \uac12\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"condition_value = 2\n",
				"if value1 > condition_value:\n",
				"    print('value1 \uc740 ', condition_value, '\ubcf4\ub2e4 \ud07d\ub2c8\ub2e4.')\n",
				"elif value1 == condition_value:\n",
				"    print('value1 \uc740 ', condition_value, '\uacfc \uac19\uc2b5\ub2c8\ub2e4.')\n",
				"else:\n",
				"    print('value1 \uc740 ', \u25a0\u25a0\u25a0\u25a0\u25a0, '\ubcf4\ub2e4 \uc791\uc2b5\ub2c8\ub2e4.')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \uc2e4\uc218 \uac12\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"condition_value = 3\n",
				"if value2 > condition_value:\n",
				"    print('value2 \uc740 ', condition_value, '\ubcf4\ub2e4 \ud07d\ub2c8\ub2e4.')\n",
				"elif value2 == condition_value:\n",
				"    print('value2 \uc740 ', condition_value, '\uacfc \uac19\uc2b5\ub2c8\ub2e4.')\n",
				"else:\n",
				"    print('value2 \uc740 ', condition_value, '\ubcf4\ub2e4 \uc791\uc2b5\ub2c8\ub2e4.')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ucc38\uac70\uc9d3 \uac12\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"if value3:\n",
				"    print('value3 \uc740 ', value3, '\ucc38\uc785\ub2c8\ub2e4.')\n",
				"else:\n",
				"    print('value3 \uc740 ', value3, '\uac70\uc9d3\uc785\ub2c8\ub2e4.')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ubb38\uc790\uc5f4 \uac12\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"condition_value = 'YY'\n",
				"if value4 > condition_value:\n",
				"    print('value4 \uc740 ', condition_value, '\ubcf4\ub2e4 \ud07d\ub2c8\ub2e4.')\n",
				"elif value4 == condition_value:\n",
				"    print('value4 \uc740 ', condition_value, '\uacfc \uac19\uc2b5\ub2c8\ub2e4.')\n",
				"else:\n",
				"    print('value4 \uc740 ', condition_value, '\ubcf4\ub2e4 \uc791\uc2b5\ub2c8\ub2e4.')\n",
				"# ====================================================================================================================================="
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ubc18\ubcf5\ubb38"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# =====================================================================================================================================\n",
				"# For \uad6c\ubb38\uc785\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"list_to_log = list()\n",
				"for index in range(10):\n",
				"    list_to_log.append(index)\n",
				"print(list_to_log)\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# While \uad6c\ubb38 \uc785\ub2c8\ub2e4. (\ubb34\ud55c \ubc18\ubcf5\uc744 \uc8fc\uc758\ud569\ub2c8\ub2e4.)\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"list_to_log = list()\n",
				"MAX_LOOP_COUNT = 10\n",
				"flag_loop = True\n",
				"loop_count = 0\n",
				"while flag_loop:\n",
				"    list_to_log.append(loop_count)\n",
				"    loop_count += 1\n",
				"    if loop_count > MAX_LOOP_COUNT:\n",
				"        flag_loop = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"print(list_to_log)\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# Do-While \uad6c\ubb38 \uc785\ub2c8\ub2e4. (\ud30c\uc774\uc36c\uc5d0\ub294 \uc5c6\uc2b5\ub2c8\ub2e4.)\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"list_to_log = list()\n",
				"MAX_LOOP_COUNT = 10\n",
				"flag_loop = True\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = 0\n",
				"list_to_log.append(loop_count)\n",
				"loop_count += 1\n",
				"while flag_loop:\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0(loop_count)\n",
				"    loop_count += \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    if loop_count > MAX_LOOP_COUNT:\n",
				"        flag_loop = False\n",
				"print(list_to_log)\n",
				"# =====================================================================================================================================\n",
				"print('----------------------------------------------------------------------------------------------------')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc608\uc678 \ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"value1 = '1.2'\n",
				"try:\n",
				"    print('\ubb38\uc790\uc5f4( \uac12:', value1, ')\uc744 \uc815\uc218 \uc720\ud615\uc73c\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.')\n",
				"    print('\uac12\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4. [', int(value1), ']')\n",
				"    print('\uccab \uc2dc\ub3c4\uc5d0\uc11c \ub370\uc774\ud130 \uc720\ud615 \ubcc0\uacbd\uc5d0 \uc131\uacf5\ud558\uc600\uc2b5\ub2c8\ub2e4.')\n",
				"except:\n",
				"    print('\ub370\uc774\ud130 \uc720\ud615 \ubcc0\ud658\uc5d0 \uc2e4\ud328\ud558\uc5ec, \uc2e4\uc218\ud615\uc73c\ub85c \uba3c\uc800 \ubcc0\uacbd\ud55c \ud6c4 \uc815\uc218\ud615\uc73c\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4.')\n",
				"    print('\uac12\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4. [', int(\u25a0\u25a0\u25a0\u25a0\u25a0(value1)), ']')\n",
				"    print('\uc608\uc678 \ucc98\ub9ac\ub85c \ub370\uc774\ud130 \uc720\ud615 \ubcc0\uacbd\uc5d0 \uc131\uacf5\ud558\uc600\uc2b5\ub2c8\ub2e4.')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud568\uc218"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def function_sample(p_args):\n",
				"    print(type(p_args), p_args)\n",
				"\n",
				"value1 = 1\n",
				"value2 = 1.2\n",
				"value3 = '2.3'\n",
				"value4 = {'key': 'value'}\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = ['element']\n",
				"\n",
				"function_sample(value1)\n",
				"function_sample(value2)\n",
				"function_sample(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"function_sample(value4)\n",
				"function_sample(value5)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud074\ub798\uc2a4"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"class class_sample():\n",
				"    def method_sample(self, p_args):\n",
				"        print(\u25a0\u25a0\u25a0\u25a0\u25a0(p_args), p_args)\n",
				"\n",
				"value1 = 1\n",
				"value2 = 1.2\n",
				"value3 = '2.3'\n",
				"value4 = {'key': 'value'}\n",
				"value5 = ['element']\n",
				"\n",
				"sample = class_sample()\n",
				"sample.method_sample(value1)\n",
				"sample.method_sample(value2)\n",
				"sample.method_sample(value3)\n",
				"sample.method_sample(value4)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(value5)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud30c\uc77c \ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# =====================================================================================================================================\n",
				"# \ud30c\uc77c\uc5d0 \ub0b4\uc6a9\uc744 \uae30\ub85d\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\ud30c\uc77c\uc5d0 \ub0b4\uc6a9\uc744 \uae30\ub85d\ud569\ub2c8\ub2e4.')\n",
				"sample_file_name = 'sample.csv'\n",
				"with open(sample_file_name, 'w') as \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"    f.write('C1,C2\\n')\n",
				"    f.write('1,2\\n')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ud30c\uc77c \uc548\uc758 \ub0b4\uc6a9\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('\ud30c\uc77c \uc548\uc758 \ub0b4\uc6a9\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.')\n",
				"###display(pandas.read_csv(sample_file_name))\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"# \ud30c\uc77c\uc758 \ub0b4\uc6a9\uc744 \uc77d\uc2b5\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('\ud30c\uc77c\uc758 \ub0b4\uc6a9\uc744 \uc77d\uc2b5\ub2c8\ub2e4.')\n",
				"with open(sample_file_name, 'r') as f:\n",
				"    print(f.read())\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \uc0ac\uc6a9\ud55c \ud30c\uc77c\uc744 \uc0ad\uc81c\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('\uc0ac\uc6a9\ud55c \ud30c\uc77c\uc744 \uc0ad\uc81c\ud569\ub2c8\ub2e4.')\n",
				"os.remove(sample_file_name)\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# ====================================================================================================================================="
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130\ubca0\uc774\uc2a4 \ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# =====================================================================================================================================\n",
				"# \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc640 \uc5f0\uacb0\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('connect to database')\n",
				"conn = sqlite3.connect(':memory:')\n",
				"curs = conn.cursor()\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ud14c\uc774\ube14\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('create table')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"    curs.execute('drop table sample')\n",
				"    conn.commit()\n",
				"except:\n",
				"    pass\n",
				"curs.execute('''\n",
				"create table sample (\n",
				"    c1      int,\n",
				"    c2      float,\n",
				"    c3      text\n",
				")\n",
				"''')\n",
				"conn.commit()\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ub370\uc774\ud130\ub97c \uc785\ub825\ud569\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('insert data to table')\n",
				"df = pandas.DataFrame([\n",
				"    {\n",
				"        'c1': 1,\n",
				"        'c2': 1.1,\n",
				"        'c3': '1.1.1',\n",
				"    },\n",
				"    {\n",
				"        'c1': 2,\n",
				"        'c2': 2.2,\n",
				"        'c3': '2.2.2',\n",
				"    },\n",
				"])\n",
				"###display(df)\n",
				"df.to_sql('sample', conn, if\u25a0\u25a0\u25a0\u25a0\u25a0='append', index=None)\n",
				"\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('check data in table')\n",
				"df2 = pandas.read_sql('select * from sample', conn)\n",
				"###display(df2)\n",
				"# =====================================================================================================================================\n",
				"\n",
				"# =====================================================================================================================================\n",
				"# \ub370\uc774\ud130\ub97c \uc77d\uc2b5\ub2c8\ub2e4.\n",
				"# -------------------------------------------------------------------------------------------------------------------------------------\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"print('read data in table')\n",
				"curs.execute('select * from sample')\n",
				"rs = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"for row in rs:\n",
				"    print('c1:[', row[0], '], c2:[', row[1], '], c3:[', row[2], ']')\n",
				"print('----------------------------------------------------------------------------------------------------')\n",
				"# ====================================================================================================================================="
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ubcd1\ub82c\ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = list()\n",
				"for idx in range(10):\n",
				"    dict_param = dict()\n",
				"    dict_param['index'] = idx\n",
				"    list_param.append(dict_param)\n",
				"\n",
				"p = multiprocessing.Pool(3)\n",
				"# parallel_print \ud568\uc218\uc5d0 \ub300\ud55c \uc815\uc758\ub294 .py \ud30c\uc77c\uc5d0 \ubcc4\ub3c4\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n",
				"p.map(parallel_print, list_param)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc804\ucc98\ub9ac \uae30\ucd08"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc0ac\uc804 \uc900\ube44"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import pandas\n",
				"import sqlite3"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. \uc608\uc81c \ub370\uc774\ud130 \uc900\ube44"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"t1 = pandas.DataFrame([\n",
				"    { 'c1': 11, 'c2': 111, 'c3': 1111, },\n",
				"    { 'c1': 12, 'c2': 121, 'c3': \u25a0\u25a0\u25a0\u25a0\u25a0, },\n",
				"    { 'c1': 13, 'c2': \u25a0\u25a0\u25a0\u25a0\u25a0, 'c3': 1311, },\n",
				"    { 'c1': 14, 'c2': 141, 'c3': 1411, },\n",
				"    { 'c1': 15, 'c2': 151, 'c3': 1511, },\n",
				"    { 'c1': 16, 'c2': 161, 'c3': 1611, },\n",
				"])\n",
				"t2 = pandas.DataFrame([\n",
				"    { 'c1': 11, 'c2': 111, 'c3': 1111, 'c4': 21111, 'c5': 211111, },\n",
				"    { 'c1': 12, 'c2': 121, 'c3': 1211, 'c4': 22111, 'c5': 221111, },\n",
				"    { 'c1': 13, 'c2': 131, 'c3': 1311, 'c4': 23111, 'c5': \u25a0\u25a0\u25a0\u25a0\u25a0, },\n",
				"    { 'c1': 21, 'c2': 211, 'c3': 2111, 'c4': 21111, 'c5': 211111, },\n",
				"    { 'c1': 22, 'c2': 221, 'c3': 2211, 'c4': 22111, 'c5': 221111, },\n",
				"    { 'c1': 23, 'c2': 231, 'c3': \u25a0\u25a0\u25a0\u25a0\u25a0, 'c4': 23111, 'c5': 231111, },\n",
				"    { 'c1': 24, 'c2': 241, 'c3': 2411, 'c4': 24111, 'c5': 241111, },\n",
				"    { 'c1': 25, 'c2': 251, 'c3': 2511, 'c4': 25111, 'c5': 251111, },\n",
				"    { 'c1': 26, 'c2': 261, 'c3': 2611, 'c4': 26111, 'c5': 261111, },\n",
				"])\n",
				"\n",
				"db_conn = sqlite3.connect(':memory:')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0('t1', db_conn, if_exists='replace')\n",
				"t2.to_sql('t2', db_conn, if_exists='replace')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# UNION"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.concat([t1, t2], sort=False)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df[['c1', 'c2', 'c3']]\n",
				"df = df.drop_duplicates()\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, c3\n",
				"  from t1\n",
				" union\n",
				"select c1, c2, c3\n",
				"  from t2\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"#print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# UNION ALL"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.concat([t1, t2], sort=False)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df[['c1', 'c2', 'c3']]\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, c3\n",
				"  from t1\n",
				" union all\n",
				"select c1, c2, c3\n",
				"  from t2\n",
				"'''\n",
				"df_from_db = \u25a0\u25a0\u25a0\u25a0\u25a0(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# RELATIVE COMPLEMENT"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"set_t2 = set(t2['c1'].tolist())\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = t1[~t1['c1'].isin(set_t2)]\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, c3\n",
				"  from t1\n",
				" where not exists (select 1\n",
				"                     from t2\n",
				"                    where t1.c1 = t2.c1)\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"#display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# INNER JOIN"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_t1 = t1.set_index(['c1'])\n",
				"df_t2 = t2.set_index(['c1'])\n",
				"df = df_t1.join(df_t2, how='inner', lsuffix='', rsuffix='_DEL')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df.reset_index()\n",
				"df = df[['c1', 'c2', 'c3', 'c4']]\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select t1.c1, t1.c2, t1.c3, t2.c4\n",
				"  from t1\n",
				" inner join t2\n",
				"         on t1.c1 = t2.c1\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# LEFT OUTER JOIN"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_t1 = t1.set_index(['c1'])\n",
				"df_t2 = t2.set_index(['c1'])\n",
				"df = df_t1.join(df_t2, how='left', lsuffix='', rsuffix='_DEL')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df.reset_index()\n",
				"df = df[['c1', 'c2', 'c3', 'c4']]\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select t1.c1, t1.c2, t1.c3, t2.c4\n",
				"  from t1\n",
				"  left outer join t2\n",
				"               on t1.c1 = t2.c1\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# GROUP BY"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ( 'c3', 'count')).reset_index()\n",
				"print(df.shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, count(c3) as cnt_c3\n",
				"  from t1\n",
				" group by c1, c2\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# WHERE"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = t2[t2['c4'] == 22111]\n",
				"df = df[['c1', 'c2', 'c3']]\n",
				"print(\u25a0\u25a0\u25a0\u25a0\u25a0shape)\n",
				"###display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, c3\n",
				"  from t2\n",
				" where c4 = 22111\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# HAVING"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. Python"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy\n",
				"df = t1.groupby(['c1', 'c2']).agg( cnt_c3 = ( 'c3', numpy.sum)).reset_index()\n",
				"df = df[df['cnt_c3'] > \u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"print(df.shape)\n",
				"##display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. SQL"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sql_select = '''\n",
				"select c1, c2, sum(c3) as cnt_c3\n",
				"  from t1\n",
				" group by c1, c2\n",
				"having sum(c3) > 1311\n",
				"'''\n",
				"df_from_db = pandas.read_sql(sql_select, db_conn)\n",
				"print(df_from_db.shape)\n",
				"###display(df_from_db)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc804\ucc98\ub9ac\ub97c \ud1b5\ud55c \ubaa8\ub378 \ub370\uc774\ud130\uc14b \uc0dd\uc131\ud558\uae30"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"import numpy as np\n",
				"import json\n",
				"\n",
				"import matplotlib.pyplot as plt\n",
				"\n",
				"from tqdm import tqdm\n",
				"\n",
				"import warnings\n",
				"warnings.filterwarnings('ignore')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### \ub370\uc774\ud130 \uc124\uba85\n",
				"\n",
				"#### \ud30c\uc77c \ubc0f \uc124\uba85\n",
				"\n",
				"- sales.csv - 2017~2019 3\uac1c\ub144\uce58 \ud310\ub9e4\ub370\uc774\ud130 \uc911 2019\ub144 \ub370\uc774\ud130\ub9cc \ud65c\uc6a9\n",
				"- product_hierarchy.csv - \ub300\uc911\uc18c \ub4f1 5\ub2e8\uacc4\ubcc4 \uc0c1\ud488\uadf8\ub8f9\ucf54\ub4dc \ubc0f \uc0c1\ud488 \uc0ac\uc774\uc988\n",
				"- store_cities.csv - \ub9e4\uc7a5\ub3c4\uc2dc, \uc720\ud615, \uba74\uc801\n",
				"\n",
				"#### \uceec\ub7fc(\ubcc0\uc218) \uc124\uba85\n",
				"\n",
				"- store_id - \ub9e4\uc7a5\ucf54\ub4dc\n",
				"- product_id - \uc0c1\ud488\ucf54\ub4dc\n",
				"- date - \ud310\ub9e4\uc77c\uc790\n",
				"- sales - \uc77c\ubcc4 \ud310\ub9e4\ub7c9\n",
				"- revenue - \uc77c\ubcc4 \uc774\uc775\uc561\n",
				"- stock - \uc77c\ubcc4 \uc7ac\uace0\ub7c9\n",
				"- price - \ud310\ub9e4\uac00/\uac00\uaca9\n",
				"- promotype1 - \ud310\ub9e4\ucc44\ub1101\uc758 \ud504\ub85c\ubaa8\uc1581 \uc720\ud615\n",
				"- promobin1 - \ud310\ub9e4\ucc44\ub1101 \ud504\ub85c\ubaa8\uc1581\uc758 \ud504\ub85c\ubaa8\uc158 binning \uc801\uc6a9\uc728\n",
				"- promotype2 - \ud310\ub9e4\ucc44\ub1102\uc758 \ud504\ub85c\ubaa8\uc1582 \uc720\ud615\n",
				"- promobin2 - \ud310\ub9e4\ucc44\ub1102 \ud504\ub85c\ubaa8\uc1582\uc758 \ud504\ub85c\ubaa8\uc158 binning \uc801\uc6a9\uc728\n",
				"- promodiscount2 - \ud504\ub85c\ubaa8\uc1582\uc758 \uc801\uc6a9\ub41c \ud560\uc778\uc728\n",
				"- promodiscounttype_2 - \ud504\ubaa8\ub85c\uc1582 \ud560\uc778\uc728 \uc720\ud615\n",
				"- product_length - \uc81c\ud488 \uac00\ub85c\uc0ac\uc774\uc988\n",
				"- product_depth - \uc81c\ud488 \uc138\ub85c\uc0ac\uc774\uc988\n",
				"- product_width - \uc81c\ud488 \ub192\uc774\uc0ac\uc774\uc988\n",
				"- hierarchy1_id - \uadf8\ub8f9\ucf54\ub4dc1\n",
				"- hierarchy2_id - \uadf8\ub8f9\ucf54\ub4dc2\n",
				"- hierarchy3_id - \uadf8\ub8f9\ucf54\ub4dc3\n",
				"- hierarchy4_id - \uadf8\ub8f9\ucf54\ub4dc4\n",
				"- hierarchy5_id - \uadf8\ub8f9\ucf54\ub4dc5\n",
				"- storetype_id - \ub9e4\uc7a5\uc720\ud615\ucf54\ub4dc\n",
				"- store_size - \ub9e4\uc7a5\uba74\uc801\n",
				"- city_id - \ub3c4\uc2dc\ucf54\ub4dc"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"#### \uacb0\uce21\uce58 \ucc98\ub9ac \ubc0f \ubaa8\ub378\ub9c1 \ubcc0\uc218 \ud65c\uc6a9 \ubc29\ubc95 \n",
				"#### \ubc94\uc8fc\ud615(\uba85\ubaa9\ud615 \ub610\ub294 \uc21c\uc11c\ud615) - \uac01 \uc870\uac74\ubcc4\ub85c One-hot encoding\n",
				"#### \uc5f0\uc18d\ud615(\uc218\uce58\ud615) - \uad6c\uac04 \uc124\uc815 \ud6c4 \uac01 \uad6c\uac04\ubcc4\ub85c One-hot encoding - EDA\ub97c \ud1b5\ud574 \uad6c\uac04 \uc815\uc758 \ud544\uc694\n",
				"#### \ub610\ub294 \uc5f0\uc18d\ub370\uc774\ud130\ub85c \ud65c\uc6a9\n",
				"\n",
				"#### Shift \ubc0f Rolling\uc744 \ud65c\uc6a9\ud55c \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 (csv)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ud30c\uc77c \uacbd\ub85c \uc124\uc815\n",
				"PATH = os.path.abspath(\"./Data\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# sales_df = pd.read_csv(PATH+'sales.csv') #2019\ub144\ub3c4 \ub370\uc774\ud130\ub9cc \ucd94\ucd9c\n",
				"sales_df = pd.read_csv(PATH+'/sales_2019.csv')\n",
				"store_df = pd.read_csv(PATH+'/store_cities.csv')\n",
				"product_df = \u25a0\u25a0\u25a0\u25a0\u25a0(PATH+'/product_hierarchy.csv')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30(csv) -> \uc4f0\uae30(json) "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"file_handler = open(PATH+'/sample_json.csv', 'r')\n",
				"lines = file_handler.readlines()\n",
				"# print(lines)\n",
				"previous_data = None\n",
				"json_dictionary = {}\n",
				"for line in lines:\n",
				"    line_data = [data.strip() for \u25a0\u25a0\u25a0\u25a0\u25a0 in line.split(',')]\n",
				"    \n",
				"    # \uc774\uc804 \uc904\uc774 \uc788\uc744 \ub54c\uc5d0\ub9cc \uc218\ud589\n",
				"    if previous_data is not None:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0, p2, p3 = previous_data[0], previous_data[1], previous_data[2]\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0, c2, c3 = line_data[0], line_data[1], line_data[2]\n",
				"        \n",
				"        # level1\uc774 \ub2e4\ub978 \uacbd\uc6b0 \uc0c8\ub85c\uc6b4 level1, level2 \ud0a4\ub97c \ub9cc\ub4e4\uc5b4\uc11c \uac12 \ud558\ub098\ub97c \uc800\uc7a5\n",
				"        if p1 != c1: \n",
				"            json_dictionary[c1] = {}\n",
				"            json_dictionary[c1][c2]=c3\n",
				"        else:\n",
				"            # level2\uac00 \ub2e4\ub978 \uacbd\uc6b0 \uc0c8\ub85c\uc6b4 level2 \ud0a4\ub97c \ub9cc\ub4e4\uc5b4\uc11c \uac12 \ud558\ub098\ub97c \uc800\uc7a5\n",
				"            if p2 != c2:\n",
				"                json_dictionary[c1][\u25a0\u25a0\u25a0\u25a0\u25a0]=c3\n",
				"                # level1, level2 \uac00 \uc774\uc804\uacfc \uac19\uc73c\ubbc0\ub85c level3\uc758 \uac12\ub9cc \ucd94\uac00\ud558\uc5ec \uac31\uc2e0\n",
				"            else:\n",
				"                previous_value = json_dictionary[c1][c2]\n",
				"                json_dictionary[\u25a0\u25a0\u25a0\u25a0\u25a0][c2]= f'{previous_value}:{c3}'        \n",
				"        \n",
				"    previous_data = line_data\n",
				"\n",
				"file_handler.close()\n",
				"\n",
				"json_data = json.dumps(json_dictionary)\n",
				"print(json_data)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 (json) -> \uc4f0\uae30(csv)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"json_parsed = json.loads(json_data)\n",
				"columns = ['C1', 'C2', 'C3']\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = ','.join(columns)\n",
				"\n",
				"first_keys = sorted(list(json_parsed.keys()))\n",
				"for key1 in first_keys:\n",
				"    second_keys = sorted(list(json_parsed[key1].keys()))\n",
				"    for key2 in second_keys:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = json_parsed[key1][key2].split(':')\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 key3 in third_keys:\n",
				"            csv_string = csv_string + f'\\n{key1},{key2},{key3}'\n",
				"            \n",
				"file_handler = open('json_to_csv.csv', 'w')\n",
				"file_handler.write(csv_string)\n",
				"file_handler.close()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud544\uc694\ud55c \ub370\uc774\ud130 \uc720\uc9c0 (\ud544\uc694\uc5c6\ub294 \ubcc0\uc218 \uc0ad\uc81c)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#### drop \ud568\uc218 \uc0ac\uc6a9\ud558\uc5ec 'year' \ubcc0\uc218 \uc0ad\uc81c\n",
				"sales_df.drop('year', axis=1, inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#### \ud544\uc694 \ubcc0\uc218\ub9cc \uc720\uc9c0 - 'year' \ubcc0\uc218\ub9cc \uc81c\uc678\ud558\uace0 \uc720\uc9c0\n",
				"columns = ['product_id','store_id','date','sales','revenue','stock','price','promo_type_1','promo_bin_1','promo_type_2','promo_bin_2','promo_discount_2','promo_discount_type_2']\n",
				"sales_df = sales_df[columns]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uacb0\uce21\uce58 \ubcc0\uc218 \uc0ad\uc81c"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uacb0\uce21\uce58\uc5ec\ubd80 - \uac01 \ubcc0\uc218\ubcc4 \uacb0\uce21\uce58 \uac1c\uc218 \ud655\uc778\n",
				"print(sales_df.isna().sum())"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \"sales\" \ubcc0\uc218 \uae30\uc900\uc73c\ub85c \uacb0\uce21\uce58 \ud589 \uc0ad\uc81c\n",
				"#\ubc29\ubc95 1 - \uacb0\uce21\uce58\ub97c \uc81c\uc678\ud55c \uc778\ub371\uc2a4 \uc720\uc9c0\n",
				"sales_df = sales_df[sales_df['sales'].isna()!=\u25a0\u25a0\u25a0\u25a0\u25a0].reset_index(drop=True)\n",
				"#\ubc29\ubc95 2 - \uacb0\uce21\uce58 \uc778\ub371\uc2a4\ub97c \ub4dc\ub86d\ud55c \ub0a8\uc740 \uc778\ub371\uc2a4 \ucd94\ucd9c\n",
				"#sales_df = sales_df.iloc[sales_df['sales'].dropna().index].reset_index(drop=True)\n",
				""
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ud310\ub9e4\uc218\ub7c9 1\uc774\uc0c1\uc778 \ub370\uc774\ud130\ub9cc \uc720\uc9c0\n",
				"#sales_df = sales_df[sales_df.sales>0]\n",
				"#\uc778\ub371\uc2a4 \uc218\uc815\n",
				"#sales_df.reset_index(drop=True, inplace=True)\n",
				"\n",
				"#\ub610\ub294 \n",
				"sales_df = \u25a0\u25a0\u25a0\u25a0\u25a0[sales_df.sales>0].reset_index(drop=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uacb0\uce21\uce58 \ucc44\uc6b0\uae30"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ubb38\uc790\uc5f4 \ubcc0\uc218\uc758 \uacb0\uce21\uce58\ub294 ''\uc73c\ub85c \ubcc0\uacbd"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uacb0\uce21\uce58 \uc218\uc815\n",
				"sales_df.promo_bin_1.fillna('', inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uacb0\uce21\uce58 \uc218\uc815\n",
				"sales_df.promo_bin_2.fillna('', inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sales_df.promo_discount_type_2.fillna('', inplace=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uac00\uaca9 \uacb0\uce21\uce58 \ucc44\uc6b0\uae30"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uac00\uaca9 \ub370\uc774\ud130\uc640 \uacb0\uce21\uce58 \ub370\uc774\ud130 \ubd84\ub9ac\n",
				"#\uac00\uaca9\uc815\ubcf4 \uc720 \ub370\uc774\ud130 df1 \uc73c\ub85c \uc0dd\uc131\n",
				"df1 = sales_df[sales_df.price.isna()!=True]\n",
				"#\uac00\uaca9\uc815\ubcf4 \ubb34 \ub370\uc774\ud130 df0 \uc73c\ub85c \uc0dd\uc131\n",
				"df0 = sales_df[\u25a0\u25a0\u25a0\u25a0\u25a0()==True]\n",
				"\n",
				"#\uac00\uaca9\uc815\ubcf4 \uc5c6\ub294 \ub9e4\uc7a5\uc758 \uc0c1\ud488 \uc815\ubcf4\uac00 \uac00\uaca9\uc815\ubcf4\uac00 \uc788\ub294 \ub370\uc774\ud130\uc5d0\uc11c \uc874\uc7ac\ud558\ub294\uc9c0 \ud655\uc778\n",
				"#\uc0c1\ud488\ucf54\ub4dc/\ub9e4\uc7a5\ucf54\ub4dc \uae30\uc900\uc73c\ub85c \ubcd1\ud569\n",
				"#\ub0a0\uc9dc\ubcc4 \uac00\uaca9 \uc815\ubcf4 \ud3ec\ud568\n",
				"#df1\uc758 \uc0c1\ud488\ucf54\ub4dc/\ub9e4\uc7a5\ucf54\ub4dc/\uac00\uaca9/\ub0a0\uc9dc \uc640 df0\uc758 \uc0c1\ud488\ucf54\ub4dc/\ub9e4\uc7a5\ucf54\ub4dc\ub85c \uc0c1\ud488\ucf54\ub4dc/\ub9e4\uc7a5\ucf54\ub4dc \uae30\uc900\uc73c\ub85c \ubcd1\ud569 \ud6c4 dfz\ub85c \uc0dd\uc131\n",
				"dfz = pd.merge(df1[['product_id', 'store_id', 'price', 'date']], df0[['product_id', 'store_id']], \u25a0\u25a0\u25a0\u25a0\u25a0=['product_id', 'store_id'], how='inner')\n",
				"\n",
				"#\uc0c1\ud488\ucf54\ub4dc/\ub9e4\uc7a5\ucf54\ub4dc/\ub0a0\uc9dc/\ud310\ub9e4\uac00\uaca9 \uc21c\uc73c\ub85c \uc815\ub82c (\ub0a0\uc9dc\uc640 \ud310\ub9e4\uac00\uaca9\uc740 \ub0ae\uc740\uc21c\uc73c\ub85c)\n",
				"dfz = dfz.sort_values(['product_id', 'store_id', 'date', 'price'], ascending=[True, True, \u25a0\u25a0\u25a0\u25a0\u25a0, True]).reset_index(drop=True)\n",
				"\n",
				"#\uac00\uaca9\uc815\ubcf4 \uc5c6\ub294 \ub370\uc774\ud130\uc5d0 \uac00\uaca9\uc815\ubcf4 \ubcd1\ud569\n",
				"#\uac00\uaca9\uc815\ubcf4 \uc5c6\ub294 \ub370\uc774\ud130\uc5d0\uc11c \uac00\uaca9 \ubcc0\uc218 \uc0ad\uc81c\ud6c4 \ubcd1\ud569\n",
				"df0.drop('price', axis=1, inplace=True)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pd.merge(df0, dfz, \u25a0\u25a0\u25a0\u25a0\u25a0=['product_id', 'store_id'], how='left')\n",
				"\n",
				"#\uc218\uc815 \ud6c4 \uac00\uaca9 \ud655\uc778 \uac74 df01\uc73c\ub85c \uc0dd\uc131\n",
				"df01 = df0[df0.price.isna()!=True]\n",
				"# df01.shape\n",
				"\n",
				"#\uc218\uc815 \ud6c4 \uac00\uaca9 \ubbf8\ud655\uc778 \uac74 df00\uc73c\ub85c \uc0dd\uc131\n",
				"df00 = df0[df0.price.isna()]\n",
				"# df00.shape\n",
				"\n",
				"#\uc218\uc815 \ud6c4\uc5d0\ub3c4 \ud655\uc778 \uc548\ub418\ub294 \uc0c1\ud488\uc740 \ub9e4\uc7a5\uad6c\ubd84 \uc5c6\uc774 \ub3d9\uc77c \uc0c1\ud488 \uac00\uaca9 \uc815\ubcf4 \ucd94\ucd9c\n",
				"dfz0 = dfz[dfz.product_id.isin(df00.product_id)]\n",
				"\n",
				"#\uc0c1\ud488\uc758 \ucd5c\uc18c\uac00\uaca9\uc73c\ub85c \ub9f5\ud551\ud558\uae30 \uc704\ud574 \ucd5c\uc18c\uac00\uaca9 \uae30\uc900\uc73c\ub85c \uadf8\ub8f9\ud551\n",
				"dfz0 = dfz0.groupby('product_id').min()['price'].reset_index()\n",
				"\n",
				"#dfz0\ub85c \uc0dd\uc131\ud55c \ucd5c\uace0\uac00\uaca9 \uc815\ubcf4\ub97c df00 \ud14c\uc774\ube14\uc5d0 \uc5c5\ub370\uc774\ud2b8 \n",
				"#df00\uc758 \uac00\uaca9 \ubcc0\uc218\ub97c \uc0ad\uc81c\uac00 \ud544\uc694\ud568 (dfz0\uc640 df00\uc5d0 \uac01 \uac00\uaca9\ubcc0\uc218\uac00 \uc788\uae30 \ub54c\ubb38\uc5d0, \ub3d9\uc77c\uba85\ubcc0\uc218\uac00 \uc874\uc7ac\uc2dc, _x\uc640 _y\ub85c \uac01\uac01 \uc0dd\uc131\ub428\n",
				"df00.drop('price', axis=1, inplace=True)\n",
				"df00 = pd.merge(df00, dfz0, on='product_id', how='left')\n",
				"\n",
				"#df1, df01, df00\uc73c\ub85c \uac00\uaca9\uc815\ubcf4\ub97c \ub2e4 \uc5c5\ub370\uc774\ud2b8 \ud55c \ud14c\uc774\ube14\uc744 sales_df_fixed\ub85c \ubcd1\ud569\n",
				"sales_df_fixed = pd.concat([df1, df01, df00]).reset_index(drop=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# One-hot encoding"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uac00\uaca9 \uc5c5\ub370\uc774\ud2b8 \uc774\ud6c4\uc5d0\ub3c4 \uac00\uaca9 \uc815\ubcf4\uac00 \uc5c6\ub294 \uc81c\ud488\uc758 \uacbd\uc6b0 \uc6d0\ud56b\uc778\ucf54\ub529\uc73c\ub85c \uac00\uaca9\uc720\ubb34 \uc5ec\ubd80 \ubcc0\uc218\ub85c \uad6c\ubd84\uc790 \uc0dd\uc131\n",
				"#\uac00\uaca9 \uacb0\uce21\uce58 \uc5ec\ubd80\n",
				"sales_df_fixed.loc[\u25a0\u25a0\u25a0\u25a0\u25a0['price'].isna()==True, 'price_ohe'] = 1\n",
				"sales_df_fixed.loc[sales_df_fixed['price'].isna()==False, 'price_ohe'] = 0\n",
				"sales_df_fixed['price_ohe'] = sales_df_fixed['price_ohe'].astype(int)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#promo_type_1 \uc720\ud615\ubcc4 \uc6d0\ud56b\uc778\ucf54\ub529\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 i \u25a0\u25a0\u25a0\u25a0\u25a0 sales_df_fixed.promo_type_1.unique().tolist():\n",
				"    if i == '':\n",
				"        continue\n",
				"    else:\n",
				"        #\ubcc0\uc218 \uc0dd\uc131\n",
				"        sales_df_fixed['promo_type_1_'+i] = 0\n",
				"        sales_df_fixed.loc[sales_df_fixed.promo_type_1==i, 'promo_type_1_'+i] = 1\n",
				"    \n",
				"#promo_type_1 \ubcc0\uc218 \uc0ad\uc81c\n",
				"sales_df_fixed.drop('promo_type_1', axis=1, inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uac01 \ubcc0\uc218\uc758 \uc720\ud615\ubcc4_\uc6d0\ud56b\uc778\ucf54\ub529\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = ['promo_bin_1', 'promo_type_2', 'promo_bin_2', 'promo_discount_type_2']\n",
				"for col in columns:\n",
				"    for i in sales_df_fixed[col].unique().tolist():\n",
				"        if i == '':\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"        else:\n",
				"            sales_df_fixed[col+'_'+i] = 0\n",
				"            sales_df_fixed.loc[sales_df_fixed[col]==i, col+'_'+i] = 1\n",
				"\n",
				"#\uc6d0\ud56b\uc778\ucf54\ub529 \uc644\ub8d0\ub41c \ubcc0\uc218 \uc0ad\uc81c\n",
				"sales_df_fixed.drop(columns, axis=1, inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ud560\uc778\uc728 \uc5c6\ub294 \uc815\ubcf4\ub294 0\uc73c\ub85c \ub300\uccb4\n",
				"sales_df_fixed.promo_discount_2.fillna(0, inplace=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"#\uac00\uaca9 \uc5c6\ub294 \uc815\ubcf4\ub294 0\uc73c\ub85c \ub300\uccb4\n",
				"sales_df_fixed.price.fillna(0, inplace=True)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uadf8\ub8f9\ud551"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#product_id \uae30\uc900\uc73c\ub85c \ud310\ub9e4\uc218\ub7c9 \ubc0f \uc774\uc775\uc561 \ucd1d\ud569\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = sales_df_fixed[['product_id', 'sales', 'revenue']].groupby('product_id').sum().reset_index()\n",
				"\n",
				"#store_id \uae30\uc900\uc73c\ub85c \ud310\ub9e4\uc218\ub7c9 \ubc0f \uc774\uc775\uc561 \ucd1d\ud569\n",
				"store_grouping = sales_df_fixed[['store_id', 'sales', 'revenue']].groupby('store_id').sum().reset_index()\n",
				"\n",
				"#product_id + store_id \uae30\uc900\uc73c\ub85c \ud310\ub9e4\uc218\ub7c9 \ubc0f \uc774\uc775\uc561 \ucd1d\ud569\n",
				"product_store_grouping = sales_df_fixed[['product_id', 'store_id', 'sales', 'revenue']].groupby(['product_id', 'store_id']).sum().reset_index()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ub9e4\uc7a5\ubcc4 \uc0c1\ud488\ubcc4 \ud310\ub9e4\uc218\ub7c9 pivot table\n",
				"store_product_pivot = \u25a0\u25a0\u25a0\u25a0\u25a0(index='product_id', columns='store_id', values='sales').fillna(0)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \uadf8\ub8f9\ud551 \ubc0f \ud569/\ud3c9\uade0/\ucd5c\uc18c/\ucd5c\ub300 \uac12 \uad6c\ud558\uae30"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uc0c1\ud488\uc758 \ucd5c\uc800\uac12 \uad6c\ud558\uae30 - 0\uac12 \uc81c\uc678\n",
				"sales_df_fixed0 = sales_df_fixed[sales_df_fixed.price_ohe==0]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = sales_df_fixed0[['product_id', 'price']].groupby('product_id').min().reset_index()\n",
				"\n",
				"#\uc0c1\ud488\uc758 \ucd5c\uace0\uac12 \uad6c\ud558\uae30\n",
				"max_price = sales_df_fixed0[['product_id', 'price']].groupby('product_id').max().reset_index()\n",
				"\n",
				"#\ucd5c\uc800/\ucd5c\uace0\uac12 \ube44\uad50\ud558\uae30\n",
				"price_compare = \u25a0\u25a0\u25a0\u25a0\u25a0(min_price, max_price, on='product_id')\n",
				"\n",
				"#\ubcc0\uc218\uba85 \ubcc0\uacbd\n",
				"price_compare.columns = ['product_id', 'min_price', 'max_price']\n",
				"\n",
				"#\ucd5c\uace0/\ucd5c\uc800\uac12 \ucc28\uc774\n",
				"price_compare['price_diff'] = price_compare['max_price'] - price_compare['min_price']"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ucd5c\uace0=\ucd5c\uc800 \ub3d9\uc77c \uc0c1\ud488\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = price_compare[price_compare.price_diff==0]\n",
				"\n",
				"# \ucd5c\uace0!=\ucd5c\uc800 \ub2e4\ub978 \uc0c1\ud488 \n",
				"diff_price = price_compare[price_compare.price_diff!=0]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud14c\uc774\ube14 \ubcd1\ud569 Merging & Concatenating left, inner, outer"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pd.merge(sales_df_fixed, store_df, on='store_id', how='left')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#storetype_id, city_id \uc720\ud615\ubcc4 \uc6d0\ud56b\uc778\ucf54\ub529\n",
				"columns = ['storetype_id', 'city_id']\n",
				"for \u25a0\u25a0\u25a0\u25a0\u25a0 in columns:\n",
				"    for i in df[col].unique().tolist():\n",
				"        #\ubcc0\uc218 \uc0dd\uc131\n",
				"        df[col+'_'+i] = 0\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0[df[col]==i, col+'_'+i] = 1\n",
				"    \n",
				"#promo_type_1 \ubcc0\uc218 \uc0ad\uc81c\n",
				"df.drop(columns, axis=1, inplace=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ub9e4\uc7a5\uba74\uc801\uc758 \ubd84\ud3ec\ub97c \ud655\uc778 \ud6c4 \ub300\uc911\uc18c \uae30\uc900\uc73c\ub85c \uad6c\ubd84\ud568\n",
				"#plt.hist(df['store_size'], bins=20);"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud568\uc218 \ud65c\uc6a9"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uc544\ub798 \ub9e4\uc7a5\uba74\uc801 \uae30\uc900\uc73c\ub85c 3\uac1c \uadf8\ub8f9\uc73c\ub85c \uad6c\ubd84 \n",
				"#0~30\n",
				"#30~50\n",
				"#50<\n",
				"\n",
				"def storesize_grouping(x):\n",
				"    if x<=30:\n",
				"        return 1\n",
				"    elif x<=50:\n",
				"        return 2\n",
				"    else:\n",
				"        return 3\n",
				"    \n",
				"df['storesize_grouping'] = df['store_size']\u25a0\u25a0\u25a0\u25a0\u25a0(storesize_grouping)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uad6c\ubd84\ud55c \ub9e4\uc7a5\uc744 store_size \uc6d0\ud56b\uc778\ucf54\ub529\uc73c\ub85c \ubcc0\ud658\n",
				"df.loc[df['storesize_grouping']==1, 'storesize_small'] = 1\n",
				"df.loc[df['storesize_grouping']==2, 'storesize_medium'] = 1\n",
				"df.loc[df['storesize_grouping']==3, 'storesize_large'] = 1\n",
				"\n",
				"#NA \uac12 0 \ubcc0\uacbd\n",
				"df[['storesize_small', 'storesize_medium', 'storesize_large']] = df[['storesize_small', 'storesize_medium', 'storesize_large']].fillna(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"#store_size \ubcc0\uc218 \uc0ad\uc81c\n",
				"df.drop(['store_size', 'storesize_grouping'], axis=1, inplace=True)\n",
				"\n",
				"## \ub9e4\uc7a5\uc0ac\uc774\uc988\ubcc4 \uc6d0\ud56b\uc778\ucf54\ub529\uc744 \ub450\ub2e8\uacc4 (\ubd84\ub9ac + \uc6d0\ud56b\uc778\ucf54\ub529)\uc774 \uc544\ub2cc \ud55c\ub2e8\uacc4\ub85c \uc2e4\ud589 \uac00\ub2a5\n",
				"\n",
				"#97\ub9cc\uac74\uc73c\ub85c \uacc4\uc18d \uc804\ucc98\ub9ac \ud558\ub294 \uac83 \ubcf4\ub2e4, 628\uac74\uc73c\ub85c \uc6b0\uc120 \uc804\ucc98\ub9ac \ud6c4 \ubcd1\ud569\ud558\ub294 \ubc29\ubc95\uc774 \ud6a8\uc728\uc801\uc784\n",
				"product_df = product_df[\u25a0\u25a0\u25a0\u25a0\u25a0(df.product_id.unique())].reset_index(drop=True)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uc0c1\ud488\ubcc4 \uac00\ub85c, \uc138\ub85c, \ub192\uc774\ub85c \ubd80\ud53c \uc0b0\ucd9c \ud568\uc218\ub97c \uc0dd\uc131\n",
				"def get_volume(x, y, z):\n",
				"    return x*y*z\n",
				"\n",
				"product_df['product_volume'] = get_volume(product_df['product_length'], \u25a0\u25a0\u25a0\u25a0\u25a0['product_depth'], product_df['product_width'])"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\uc0ac\uc774\uc988 \uc815\ubcf4\uac00 \ub204\ub77d\uc73c\ub85c \uacb0\uce21\uce58 \ud655\uc778\n",
				"product_df[product_df.product_volume.isna()]\n",
				"\n",
				"#cluster_0 \uae30\uc900\uc758 \ud3c9\uade0\uac12 \ub300\ucc44\n",
				"product_df.loc[(product_df['cluster_id']=='cluster_0')&(product_df['product_volume'].isna()), 'product_volume'] = product_df[product_df['cluster_id']=='cluster_0']['product_volume'].mean()\n",
				"\n",
				"#cluster_9 \uae30\uc900\uc758 \ud3c9\uade0\uac12 \ub300\uccb4\n",
				"product_df.loc[(product_df['cluster_id']=='cluster_9')&(product_df['product_volume']\u25a0\u25a0\u25a0\u25a0\u25a0()), 'product_volume'] = product_df[product_df['cluster_id']=='cluster_9']['product_volume'].mean()\n",
				"\n",
				"#\uc0c1\ud488 \ubd80\ud53c \uc0ac\uc774\uc988\ub294 \ubc31\ubd84\uc704\uc218 \uae30\uc900 10\ub2e8\uacc4\ub85c \uadf8\ub8f9\ud654\n",
				"labels = np.arange(1,11)\n",
				"product_df['volume_qcut'] = pd.qcut(product_df['product_volume'], q=10, labels=labels)\n",
				"\n",
				"\n",
				"#\ubd80\ud53c qcut \uc720\ud615\ubcc4 \uc6d0\ud56b\uc778\ucf54\ub529\n",
				"for \u25a0\u25a0\u25a0\u25a0\u25a0 in sorted(set(product_df.volume_qcut)):\n",
				"    #\ubcc0\uc218 \uc0dd\uc131\n",
				"    product_df['volume_'+str(i)] = 0\n",
				"    product_df.loc[product_df['volume_qcut']==i, 'volume_'+str(i)] = 1\n",
				"\n",
				"#\ub300\ubd84\ub958 \uc911\ubd84\ub958 \uae30\uc900\uc73c\ub85c \uc6d0\ud56b\uc778\ucf54\ub529 \n",
				"#hierarchy1_id, hierarchy2_id \uc720\ud615\ubcc4 \uc6d0\ud56b\uc778\ucf54\ub529\n",
				"columns = ['cluster_id', 'hierarchy1_id', 'hierarchy2_id']\n",
				"for \u25a0\u25a0\u25a0\u25a0\u25a0 \u25a0\u25a0\u25a0\u25a0\u25a0 columns:\n",
				"    for i in \u25a0\u25a0\u25a0\u25a0\u25a0[col].unique().tolist():\n",
				"        #\ubcc0\uc218 \uc0dd\uc131\n",
				"        product_df[i] = 0\n",
				"        product_df.loc[product_df[col]==i, i] = 1\n",
				"\n",
				"#\uc6d0\ud56b\uc778\ucf54\ub529 \ub4f1 \ubcc0\uc218\uc0dd\uc131\uc774 \uc644\ub8cc\ub41c \uc6d0\ub798 \ubcc0\uc218\ub294 \uc0ad\uc81c\n",
				"product_df.drop(['product_length', 'product_depth', 'product_width', 'product_volume', 'volume_qcut', 'cluster_id', 'hierarchy1_id', 'hierarchy2_id', 'hierarchy3_id', 'hierarchy4_id', 'hierarchy5_id'], axis=1, inplace=True)\n",
				"\n",
				"#\ubcc0\uc218 \uc804\ucc98\ub9ac\uac00 \uc644\ub8cc\ub41c \uc0c1\ud488 \ub370\uc774\ud130\ub97c df\ub370\uc774\ud130\uc640 \ubcd1\ud569\n",
				"df = pd.merge(df, product_df, on='product_id', how='left')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130 \uc0dd\uc131\ud558\uae30 Shifting\uacfc Rolling"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"#\ub0a0\uc9dc\ubcc4 \ud310\ub9e4\ub7c9 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130 \uc0dd\uc131\n",
				"ts = df[['date', 'sales']]\n",
				"\n",
				"#\ub0a0\uc9dc\ubcc4\ub85c \ucd1d \ud310\ub9e4\ub7c9 \uadf8\ub8f9\ud551\n",
				"ts = ts.groupby('date')\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"\n",
				"#\ud310\ub9e4\ub7c9 Shifiting \n",
				"ts['sales_p1'] = ts.sales.shift(1)\n",
				"ts['sales_m1'] = ts.sales.shift(-1)\n",
				"\n",
				"#\ud310\ub9e4\ub7c9 5\ub2e8\uc704\ub85c Rolling\n",
				"#\ud569\n",
				"ts['sales_sum5'] = ts.sales.rolling(5)\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"#\ucd5c\uc18c\n",
				"ts['sales_min5'] = ts.sales.rolling(5).min()\n",
				"#\ucd5c\ub300\n",
				"ts['sales_max5'] = ts.sales.rolling(5).max()\n",
				"#\ud3c9\uade0\n",
				"ts['sales_avg5'] = ts.sales.rolling(5).mean()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc2dc\uac01\ud654 \uae30\ucd08"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc774\uc6a9 \ud30c\uc77c \uba85\n",
				"Groceries_dataset.csv\n",
				"\n",
				"# \uc774\uc6a9 \uceec\ub7fc \uba85\n",
				"['Member_number', 'Date', 'itemDescription']"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy\n",
				"import pandas\n",
				"import seaborn as sns\n",
				"import matplotlib.pyplot as plt\n",
				"import matplotlib.font_manager as fm"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud654\uba74 \ud45c\uc2dc \ubc29\uc2dd \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%matplotlib inline\n",
				"plt.rcParams['font.size'] = 20\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0[\"figure.figsize\"] = (50, 10)\n",
				"plt.rcParams['lines.linewidth'] = 2\n",
				"plt.rcParams[\"axes.grid\"] = True"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"<style>\n",
				"    table { display: inline-block }\n",
				"    .rendered_html td, .rendered_html th { text-align: left; }\n",
				"</style>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc0d8\ud50c\uc6a9 \ub370\uc774\ud130 \ub85c\ub529"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n",
				"##display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Pie Chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 350\n",
				"df['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_pie = df_pie.sort_values(by='CNT', ascending=False)\n",
				"df_pie_top = df_pie.head(4)\n",
				"df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n",
				"print(df_pie.shape, \u25a0\u25a0\u25a0\u25a0\u25a0shape, df_pie_oth.shape)\n",
				"# #display(df_pie)\n",
				"\n",
				"list_labels = df_pie_top.index.tolist()\n",
				"list_labels.append('ETC')\n",
				"\n",
				"list_data = df_pie_top['CNT'].tolist()\n",
				"list_data.append(numpy.sum(df_pie_oth['CNT']))\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"colors = sns.color_palette('spring')[\u25a0\u25a0\u25a0\u25a0\u25a0:\u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"# plt.pie(list_data, labels=list_labels, colors=colors, autopct='%.0f%%')\n",
				"#plt.pie(list_data, labels=list_labels, autopct='%.0f%%')\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Bar chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 350\n",
				"df['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_pie = \u25a0\u25a0\u25a0\u25a0\u25a0(by='CNT', ascending=False)\n",
				"df_pie['CNT'] = df_pie['CNT'] - 3200\n",
				"df_pie_top = df_pie.head(9)\n",
				"df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n",
				"print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n",
				"# #display(df_pie)\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_pie_top.index.tolist()\n",
				"# list_labels.append('ETC')\n",
				"\n",
				"list_data = df_pie_top['CNT']\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"# list_data.append(numpy.sum(df_pie_oth['CNT']))\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"ax = sns.barplot(x=list_data, y=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Column chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 350\n",
				"df['key'] = df['key']\u25a0\u25a0\u25a0\u25a0\u25a0(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_pie = df.groupby(['key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_pie = df_pie.sort_values(by='CNT', \u25a0\u25a0\u25a0\u25a0\u25a0=False)\n",
				"df_pie['CNT'] = df_pie['CNT'] - 3200\n",
				"df_pie_top = df_pie.head(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df_pie_oth = df_pie.tail(\u25a0\u25a0\u25a0\u25a0\u25a0shape[0] - 9)\n",
				"print(df_pie.shape, df_pie_top.shape, df_pie_oth.shape)\n",
				"# #display(df_pie)\n",
				"\n",
				"list_labels = df_pie_top.index.tolist()\n",
				"# list_labels.append('ETC')\n",
				"\n",
				"list_data = df_pie_top['CNT'].tolist()\n",
				"# list_data.append(numpy.sum(df_pie_oth['CNT']))\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"ax = sns.barplot(y=list_data, x=list_labels)\n",
				"\n",
				"#plt.show()\n",
				"\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Line chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 350\n",
				"df['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_pie = df.groupby(['key'])\u25a0\u25a0\u25a0\u25a0\u25a0( CNT = ('itemDescription', 'count'))\n",
				"df_pie = df_pie.sort_values(by='CNT', ascending=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df_pie['CNT'] = df_pie['CNT'] - 3200\n",
				"df_pie_top = df_pie.head(9)\n",
				"df_pie_oth = df_pie.tail(df_pie.shape[0] - 9)\n",
				"print(df_pie.shape, df_pie_top.shape, \u25a0\u25a0\u25a0\u25a0\u25a0shape)\n",
				"# #display(df_pie)\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_pie_top.index.tolist()\n",
				"# list_labels.append('ETC')\n",
				"\n",
				"list_data = df_pie_top['CNT'].tolist()\n",
				"# list_data.append(numpy.sum(df_pie_oth['CNT']))\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"#plt.rcParams['lines.linewidth'] = 10\n",
				"ax = sns.lineplot(y=list_data, x=list_labels)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Area chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 20\n",
				"df['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + \u25a0\u25a0\u25a0\u25a0\u25a0['key']\n",
				"df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n",
				"\n",
				"df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = ['PV-' + \u25a0\u25a0\u25a0\u25a0\u25a0(col) for col in df_transpose.columns.values]\n",
				"df_transpose = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"# #display(df_transpose)\n",
				"df_transpose = df_transpose.set_index(['Date'])\n",
				"df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[\u25a0\u25a0\u25a0\u25a0\u25a0]) * 10\n",
				"df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose = df_transpose.head(20)\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"#plt.rcParams['lines.linewidth'] = 10\n",
				"#plt.stackplot(df_transpose.index, df_transpose['PV-Piece-90'], df_transpose['PV-Piece-80'], df_transpose['PV-Piece-70'], df_transpose['PV-Piece-60'])\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Scatter chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 20\n",
				"df['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n",
				"\n",
				"df_transpose = pandas.pivot_table(\u25a0\u25a0\u25a0\u25a0\u25a0, \u25a0\u25a0\u25a0\u25a0\u25a0='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=0)\n",
				"df_transpose.columns = ['PV-' + str(col) for \u25a0\u25a0\u25a0\u25a0\u25a0 in df_transpose.columns.values]\n",
				"df_transpose = df_transpose.reset_index()\n",
				"# #display(df_transpose)\n",
				"df_transpose = \u25a0\u25a0\u25a0\u25a0\u25a0(['Date'])\n",
				"df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(\u25a0\u25a0\u25a0\u25a0\u25a0shape[0]) * 10\n",
				"df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose = df_transpose.head(20)\n",
				"\n",
				"sns.set(\u25a0\u25a0\u25a0\u25a0\u25a0='darkgrid')\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"ax = sns.scatterplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'], s=500)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Secondary Axis"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 20\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['key'] = df['key'].astype(int)\n",
				"df['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_groupby = df.groupby(['Date', 'key'])\u25a0\u25a0\u25a0\u25a0\u25a0( CNT = ('itemDescription', 'count'))\n",
				"df_groupby = df_groupby.sort_values(by='CNT', ascending=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.pivot_table(df_groupby, \u25a0\u25a0\u25a0\u25a0\u25a0='CNT', index=['Date'], columns=['key'], aggfunc=\u25a0\u25a0\u25a0\u25a0\u25a0, fill_value=0)\n",
				"df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n",
				"df_transpose = df_transpose.reset_index()\n",
				"# #display(df_transpose)\n",
				"df_transpose = df_transpose.set_index(['Date'])\n",
				"df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-90'] = df_transpose['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10000\n",
				"df_transpose['PV-Piece-70'] = df_transpose['PV-Piece-70'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-60'] = df_transpose['PV-Piece-60'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose = df_transpose.head(20)\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"#plt.rcParams['lines.linewidth'] = 7\n",
				"\n",
				"ax = sns.barplot(x=df_transpose.index, y=df_transpose['PV-Piece-90'])\n",
				"ax2 = plt.twinx()\n",
				"ax = sns.lineplot(x=df_transpose.index, y=df_transpose['PV-Piece-60'], ax=ax2)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Heatmap chart"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 20\n",
				"df['key'] = \u25a0\u25a0\u25a0\u25a0\u25a0['key'].astype(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df['key'] = df['key'].astype(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"df_groupby = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n",
				"\n",
				"df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=\u25a0\u25a0\u25a0\u25a0\u25a0, fill_value=0)\n",
				"df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n",
				"df_transpose = df_transpose.reset_index()\n",
				"df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose['PV-Piece-90'] = \u25a0\u25a0\u25a0\u25a0\u25a0['PV-Piece-90'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose = df_transpose[['PV-Piece-80', 'PV-Piece-90']]\n",
				"df_transpose = df_transpose.astype(int)\n",
				"df_transpose['CNT'] = 1\n",
				"df_chart = pandas.pivot_table(df_transpose, values='CNT', index=['PV-Piece-90'], columns=['PV-Piece-80'], aggfunc=\u25a0\u25a0\u25a0\u25a0\u25a0, fill_value=0)\n",
				"# #display(df_chart)\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"\n",
				"#plt.rcParams['lines.linewidth'] = 7\n",
				"\n",
				"sns.set(font_scale=1.5)\n",
				"heat_map = sns.heatmap(df_chart, annot=False, cmap='coolwarm', robust=True, fmt='.0f', linewidths=.5)\n",
				"heat_map.tick_params(labelsize=10)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Box plot"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df['key'] = df['Member_number'] / 20\n",
				"df['key'] = \u25a0\u25a0\u25a0\u25a0\u25a0['key'].astype(int)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['key'] = df['key'].astype(str)\n",
				"df['key'] = 'Piece-' + df['key']\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df.groupby(['Date', 'key']).agg( CNT = ('itemDescription', 'count'))\n",
				"df_groupby = df_groupby.sort_values(by='CNT', ascending=False)\n",
				"\n",
				"df_transpose = pandas.pivot_table(df_groupby, values='CNT', index=['Date'], columns=['key'], aggfunc=numpy.sum, fill_value=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df_transpose.columns = ['PV-' + str(col) for col in df_transpose.columns.values]\n",
				"df_transpose = df_transpose.reset_index()\n",
				"df_transpose['PV-Piece-80'] = df_transpose['PV-Piece-80'] + numpy.random.rand(df_transpose.shape[0]) * 10\n",
				"df_transpose = df_transpose[['PV-Piece-80']]\n",
				"# #display(df_transpose)\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"#plt.rcParams['lines.linewidth'] = 7\n",
				"\n",
				"sns.set(font_scale=1.5)\n",
				"ax = sns.boxplot(x = \"PV-Piece-80\",  data = df_transpose)\n",
				"\n",
				"#plt.show()\n",
				""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 5. \ube44\uc9c0\ub3c4 \ud559\uc2b5 \uae30\ucd08"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc774\uc6a9 \ud30c\uc77c \uba85\n",
				"Groceries_dataset.csv\n",
				"\n",
				"# \uc774\uc6a9 \uceec\ub7fc \uba85\n",
				"['Member_number', 'Date', 'itemDescription']"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import mlxtend.frequent_patterns\n",
				"import numpy\n",
				"import pandas\n",
				"import matplotlib.pyplot as plt\n",
				"import matplotlib.font_manager as fm\n",
				"import seaborn as sns\n",
				"import warnings\n",
				"import yellowbrick.cluster\n",
				"\n",
				"import sklearn.cluster\n",
				"import sklearn.metrics\n",
				"import sklearn.decomposition"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%matplotlib inline\n",
				"plt.rcParams['font.size'] = 20\n",
				"plt.rcParams[\"figure.figsize\"] = (\u25a0\u25a0\u25a0\u25a0\u25a0, 10)\n",
				"plt.rcParams['lines.linewidth'] = 5\n",
				"plt.rcParams[\"axes.grid\"] = True"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%html\n",
				"<style>\n",
				"    table { display: inline-block }\n",
				"    .rendered_html td, .rendered_html \u25a0\u25a0\u25a0\u25a0\u25a0 { text-align: left; }\n",
				"</style>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(os.path.abspath(\"./Data\") + '/Groceries_dataset.csv')\n",
				"#display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc870\uac74 \ubc0f \ud30c\ub77c\ubbf8\ud130 \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"dict_args = dict()\n",
				"\n",
				"dict_args['id_col_name'] = 'Member_number'\n",
				"dict_args['sequence_col_name'] = 'Date'\n",
				"dict_args['product_col_name'] = 'itemDescription'\n",
				"dict_args['base_file'] = os.path.abspath(\"./Data\") + '/Groceries_dataset.csv'\n",
				"\n",
				"# \ucd5c\uc18c \uc9c0\uc9c0\ub3c4\n",
				"dict_args['min_support'] = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"# \ucd5c\uc18c \uc2e0\ub8b0\ub3c4\n",
				"dict_args['min_threshold'] = 0.01\n",
				"print(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"set_items = set()\n",
				"df_source = pandas.read_csv(dict_args['base_file'])\n",
				"df_source = df_source.sort_values(by=[dict_args['id_col_name'], dict_args['sequence_col_name'], dict_args['product_col_name']], ascending=True)\n",
				"df_source[dict_args['product_col_name']] = df_source[dict_args['product_col_name']].str.strip()\n",
				"df_source['constants'] = 1\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = \u25a0\u25a0\u25a0\u25a0\u25a0(df_source, values='constants', index=[dict_args['id_col_name']], \u25a0\u25a0\u25a0\u25a0\u25a0=[dict_args['product_col_name']], aggfunc=numpy.sum, fill_value=0)\n",
				"for col_name in df_ar.columns:\n",
				"    df_ar[col_name] = numpy.where(df_ar[col_name] > 1, 1, df_ar[col_name])\n",
				"#display(df_ar)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc5f0\uad00\uc131\ubd84\uc11d (Association Rule) \uc2e4\ud589"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"freq_items = mlxtend.frequent_patterns.apriori(df_ar, min_support=dict_args['min_support'], use_colnames=True, verbose=1)\n",
				"df_rules = mlxtend.frequent_patterns.association_rules(freq_items, metric=\"confidence\", min_threshold=dict_args['min_threshold'])\u25a0\u25a0\u25a0\u25a0\u25a0(by = ['lift', 'confidence', 'support'], \u25a0\u25a0\u25a0\u25a0\u25a0 =False)\n",
				"df_rules = df_rules.sort_values(by=['support', 'lift'], ascending=False)\n",
				"#display(df_rules)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_plot = \u25a0\u25a0\u25a0\u25a0\u25a0(by='support', ascending=False)\n",
				"df_plot['index'] = numpy.arange(df_plot.shape[0])\n",
				"df_plot = df_plot.set_index(['index'])\n",
				"df_plot = df_plot[['support']]\n",
				"#df_plot.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc870\uac74 \ubc0f \ud30c\ub77c\ubbf8\ud130 \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = dict()\n",
				"dict_args['cluster_count'] = [3]\n",
				"dict_args['base_file'] = os.path.abspath(\"./Data\") + '/OnlineRetail.csv'\n",
				"dict_args['min_cluster_size'] = 2\n",
				"print(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(dict_args['base_file'], encoding='cp1252')\n",
				"#display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df.copy()\n",
				"df_source['CustomerID'] = df_source['CustomerID'].fillna(-1)\n",
				"df_source['CustomerID'] = \u25a0\u25a0\u25a0\u25a0\u25a0['CustomerID'].astype(float)\n",
				"df_source['CustomerID'] = df_source['CustomerID'].astype(int)\n",
				"df_source['CustomerID'] = df_source['CustomerID'].astype(str)\n",
				"df_source['Amount'] = df_source['UnitPrice'] * df_source['Quantity']\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['InvoiceDate'] = pandas.to_datetime(df_source['InvoiceDate'],format='%d-%m-%Y %H:%M')\n",
				"\n",
				"# Recency\n",
				"df_max = numpy.max(df_source['InvoiceDate'])\n",
				"df_source['diff'] = df_max - df_source['InvoiceDate']\n",
				"df_diffs = df_source.groupby(['CustomerID']).agg( Recency = ('diff', numpy.min) )\n",
				"df_diffs['Recency'] = df_diffs['Recency'].dt.days\n",
				"# Frequency\n",
				"df_frequency = df_source.groupby(['CustomerID']).agg( Frequency = ('InvoiceNo', 'count') )\n",
				"# Monetary\n",
				"df_amount = df_source.groupby('CustomerID')['Amount'].sum()\n",
				"\n",
				"df_final = df_diffs.join(\u25a0\u25a0\u25a0\u25a0\u25a0, how='inner')\n",
				"df_final = \u25a0\u25a0\u25a0\u25a0\u25a0(df_amount, how='inner')\n",
				"df_final = df_final.fillna(-1)\n",
				"# df_source = df_source.sample(n=20000)\n",
				"#display(df_final)\n",
				"df_final = df_final[df_final.index != '-1']\n",
				"#display(df_final)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uad70\uc9d1\ubd84\uc11d (Clustering) \uc2e4\ud589"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_base_cluster_source = df_final.copy()\n",
				"\n",
				"df_cluster_lst = pandas.DataFrame()\n",
				"\n",
				"cluster_number = 1\n",
				"loop_count = 0\n",
				"while \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"    df_cluster_con = pandas.DataFrame()\n",
				"    df_cluster_raw = pandas.DataFrame()\n",
				"\n",
				"    if df_base_cluster_source.shape[0] < dict_args['min_cluster_size']:\n",
				"        #display(df_cluster_lst)\n",
				"        #display(df_base_cluster_source)\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=False)\n",
				"        break\n",
				"\n",
				"    for n_clusters in dict_args['cluster_count']:\n",
				"        kmeans = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10, init='k-means++')\n",
				"        kmeans.fit(df_base_cluster_source)\n",
				"        cluster_labels = kmeans.fit_predict(df_base_cluster_source)\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0['cluster'] = cluster_labels\n",
				"\n",
				"        silhouette_avg = sklearn.metrics.silhouette_score(df_base_cluster_source, cluster_labels)\n",
				"        score_samples = sklearn.metrics.silhouette_samples(df_base_cluster_source, cluster_labels)\n",
				"        df_base_cluster_source['silhouette_coeff'] = score_samples\n",
				"        df_silhouette_coeff = df_base_cluster_source.groupby(['cluster'])\u25a0\u25a0\u25a0\u25a0\u25a0(\n",
				"            silhouette_coeff_AVG = ('silhouette_coeff', numpy.mean),\n",
				"            silhouette_coeff_STD = ('silhouette_coeff', numpy.std),\n",
				"            silhouette_coeff_CNT = ('silhouette_coeff', 'count'),\n",
				"        )\n",
				"        df_silhouette_coeff['CoV'] = df_silhouette_coeff['silhouette_coeff_STD'] / df_silhouette_coeff['silhouette_coeff_AVG'] * 100\n",
				"\n",
				"        df_silhouette_coeff['n_clusters'] = n_clusters\n",
				"        df_base_cluster_source['n_clusters'] = n_clusters\n",
				"        df_cluster_con = pandas.concat([\u25a0\u25a0\u25a0\u25a0\u25a0, df_silhouette_coeff], sort=False)\n",
				"        df_cluster_raw = pandas.concat([df_cluster_raw, df_base_cluster_source], sort=False)\n",
				"\n",
				"        print('Cluster:', n_clusters, ', Silhouette_score:', silhouette_avg, ', Inertia:', kmeans.inertia_, df_base_cluster_source['cluster']\u25a0\u25a0\u25a0\u25a0\u25a0())\n",
				"\n",
				"    df_cluster_con = df_cluster_con.sort_values(by=['CoV'], \u25a0\u25a0\u25a0\u25a0\u25a0=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"    # #display(df_cluster_con)\n",
				"    df_cluster_con = df_cluster_con[df_cluster_con['silhouette_coeff_CNT'] > dict_args['min_cluster_size']]\n",
				"    df_cluster_con = \u25a0\u25a0\u25a0\u25a0\u25a0[df_cluster_con['silhouette_coeff_CNT'] < int(df_base_cluster_source.shape[0] * 0.3)]\n",
				"    if df_cluster_con.shape[0] == 0:\n",
				"        df_base_cluster_source['cluster_number'] = cluster_number\n",
				"        try:\n",
				"            del df_base_cluster_source['cluster']\n",
				"        except:\n",
				"            pass\n",
				"        try:\n",
				"            del df_base_cluster_source['silhouette_coeff']\n",
				"        except:\n",
				"            pass\n",
				"        try:\n",
				"            del df_base_cluster_source['n_clusters']\n",
				"        except:\n",
				"            pass\n",
				"        df_cluster_lst = pandas.concat([df_cluster_lst, df_base_cluster_source], sort=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"        break\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = \u25a0\u25a0\u25a0\u25a0\u25a0(1)['CoV'].tolist()[\u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"    max_n_clusters = df_cluster_con.head(1)['n_clusters'].tolist()[0]\n",
				"    max_cluster = df_cluster_con.head(1).index.tolist()[0]\n",
				"    df_selected = df_cluster_raw[df_cluster_raw['n_clusters'] == max_n_clusters]\n",
				"    df_selected = df_selected[df_selected['cluster'] == max_cluster]\n",
				"    df_selected['cluster_number'] = cluster_number\n",
				"    # cluster\tsilhouette_coeff\tn_clusters\n",
				"    try:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 df_selected['cluster']\n",
				"    except:\n",
				"        pass\n",
				"    try:\n",
				"        del df_selected['silhouette_coeff']\n",
				"    except:\n",
				"        pass\n",
				"    try:\n",
				"        del df_selected['n_clusters']\n",
				"    except:\n",
				"        pass\n",
				"    df_cluster_lst = pandas.concat([df_cluster_lst, df_selected], sort=False)\n",
				"    print('Cluster:', max_n_clusters, ', Silhouette_score:', max_silhouette_coeff_AVG)\n",
				"    set_selected = set(df_selected.index.tolist())\n",
				"    df_base_cluster_source = df_base_cluster_source[~df_base_cluster_source.index.isin(set_selected)]\n",
				"    print(df_base_cluster_source.shape, df_cluster_lst.shape, df_selected.shape)\n",
				"    # #display(df_cluster_con)\n",
				"    # #display(df_cluster_raw)\n",
				"    # #display(df_selected)\n",
				"\n",
				"    loop_count += 1\n",
				"    cluster_number += 1\n",
				"    if loop_count > 1:\n",
				"        break\n",
				"#display(df_cluster_lst)\n",
				"#display(df_cluster_lst.groupby(['cluster_number']).agg(CNT=('cluster_number', 'count')))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uacb0\uacfc \ud655\uc778 - \uc2e4\ub8e8\uc5e3 \uacc4\uc218 \uc2dc\uac01\ud654"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_cluster_display = df_cluster_lst.copy()\n",
				"\n",
				"#clusterer = sklearn.cluster.KMeans(n_clusters=n_clusters, random_state=10)\n",
				"#cluster_labels = clusterer.fit_predict(df_cluster_display)\n",
				"#visualizer_2 = yellowbrick.cluster.SilhouetteVisualizer(clusterer, colors='yellowbrick')\n",
				"#visualizer_2.fit(df_cluster_display)      \n",
				"#visualizer_2.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uacb0\uacfc \ud655\uc778 - \ubd84\ud3ec(3D) \uc2dc\uac01\ud654"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"from matplotlib.colors import ListedColormap\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 mpl_toolkits.mplot3d import Axes3D\n",
				"\n",
				"list_color_set = [ '#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', '#ff7c43', '#ffa600', '#004c6d' ]\n",
				"\n",
				"df_cluster_display = df_cluster_lst.copy()\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = ['Recency', 'Frequency', 'Amount', 'cluster_number']\n",
				"\n",
				"sns.set(style = \"darkgrid\")\n",
				"\n",
				"col_x = df_cluster_display[list_cols[0]]\n",
				"col_y = df_cluster_display[list_cols[1]]\n",
				"col_z = \u25a0\u25a0\u25a0\u25a0\u25a0display[list_cols[2]]\n",
				"col_c = df_cluster_display[list_cols[3]]\n",
				"\n",
				"# fig = plt.figure()\n",
				"# ax = fig.add_subplot(111, projection = '3d')\n",
				"fig = plt.figure(figsize=(50, 10))\n",
				"ax = Axes3D(fig, auto_add_to_figure=False)\n",
				"#fig.add_axes(ax)\n",
				"\n",
				"ax.set_xlabel(list_cols[0])\n",
				"ax.set_ylabel(list_cols[1])\n",
				"ax.set_zlabel(list_cols[2])\n",
				"\n",
				"#cmap = ListedColormap(sns.color_palette(\"husl\", 256).as_hex())\n",
				"#sc = ax.scatter(col_x, col_y, col_z, s=40, c=col_c, marker='o', cmap=cmap, alpha=1)\n",
				"\n",
				"# ax.scatter(col_x, col_y, col_z)\n",
				"#plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\n",
				"\n",
				"#plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Scikit-learn \ud648\ud398\uc774\uc9c0 \uc608\uc2dc\n",
				"#### https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0 sklearn.datasets import make_blobs\n",
				"from sklearn.cluster import KMeans\n",
				"from sklearn.metrics import silhouette_samples, silhouette_score\n",
				"\n",
				"import matplotlib.pyplot as plt\n",
				"import matplotlib.cm as cm\n",
				"import numpy as np\n",
				"\n",
				"# Generating the sample data from make_blobs\n",
				"# This particular setting has one distinct cluster and 3 clusters placed close\n",
				"# together.\n",
				"# For reproducibility\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0, y = make_blobs(\n",
				"    n_samples=10,\n",
				"    n_features=2,\n",
				"    centers=1,\n",
				"    cluster_std=1,\n",
				"    center_box=(-1.0, 1.0),\n",
				"    shuffle=True,\n",
				"    random_state=1,)  \n",
				"\n",
				"range_n_clusters = [3]\n",
				"for n_clusters in range_n_clusters:\n",
				"    # Create a subplot with 1 row and 2 columns\n",
				"    fig, (ax1, ax2) = \u25a0\u25a0\u25a0\u25a0\u25a0(1, 2)\n",
				"    fig.set_size_inches(18, 7)\n",
				"\n",
				"    # The 1st subplot is the silhouette plot\n",
				"    # The silhouette coefficient can range from -1, 1 but in this example all\n",
				"    # lie within [-0.1, 1]\n",
				"    ax1.set_xlim([-0.1, 1])\n",
				"    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
				"    # plots of individual clusters, to demarcate them clearly.\n",
				"    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
				"\n",
				"    # Initialize the clusterer with n_clusters value and a random generator\n",
				"    # seed of 10 for reproducibility.\n",
				"    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
				"    cluster_labels = clusterer.fit_predict(X)\n",
				"    df_count = pandas.DataFrame(cluster_labels)\n",
				"    df_count.columns = ['label']\n",
				"    df_groupby = df_count.groupby(['label']).agg( CNT = ('label', 'count' ))\n",
				"\n",
				"    # The silhouette_score gives the average value for all the samples.\n",
				"    # This gives a perspective into the density and separation of the formed\n",
				"    # clusters\n",
				"    silhouette_avg = silhouette_score(X, cluster_labels)\n",
				"    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg, df_groupby['CNT'].tolist())\n",
				"\n",
				"    # Compute the silhouette scores for each sample\n",
				"    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
				"\n",
				"    y_lower = 10\n",
				"    for i in range(n_clusters):\n",
				"        # Aggregate the silhouette scores for samples belonging to\n",
				"        # cluster i, and sort them\n",
				"        ith_cluster_silhouette_values = \u25a0\u25a0\u25a0\u25a0\u25a0[cluster_labels == i]\n",
				"\n",
				"        ith_cluster_silhouette_values.sort()\n",
				"\n",
				"        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
				"        y_upper = y_lower + size_cluster_i\n",
				"\n",
				"        color = cm.nipy_spectral(float(i) / n_clusters)\n",
				"        ax1.fill_betweenx(\n",
				"            np.arange(y_lower, y_upper),\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0,\n",
				"            ith_cluster_silhouette_values,\n",
				"            facecolor=color,\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0=color,\n",
				"            alpha=\u25a0\u25a0\u25a0\u25a0\u25a0,)\n",
				"\n",
				"        # Label the silhouette plots with their cluster numbers at the middle\n",
				"        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
				"\n",
				"        # Compute the new y_lower for next plot\n",
				"        # 10 for the 0 samples\n",
				"        y_lower = y_upper + \u25a0\u25a0\u25a0\u25a0\u25a0 \n",
				"\n",
				"    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
				"    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0(\"Cluster label\")\n",
				"\n",
				"    # The vertical line for average silhouette score of all the values\n",
				"    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
				"\n",
				"    # Clear the yaxis labels / ticks\n",
				"    ax1.set_yticks([])  \n",
				"    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
				"\n",
				"    # 2nd Plot showing the actual clusters formed\n",
				"    colors = \u25a0\u25a0\u25a0\u25a0\u25a0(\u25a0\u25a0\u25a0\u25a0\u25a0(float) / n_clusters)\n",
				"    ax2.scatter(\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\" )\n",
				"\n",
				"    # Labeling the clusters\n",
				"    centers = clusterer.cluster_centers_\n",
				"    # Draw white circles at cluster centers\n",
				"    ax2.scatter(\n",
				"        centers[:, 0],\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0[:, 1],\n",
				"        marker=\"o\",\n",
				"        c=\"white\",\n",
				"        alpha=1,\n",
				"        s=200,\n",
				"        edgecolor=\"k\",)\n",
				"\n",
				"    for i, c in enumerate(centers):\n",
				"        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
				"\n",
				"    ax2.set_title(\"The visualization of the clustered data.\")\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0(\"Feature space for the 1st feature\")\n",
				"    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
				"\n",
				"    plt.suptitle(\n",
				"        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
				"        % n_clusters,\n",
				"        fontsize=14,\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0=\"bold\",)\n",
				"\n",
				"#plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc9c0\ub3c4 \ud559\uc2b5 \uae30\ucd08 \u2013 I. \ubd84\ub958"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc0ac\uc6a9 \ud568\uc218 \uba85\n",
				"\n",
				"- \uce35\ud654 \ucd94\ucd9c \uc2e4\ud589 : split_file \n",
				"- \uc218\ub9ac\uc801 \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131 : variable_derive_math , variable_derive_group\n",
				"- \uc815\uaddc\ud654 \ub370\uc774\ud130 \uc0dd\uc131 : variable_derive_normalize\n",
				"- \uc815\ubcf4\uac00\uce58 (Information Value) : reduce_information_value\n",
				"- Feature Importance : reduce_feature_importance\n",
				"- Valid \ud30c\uc77c \uc0dd\uc131 : build_dataset"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc774\uc6a9 \ud30c\uc77c \uba85\n",
				"creditcard.csv\n",
				"\n",
				"# \uc774\uc6a9 \uceec\ub7fc \uba85\n",
				"['Time', 'V1', 'V2', 'Amount', 'Class']"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0. \ud658\uacbd \uc124\uc815"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.1. \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy\n",
				"import os\n",
				"import pandas\n",
				"import matplotlib.pyplot as plt\n",
				"import matplotlib.font_manager as fm\n",
				"import seaborn as sns\n",
				"import warnings\n",
				"import datetime"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%matplotlib \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"plt.rcParams['font.size'] = 20\n",
				"plt.rcParams[\"figure.figsize\"] = (50, 10)\n",
				"plt.rcParams['lines.linewidth'] = 5\n",
				"plt.rcParams[\"axes.grid\"] = True"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%html\n",
				"<style>\n",
				"    table { display: inline-block }\n",
				"    .rendered_html td, .rendered_html th { text-align: \u25a0\u25a0\u25a0\u25a0\u25a0 }\n",
				"</style>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.2. \uc870\uac74 \ubc0f \ud30c\ub77c\ubbf8\ud130 \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ================================================================================================================================\n",
				"# \uac1c\uc778\ubcc4 \ud658\uacbd\uc5d0 \ub9de\uac8c \uc870\uc815\ud558\uc2e4 \uac83\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_args = dict()\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"# \uc785\ub825 \uad00\ub828 \uc124\uc815\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_args['base_file'] = os.path.abspath(\"./Data\") + '/creditcard.csv'\n",
				"\n",
				"dict_args['id_col_name'] = 'Time'\n",
				"dict_args['target_col_name'] = 'Class'\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"# \ucd9c\ub825 \uad00\ub828 \uc124\uc815\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_args['train_file'] = './train_source.csv'\n",
				"dict_args['valid_file'] = './valid_source.csv'\n",
				"dict_args['min_max_file'] = 'refs/min_max_file.csv'\n",
				"# \ubd84\ud560\ub41c \ud558\ub098\uc758 \ud074\ub798\uc2a4\uc5d0\uc11c \ud544\uc694\ud55c \ucd5c\uc18c \uac74\uc218 \n",
				"dict_args['value_min_rows_count_in_class'] = 3\n",
				"# \uc0d8\ud50c\ub9c1\uc73c\ub85c \ucd94\ucd9c\ud560 \ud559\uc2b5 \ube44\uc728 (%)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['train_ratio'] = 80\n",
				"\n",
				"dict_args['derived_1_file_source'] = dict_args['train_file']\n",
				"dict_args['derived_1_file_output'] = 'work/derived_1_file_output.csv'\n",
				"dict_args['derived_2_file_source'] = dict_args['derived_1_file_output']\n",
				"dict_args['derived_2_file_output'] = 'work/derived_2_file_output.csv'\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['derived_3_file_source'] = dict_args['derived_2_file_output']\n",
				"dict_args['derived_output'] = 'work/derived_output.csv'\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['information_value'] = 'work/reduce_information_value.csv'\n",
				"dict_args['feature_importance'] = 'work/reduce_feature_importance.csv'\n",
				"\n",
				"dict_args['cutoff_feature_importance'] = 0.01\n",
				"dict_args['cutoff_information_value'] = 0.1\n",
				"\n",
				"dict_args['source_data_file_train'] = './train_source.csv'\n",
				"dict_args['target_data_file_train'] = './train_output.csv'\n",
				"dict_args['source_data_file_valid'] = './valid_source.csv'\n",
				"dict_args['target_data_file_valid'] = './valid_output.csv'\n",
				"\n",
				"dict_args['model_file'] = './zulu_trained_model_xgb.h5'\n",
				"\n",
				"dict_args['score_file'] = 'scores.csv'\n",
				"\n",
				"if not os.path.exists('work'):\n",
				"    os.mkdir('work')\n",
				"if not os.path.exists('refs'):\n",
				"    os.mkdir('refs')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.3. \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(dict_args['base_file'])\n",
				"#display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.4. \ud568\uc218 \uc120\uc5b8"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 0.4.1 \ud568\uc218 - 01"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \uacf5\ud1b5 \ub85c\uadf8 \ud568\uc218\n",
				"def line_logging(*messages):\n",
				"    import datetime\n",
				"    log_time = datetime.datetime.today().strftime('[%Y/%m/%d %H:%M:%S]')\n",
				"    log = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"    for message in messages:\n",
				"        log.append(str(message))\n",
				"    print(log_time + ':[' + ' '.join(log) + ']', flush=True)\n",
				"line_logging('test')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. \uc0d8\ud50c\ub9c1"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1.1. \uce35\ud654 \ucd94\ucd9c \uc2e4\ud589"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_min_max(p_args):\n",
				"    line_logging('get_min_max is started.')\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"\n",
				"    if os.path.exists(p_args['min_max_file']):\n",
				"        df_min_max = pandas.read_csv(\u25a0\u25a0\u25a0\u25a0\u25a0['min_max_file'])\n",
				"        set_min_max_exists = set(df_min_max['COL']\u25a0\u25a0\u25a0\u25a0\u25a0())\n",
				"    else:\n",
				"        df_min_max = pandas.DataFrame()\n",
				"        set_min_max_exists = set()\n",
				"\n",
				"    df_base = pandas.read_csv(p_args['base_min_max_file'])\n",
				"    df_base = df_base.set_index([l_id_col_name])\n",
				"\n",
				"    list_min_max = list()\n",
				"    df_min_max_to_save = pandas.DataFrame()\n",
				"    for col_name in df_base.columns:\n",
				"        if col_name == l_target_col_name:\n",
				"            continue\n",
				"        if col_name in set_min_max_exists:\n",
				"            list_min_max.append({\n",
				"                'COL': \u25a0\u25a0\u25a0\u25a0\u25a0,\n",
				"                'MAX': float(df_min_max[df_min_max['COL'] == col_name]['MAX'].tolist()[0]),\n",
				"                'MIN': float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0]),\n",
				"            })\n",
				"        else:\n",
				"            value_max = numpy.max(df_base[col_name])\n",
				"            value_min = numpy.min(df_base[col_name])\n",
				"            list_min_max.append({\n",
				"                'COL': col_name,\n",
				"                'MAX': value_max,\n",
				"                'MIN': value_min,\n",
				"            })\n",
				"\n",
				"    df_min_max = pandas.DataFrame(list_min_max)\n",
				"    df_min_max_to_save = pandas.concat([\u25a0\u25a0\u25a0\u25a0\u25a0, df_min_max], sort=False)\n",
				"    df_min_max.to_csv(p_args['min_max_file'])\n",
				"\n",
				"    line_logging('get_min_max is finished.')\n",
				"    return df_min_max\n",
				"\n",
				"# I.V. (Information Value) \uad6c\ud558\ub294 \ud568\uc218\n",
				"def get_information_value(p_args):\n",
				"    line_logging('get_information_value is started.')\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"\n",
				"    df_base = pandas.read_csv(p_args['base_file'])\n",
				"    df_base = df_base.set_index([l_id_col_name])\n",
				"    df_base = df_base.fillna(0)\n",
				"    df_base = df_base.replace([numpy.inf, -numpy.inf], 0)\n",
				"\n",
				"    p_args['base_min_max_file'] = p_args['base_file']\n",
				"    df_min_max = get_min_max(p_args)\n",
				"\n",
				"    df_iv_raw = pandas.DataFrame()\n",
				"    list_iv_val = list()\n",
				"    for col_name in df_base.columns:\n",
				"        if col_name == l_target_col_name:\n",
				"            continue\n",
				"\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = df_base[[col_name, l_target_col_name]]\n",
				"        value_max = float(\u25a0\u25a0\u25a0\u25a0\u25a0[df_min_max['COL'] == col_name]['MAX'].tolist()[0])\n",
				"        value_min = float(df_min_max[df_min_max['COL'] == col_name]['MIN'].tolist()[0])\n",
				"\n",
				"        df_for_iv['IV_GROUP'] = (df_base[col_name] - value_min) / (value_max - value_min) * 19\n",
				"        df_for_iv['IV_GROUP'] = df_for_iv['IV_GROUP'].astype(int)\n",
				"\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = df_for_iv.groupby(['IV_GROUP']).agg(\n",
				"            COUNT_T = (l_target_col_name, \u25a0\u25a0\u25a0\u25a0\u25a0),\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0 = (l_target_col_name, 'count'),\n",
				"        )\n",
				"        df_group['COUNT_F'] = df_group['COUNT_A'] - df_group['COUNT_T']\n",
				"        df_group = df_group[['COUNT_A', 'COUNT_T', 'COUNT_F']]\n",
				"\n",
				"\n",
				"        accm_count_T = 0\n",
				"        accm_count_F = 0\n",
				"        class_order = 1\n",
				"        list_for_iv = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"        last_var_class = df_group.head(1).index.tolist()[0]\n",
				"        for idx, row in df_group.iterrows():\n",
				"            curr_count_T = int(row['COUNT_T'])\n",
				"            curr_count_F = int(row['COUNT_F'])\n",
				"            accm_count_T += curr_count_T\n",
				"            accm_count_F += curr_count_F\n",
				"\n",
				"            if (accm_count_T * accm_count_F != 0) and (\u25a0\u25a0\u25a0\u25a0\u25a0 + accm_count_F > int(p_args['value_min_rows_count_in_class'])):\n",
				"                list_for_iv.append({\n",
				"                    'VAR_NAME': col_name,\n",
				"                    'VAR_CLASS': idx,\n",
				"                    'COUNT_T': accm_count_T,\n",
				"                    'COUNT_F': accm_count_F,\n",
				"                    'TICK_VALUE': value_max - \u25a0\u25a0\u25a0\u25a0\u25a0,\n",
				"                    'NEW_CLASS': class_order,\n",
				"                })\n",
				"                last_var_class = idx\n",
				"                accm_count_T = 0\n",
				"                accm_count_F = 0\n",
				"                class\u25a0\u25a0\u25a0\u25a0\u25a0 += 1\n",
				"\n",
				"        df_class = pandas.DataFrame(list_for_iv)\n",
				"        if last_var_class != df_group.tail(1)\u25a0\u25a0\u25a0\u25a0\u25a0()[0]:\n",
				"            count_T = df_class.tail(1)['COUNT_T'].tolist()[0]\n",
				"            count_F = df_class.tail(1)['COUNT_F'].tolist()[0]\n",
				"            df_class.loc[df_class['VAR_CLASS'] == last_var_class , 'COUNT_T'] = count_T + \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"            df_class.loc[df_class['VAR_CLASS'] == \u25a0\u25a0\u25a0\u25a0\u25a0class , 'COUNT_F'] = count_F + accm_count_F\n",
				"\n",
				"        df_class['TOTAL_T'] = numpy.sum(\u25a0\u25a0\u25a0\u25a0\u25a0class['COUNT_T'])\n",
				"        df_class['TOTAL_F'] = numpy.sum(df_class['COUNT_F'])\n",
				"        df_class['ratio_T'] = df_class['COUNT_T'] / df_class['TOTAL_T']\n",
				"        df_class['ratio_F'] = df_class['COUNT_F'] / df_class['TOTAL_F']\n",
				"        df_class['diff'] = df_class['ratio_T'] - df_class['ratio_F']\n",
				"        df_class['woe'] = numpy.log(df_class['ratio_T'] / df_class['ratio_F'])\n",
				"        df_class['IV'] = df_class['diff'] * df_class['woe']\n",
				"\n",
				"        df_iv_raw = pandas.concat([df_iv_raw, df_class], sort=False)\n",
				"        list_iv_val.append({\n",
				"            'VAR_NAME': col_name,\n",
				"            'IV_VALUE': \u25a0\u25a0\u25a0\u25a0\u25a0(df_class['IV']),\n",
				"        })\n",
				"\n",
				"    df_iv_val = pandas.DataFrame(list_iv_val)\n",
				"    df_iv_val = df_iv_val.set_index(['IV_VALUE'])\n",
				"    df_iv_val = df_iv_val.sort_index(ascending=False)\n",
				"\n",
				"    line_logging('get_information_value is finished.')\n",
				"    return df_iv_val\n",
				"\n",
				"# \uc0d8\ud50c\ub9c1\ud558\uc5ec Train / Valid \ub85c \ud30c\uc77c \ubd84\ub9ac\n",
				"def split_file(p_args):\n",
				"    line_logging('split_file is started.')\n",
				"\n",
				"    # I.V. Section ====================================================================================================================================\n",
				"    df_iv = get_information_value(p_args)\n",
				"\n",
				"    p_args['base_min_max_file'] = p_args['base_file']\n",
				"    df_min_max = \u25a0\u25a0\u25a0\u25a0\u25a0(p_args)\n",
				"\n",
				"    sampling_column_name = df_iv[df_iv.index < 1].head(1)['VAR_NAME'].tolist()[0]\n",
				"    sampling_column_iv = \u25a0\u25a0\u25a0\u25a0\u25a0[df_iv.index < 1].head(\u25a0\u25a0\u25a0\u25a0\u25a0).index.tolist()[0]\n",
				"    sampling_column_max = df_min_max[df_min_max['COL'] == sampling_column_name]['MAX'].tolist()[0]\n",
				"    sampling_column_min = df_min_max[df_min_max['COL'] == sampling_column_name]['MIN'].tolist()[0]\n",
				"    line_logging('Sampling >> Column name:[', sampling_column_name, '], IV:[', sampling_column_iv, ']')\n",
				"    # I.V. Section ====================================================================================================================================\n",
				"\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"    l_value_min_rows_count_in_class = int(p_args['value_min_rows_count_in_class'])\n",
				"    l_train_ratio = p_args['train_ratio']\n",
				"\n",
				"    df_base = pandas.read_csv(\u25a0\u25a0\u25a0\u25a0\u25a0['base_file'])\n",
				"    df_base['UID'] = numpy.arange(df_base.shape[0])\n",
				"    df_base = df_base.set_index(['UID'])\n",
				"    # df_base = df_base.set_index([l_id_col_name])\n",
				"\n",
				"    # 20 \ubd84\ud560 \uadf8\ub8f9 \uc0dd\uc131\n",
				"    df_base['StratafiedGroup'] = (df_base[sampling_column_name] - sampling_column_min) / (sampling_column_max - sampling_column_min) * 19\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0['StratafiedGroup'] = df_base['StratafiedGroup'].astype(int)\n",
				"\n",
				"    list_StratafiedGroup = df_base['StratafiedGroup'].unique()\n",
				"    list_StratafiedGroup = sorted(list_StratafiedGroup)\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.DataFrame()\n",
				"    accm_count_T = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    accm_count_F = 0\n",
				"    last_group_number = 0\n",
				"    dict_df_group = dict()\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 stratafiedGroup \u25a0\u25a0\u25a0\u25a0\u25a0 list_StratafiedGroup:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = df_base[df_base['StratafiedGroup'] == stratafiedGroup]\n",
				"        df_accum = pandas.concat([df_accum, df_part], sort=False)\n",
				"\n",
				"        accm_count_T += numpy.sum(df_part[l_target_col_name])\n",
				"        accm_count_F += df_part.shape[0] - numpy.sum(df_part[l_target_col_name])\n",
				"\n",
				"        if (accm_count_T * \u25a0\u25a0\u25a0\u25a0\u25a0 != 0) and (accm_count_T + accm_count_F > \u25a0\u25a0\u25a0\u25a0\u25a0class):\n",
				"            last_group_number = stratafiedGroup\n",
				"            dict_df_group[last_group_number] = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"            df_accum = pandas.DataFrame()\n",
				"            accm_count_T = 0\n",
				"            accm_count_F = 0\n",
				"    \n",
				"    if last_group_number != numpy.max(df_accum['StratafiedGroup']):\n",
				"        df_last = dict_df_group[last_group_number]\n",
				"        df_last = pandas.concat([\u25a0\u25a0\u25a0\u25a0\u25a0, df_accum], sort=False)\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0[last_group_number] = df_last\n",
				"\n",
				"    df_sample_train = pandas.DataFrame()\n",
				"    for class_number in dict_df_group.keys():\n",
				"        df_part = dict_df_group[class_number]\n",
				"        df_part_T = df_part[df_part[l_target_col_name] == 1]\n",
				"        df_part_F = df_part[df_part[l_target_col_name] != 1]\n",
				"        l_train_count_T = int(df_part_T.shape[0] * l_train_ratio / 100)\n",
				"        l_train_count_F = int(df_part_F.shape[0] * l_train_ratio / 100)\n",
				"\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.concat([df_sample_train, df_part_T.sample(n=l_train_count_T)])\n",
				"        df_sample_train = pandas.concat([df_sample_train, \u25a0\u25a0\u25a0\u25a0\u25a0(n=l_train_count_F)])\n",
				"\n",
				"    set_train_ids = set(df_sample_train.index.tolist())\n",
				"    df_sample_valid = df_base[~df_base.index.isin(set_train_ids)]\n",
				"\n",
				"    df_sample_train = df_sample_train.reset_index()\n",
				"    del df_sample_train['UID']\n",
				"    del df_sample_train['StratafiedGroup']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0(\u25a0\u25a0\u25a0\u25a0\u25a0['train_file'], index=None)\n",
				"\n",
				"    df_sample_valid = df_sample_valid.reset_index()\n",
				"    del df_sample_valid['UID']\n",
				"    del df_sample_valid['StratafiedGroup']\n",
				"    df_sample_valid.to_csv(p_args['valid_file'], index=None)\n",
				"\n",
				"    line_logging('[Sampling-Base]  Shape:', df_base.shape, ', Target count:', numpy.sum(df_base[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_base[l_target_col_name]) / df_base.shape[0] * 10000)/ 100, '%')\n",
				"    line_logging('[Sampling-Train] Shape:', df_sample_train.shape, ', Target count:', numpy.sum(df_sample_train[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_sample_train[l_target_col_name]) / df_sample_train.shape[0] * 10000)/ 100, '%')\n",
				"    line_logging('[Sampling-Valid] Shape:', \u25a0\u25a0\u25a0\u25a0\u25a0shape, ', Target count:', numpy.sum(df_sample_valid[l_target_col_name]), ', Target ratio:', int(numpy.sum(df_sample_valid[l_target_col_name]) / df_sample_valid.shape[0] * 10000)/ 100, '%')\n",
				"\n",
				"    line_logging('split_file is finished.')\n",
				"\n",
				"warnings.filterwarnings('ignore')\n",
				"split_file(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1.2. \uce35\ud654 \ucd94\ucd9c \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_train = pandas.read_csv(dict_args['train_file'])\n",
				"df_valid = pandas.read_csv(dict_args['valid_file'])\n",
				"df_min_max = pandas.read_csv(dict_args['min_max_file'])\n",
				"#display(df_train)\n",
				"#display(df_valid)\n",
				"#display(df_min_max)\n",
				"print('Train:', \u25a0\u25a0\u25a0\u25a0\u25a0(df_train[dict_args['target_col_name']]), ', Valid:', numpy.sum(df_valid[dict_args['target_col_name']]))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"# 2. \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1. \uc218\ub9ac\uc801 \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ud30c\uc0dd \ubcc0\uc218 \uc0dd\uc131 - 1. \uc218\ub9ac\uc801 \ud30c\uc0dd\n",
				"def variable_derive_math(p_args):\n",
				"    line_logging('variable_derive_math is started.')\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = p_args['target_col_name']\n",
				"\n",
				"    df_base = pandas.read_csv(p_args['derived_1_file_source'])\n",
				"    df_base = df_base.set_index([l_id_col_name])\n",
				"\n",
				"    p_args['base_min_max_file'] = \u25a0\u25a0\u25a0\u25a0\u25a0['derived_1_file_source']\n",
				"    df_min_max = get_min_max(p_args)\n",
				"\n",
				"    list_positive_columns = list()\n",
				"    list_negative_columns = list()\n",
				"\n",
				"    for col_name in df_base.columns:\n",
				"        if col_name == l_target_col_name:\n",
				"            continue\n",
				"        if numpy.min(df_base[col_name]) <= 0:\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0(col_name)\n",
				"        else:\n",
				"            list_positive_columns.append(col_name)\n",
				"    \n",
				"    for col_name in list_positive_columns:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0['SQUARE-' + col_name] = df_base[col_name] * df_base[col_name]\n",
				"        df_base['ROOT-' + col_name] = numpy.sqrt(df_base[col_name])\n",
				"        df_base['LOG-' + col_name] = numpy.log(df_base[col_name])\n",
				"    \n",
				"    for col_name in list_negative_columns:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 = float(\u25a0\u25a0\u25a0\u25a0\u25a0[df_min_max['COL'] == col_name]['MIN'].tolist()[0])\n",
				"        df_base['MOVE-' + col_name] = df_base[col_name] - min_value + 1\n",
				"        df_base['MOVE-' + col_name] = df_base['MOVE-' + col_name]\u25a0\u25a0\u25a0\u25a0\u25a0(float)\n",
				"        df_base['SQUARE-MOVE-' + col_name] = df_base['MOVE-' + col_name] * df_base['MOVE-' + col_name]\n",
				"        df_base['SQUARE-' + col_name] = \u25a0\u25a0\u25a0\u25a0\u25a0[\u25a0\u25a0\u25a0\u25a0\u25a0] * df_base[col_name]\n",
				"        df_base['ROOT-' + col_name] = numpy.sqrt(df_base['MOVE-' + col_name])\n",
				"        df_base['LOG-' + col_name] = numpy.log(df_base['MOVE-' + col_name])\n",
				"\n",
				"    df_base.to_csv(p_args['derived_1_file_output'])\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0('variable_derive_math is finished.')\n",
				"\n",
				"# \ud30c\uc0dd \ubcc0\uc218 \uc0dd\uc131 - 2. \uadf8\ub8f9 \ubcc0\uc218 \uc801\uc6a9\n",
				"def variable_derive_group(p_args):\n",
				"    line_logging('variable_derive_group is started.')\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"\n",
				"    df_base = pandas.read_csv(p_args['derived_2_file_source'])\n",
				"    df_base = df_base.set_index([l_id_col_name])\n",
				"\n",
				"    p_args['base_min_max_file'] = p_args['derived_2_file_source']\n",
				"    df_min_max = get_min_max(p_args)\n",
				"\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 col_name in \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"        if col_name == l_target_col_name:\n",
				"            continue\n",
				"        df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"        max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"        min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"        if max_value == min_value:\n",
				"            line_logging('Column[' + col_name + '] is skipped. (MIN == MAX)')\n",
				"            del df_base[col_name]\n",
				"            continue\n",
				"        df_base['GROUP-' + col_name] = (df_base[col_name] - min_value) / (max_value - min_value) * 19\n",
				"        df_base['GROUP-' + col_name] = df_base['GROUP-' + col_name].astype(float)\n",
				"        try:\n",
				"            df_base['GROUP-' + col_name] = df_base['GROUP-' + col_name]\u25a0\u25a0\u25a0\u25a0\u25a0(int)\n",
				"        except:\n",
				"            line_logging('Column[' + col_name + '] is skipped. (ERROR)', df_base['GROUP-' + col_name].unique())\n",
				"            del \u25a0\u25a0\u25a0\u25a0\u25a0['GROUP-' + col_name]\n",
				"\n",
				"    df_base.to_csv(p_args['derived_2_file_output'])\n",
				"    line_logging('variable_derive_group is finished.')\n",
				"\n",
				"variable_derive_math(dict_args)\n",
				"variable_derive_group(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1.1. \uc218\ub9ac\uc801 \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131 \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_derived_1 = \u25a0\u25a0\u25a0\u25a0\u25a0(dict_args['derived_1_file_output'])\n",
				"df_derived_2 = pandas.read_csv(dict_args['derived_2_file_output'])\n",
				"#display(df_derived_1)\n",
				"#display(df_derived_2)\n",
				"\n",
				"if dict_args['target_col_name'] in df_derived_1.columns:\n",
				"    print('df_derived_1', dict_args['target_col_name'])\n",
				"if dict_args['target_col_name'] in df_derived_2.columns:\n",
				"    print('df_derived_2', dict_args['target_col_name'])"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2. \uc815\uaddc\ud654 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ud30c\uc0dd \ubcc0\uc218 \uc0dd\uc131 - 3. \uc815\uaddc\ud654 \ubcc0\uc218 \uc801\uc6a9\n",
				"def \u25a0\u25a0\u25a0\u25a0\u25a0(p_args):\n",
				"    line_logging('variable_derive_normalize is started.')\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"\n",
				"    df_base = pandas.read_csv(p_args['derived_3_file_source'])\n",
				"    df_base = df_base.set_index([\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"\n",
				"    p_args['base_min_max_file'] = p_args['derived_3_file_source']\n",
				"    df_min_max = get_min_max(p_args)\n",
				"\n",
				"    for col_name \u25a0\u25a0\u25a0\u25a0\u25a0 df_base.columns:\n",
				"        if col_name == l_target_col_name:\n",
				"            continue\n",
				"        if 'GROUP-' == col_name[0:6]:\n",
				"            continue\n",
				"        df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"        max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"        min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"        if max_value == min_value:\n",
				"            continue\n",
				"\n",
				"        df_base[col_name] = (df_base[\u25a0\u25a0\u25a0\u25a0\u25a0] - min_value) / (max_value - min_value)\n",
				"\n",
				"    df_base.to_csv(p_args['derived_output'])\n",
				"    line_logging('variable_derive_normalize is finished.')\n",
				"\n",
				"variable_derive_normalize(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2.1. \uc815\uaddc\ud654 \ub370\uc774\ud130 \uc0dd\uc131 \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"derived_3_file_source = pandas.read_csv(dict_args['derived_3_file_source'])\n",
				"#display(derived_3_file_source)\n",
				"df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n",
				"#display(df_derived_3)\n",
				"# print(list(df_derived_3.columns))"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3. \uc815\ubcf4\uac00\uce58 (Information Value)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ubcc0\uc218 \ucd95\uc18c - 1. Information Value\n",
				"def reduce_information_value(p_args):\n",
				"    line_logging('reduce_information_value is started.')\n",
				"\n",
				"    l_reduce_IV_file_name = p_args['information_value']\n",
				"    p_args['base_file'] = p_args['derived_output']\n",
				"\n",
				"    df_iv = get_information_value(p_args)\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0(l_reduce_IV_file_name)\n",
				"\n",
				"    line_logging('reduce_information_value is finished.')\n",
				"    \n",
				"warnings.filterwarnings('ignore')\n",
				"reduce_information_value(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.1. \uc815\ubcf4\uac00\uce58 (Information Value) \uc0dd\uc131 \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_iv = pandas.read_csv(dict_args['information_value'])\n",
				"#display(df_iv)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.2. \uc815\ubcf4\uac00\uce58 (Information Value) \uc0dd\uc131 \uacb0\uacfc \ubc18\uc601 (1\ucc28 \ucd95\uc18c)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_iv = pandas.read_csv(dict_args['information_value'])\n",
				"df_iv = df_iv[df_iv['IV_VALUE'] > dict_args['cutoff_information_value']]\n",
				"list_reduced_columns = [dict_args['id_col_name'], dict_args['target_col_name']]\n",
				"list_reduced_columns.extend(df_iv['VAR_NAME'].tolist())\n",
				"df_derived_3 = pandas.read_csv(dict_args['derived_output'])\n",
				"df_derived_3 = \u25a0\u25a0\u25a0\u25a0\u25a0[\u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"df_derived_3.to_csv('work/reduced.csv', index=None)\n",
				"#display(df_derived_3)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4. Feature Importance"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ubcc0\uc218 \ucd95\uc18c - 2. Feature Importance\n",
				"def reduce_feature_importance(p_args):\n",
				"    line_logging('reduce_feature_importance is started.')\n",
				"\n",
				"    import sklearn.ensemble\n",
				"\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    l_target_col_name = p_args['target_col_name']\n",
				"    l_reduce_FI_file_name = p_args['feature_importance']\n",
				"\n",
				"    df_source_X = pandas.read_csv(p_args['derived_output'])\n",
				"    df_source_X = df_source_X.set_index([l_id_col_name])\n",
				"    df_source_X.dropna(inplace=True)\n",
				"    df_source_Y = df_source_X[l_target_col_name]\n",
				"    del df_source_X[l_target_col_name]\n",
				"\n",
				"    clf = sklearn.ensemble.RandomForestClassifier(\n",
				"                                                    random_state = 0,\n",
				"                                                    criterion = 'entropy',\n",
				"                                                    max_depth = 2,\n",
				"                                                    min_samples_split = \u25a0\u25a0\u25a0\u25a0\u25a0,\n",
				"                                                    min_samples_leaf = 2\n",
				"                                                )\n",
				"    clf.fit(df_source_X, df_source_Y)\n",
				"\n",
				"    l_importance = clf.feature_importances_\n",
				"    indices = numpy.argsort(l_importance)\n",
				"\n",
				"    columns = df_source_X.columns.tolist()\n",
				"    column_count = \u25a0\u25a0\u25a0\u25a0\u25a0(columns)\n",
				"\n",
				"    each_vars = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"    for idx in range(column_count):\n",
				"        each_vars.append({\n",
				"            'var_name': columns[indices[idx]],\n",
				"            'importance': l_importance[\u25a0\u25a0\u25a0\u25a0\u25a0[idx]]\n",
				"        })\n",
				"\n",
				"    df = pandas.DataFrame(each_vars)\n",
				"    df = df.set_index(['importance'])\n",
				"    df = df.sort_index(\u25a0\u25a0\u25a0\u25a0\u25a0=False)\n",
				"    df.to_csv(l_reduce_FI_file_name)\n",
				"\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0('reduce_feature_importance is finished.')\n",
				"    \n",
				"dict_args['derived_output'] = 'work/reduced.csv'\n",
				"reduce_feature_importance(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.1 Feature Importance \uc0dd\uc131 \uacb0\uacfc \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_fi = pandas.read_csv(dict_args['feature_importance'])\n",
				"#display(df_fi)\n",
				"\n",
				"#plt.rcParams[\"figure.figsize\"] = (50, 50)\n",
				"list_labels = df_fi['var_name']\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"list_data = df_fi['importance'].tolist()\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"#ax = sns.barplot(x=list_data, y=list_labels)\n",
				"\n",
				"#plt.show()\n",
				"#plt.rcParams[\"figure.figsize\"] = (50, 10)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.2 Feature Importance \uc0dd\uc131 \uacb0\uacfc \ubc18\uc601 (2\ucc28 \ucd95\uc18c)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_fi = pandas.read_csv(\u25a0\u25a0\u25a0\u25a0\u25a0['feature_importance'])\n",
				"df_fi = df_fi.head(100)\n",
				"list_reduced_columns = [\u25a0\u25a0\u25a0\u25a0\u25a0['id_col_name'], dict_args['target_col_name']]\n",
				"list_reduced_columns.extend(df_fi['var_name'].tolist())\n",
				"df_train = pandas.read_csv('work/reduced.csv')\n",
				"df_train = df_train[list_reduced_columns]\n",
				"df_train.to_csv(dict_args['target_data_file_train'], index=None)\n",
				"#display(df_train)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5. Valid \ud30c\uc77c \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \ub370\uc774\ud130\uc14b \uc0dd\uc131\n",
				"def \u25a0\u25a0\u25a0\u25a0\u25a0(p_args):\n",
				"    line_logging('build_dataset is started.')\n",
				"\n",
				"    df_var = pandas.read_csv(p_args['selected_variables'], nrows=0)\n",
				"    list_selected_var = list(df_var.columns)\n",
				"\n",
				"    p_args['base_min_max_file'] = p_args['derived_2_file_source']\n",
				"    df_min_max = get_min_max(p_args)\n",
				"\n",
				"    l_id_col_name = p_args['id_col_name']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = \u25a0\u25a0\u25a0\u25a0\u25a0['target_col_name']\n",
				"    l_source_file_name = p_args['source_data_file']\n",
				"    l_target_file_name = p_args['target_data_file']\n",
				"\n",
				"    df_source = pandas.read_csv(l_source_file_name)\n",
				"    df_source = \u25a0\u25a0\u25a0\u25a0\u25a0([l_id_col_name])\n",
				"    df_target = df_source[[l_target_col_name]]\n",
				"    del df_source[l_target_col_name]\n",
				"\n",
				"    for col_name in df_source.columns:\n",
				"        if col_name == \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"            \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"\n",
				"        df_part = df_source[[col_name]]\n",
				"\n",
				"        for check_col_name in list_selected_var:\n",
				"            if check_col_name == l_target_file_name:\n",
				"                continue\n",
				"            # print(col_name, check_col_name, df_target.shape, df_part.shape)\n",
				"            if col_name == check_col_name:\n",
				"                print(l_target_col_name, col_name, check_col_name, \u25a0\u25a0\u25a0\u25a0\u25a0shape, df_part.shape)\n",
				"                df_target[col_name] = df_part[col_name]\n",
				"\n",
				"            elif 'GROUP-SQUARE-MOVE-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == \u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"                min_value = float(\u25a0\u25a0\u25a0\u25a0\u25a0['MIN'].tolist()[0])\n",
				"\n",
				"                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                df_part['SQUARE-MOVE-' + col_name] = df_part['MOVE-' + col_name] * df_part['MOVE-' + col_name]\n",
				"\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-MOVE-' + \u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"                max_value = float(df_one_row['MAX']\u25a0\u25a0\u25a0\u25a0\u25a0()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_target['GROUP-SQUARE-MOVE-' + \u25a0\u25a0\u25a0\u25a0\u25a0] = (df_part['SQUARE-MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n",
				"                df_target['GROUP-SQUARE-MOVE-' + col_name] = df_target['GROUP-SQUARE-MOVE-' + col_name].astype(float)\n",
				"\n",
				"            elif 'GROUP-SQUARE-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == 'SQUARE-' + col_name]\n",
				"                max_value = float(\u25a0\u25a0\u25a0\u25a0\u25a0['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_part['SQUARE-' + col_name] = df_part[col_name] * df_part[col_name]\n",
				"                df_target['GROUP-SQUARE-' + col_name] = (df_part['SQUARE-' + col_name] - min_value) / (max_value - min_value) * 19\n",
				"                df_target['GROUP-SQUARE-' + col_name] = df_target['GROUP-SQUARE-' + col_name].astype(float)\n",
				"\n",
				"            elif 'GROUP-ROOT-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                if min_value <= 0:\n",
				"                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                    df_part['ROOT-' + col_name] = numpy.sqrt(df_part['MOVE-' + col_name])\n",
				"                \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"                    df_part['ROOT-' + col_name] = numpy.sqrt(df_part[\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == 'ROOT-' + col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                \u25a0\u25a0\u25a0\u25a0\u25a0 = float(df_one_row['MIN'].tolist()[\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"\n",
				"                df_target['GROUP-ROOT-' + col_name] = (df_part['ROOT-' + col_name] - min_value) / (max_value - min_value) * 19\n",
				"                df_target['GROUP-ROOT-' + col_name] = df_target['GROUP-ROOT-' + col_name]\u25a0\u25a0\u25a0\u25a0\u25a0(float)\n",
				"\n",
				"            elif 'GROUP-LOG-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                if min_value <= 0:\n",
				"                    \u25a0\u25a0\u25a0\u25a0\u25a0['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                    df_part['LOG-' + col_name] = numpy.log(\u25a0\u25a0\u25a0\u25a0\u25a0['MOVE-' + col_name])\n",
				"                else:\n",
				"                    df_part['LOG-' + col_name] = numpy.log(df_part[col_name])\n",
				"\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == 'LOG-' + col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_target['GROUP-LOG-' + col_name] = (df_part['LOG-' + col_name] - \u25a0\u25a0\u25a0\u25a0\u25a0) / (\u25a0\u25a0\u25a0\u25a0\u25a0 - min_value) * 19\n",
				"                df_target['GROUP-LOG-' + col_name] = \u25a0\u25a0\u25a0\u25a0\u25a0['GROUP-LOG-' + col_name].astype(float)\n",
				"\n",
				"            elif 'GROUP-MOVE-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_part['MOVE-' + col_name] = df_part[col_name] - \u25a0\u25a0\u25a0\u25a0\u25a0 + 1\n",
				"\n",
				"                \u25a0\u25a0\u25a0\u25a0\u25a0 = df_min_max[df_min_max['COL'] == 'MOVE-' + col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_target['GROUP-MOVE-' + col_name] = (df_part['MOVE-' + col_name] - min_value) / (max_value - min_value) * 19\n",
				"                \u25a0\u25a0\u25a0\u25a0\u25a0['GROUP-MOVE-' + col_name] = df_target['GROUP-MOVE-' + col_name].astype(float)\n",
				"\n",
				"            elif 'GROUP-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = \u25a0\u25a0\u25a0\u25a0\u25a0(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_target['GROUP-' + col_name] = (df_part[col_name] - min_value) / (max_value - min_value) * 19\n",
				"                df_target['GROUP-' + col_name] = df_target['GROUP-' + col_name].astype(float)\n",
				"\n",
				"            elif 'SQUARE-MOVE-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                \u25a0\u25a0\u25a0\u25a0\u25a0 = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(\u25a0\u25a0\u25a0\u25a0\u25a0['MIN'].tolist()[0])\n",
				"\n",
				"                df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                df_target['SQUARE-MOVE-' + col_name] = \u25a0\u25a0\u25a0\u25a0\u25a0['MOVE-' + \u25a0\u25a0\u25a0\u25a0\u25a0] * df_part['MOVE-' + col_name]\n",
				"                df_target['SQUARE-MOVE-' + col_name] = df_target['SQUARE-MOVE-' + col_name]\u25a0\u25a0\u25a0\u25a0\u25a0(float)\n",
				"\n",
				"            elif 'SQUARE-' + col_name == check_col_name:\n",
				"                df_target['SQUARE-' + col_name] = df_part[col_name] * df_part[col_name]\n",
				"\n",
				"            elif 'ROOT-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                if min_value <= 0:\n",
				"                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                    \u25a0\u25a0\u25a0\u25a0\u25a0['ROOT-' + \u25a0\u25a0\u25a0\u25a0\u25a0] = numpy.sqrt(df_part['MOVE-' + col_name])\n",
				"                else:\n",
				"                    df_target['ROOT-' + col_name] = numpy.sqrt(df_part[col_name])\n",
				"\n",
				"            elif 'LOG-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                if min_value <= 0:\n",
				"                    df_part['MOVE-' + col_name] = df_part[col_name] - min_value + 1\n",
				"                    df_target['LOG-' + col_name] = numpy.log(df_part['MOVE-' + col_name])\n",
				"                else:\n",
				"                    df_target['LOG-' + col_name] = numpy.log(df_part[col_name])\n",
				"\n",
				"            elif 'MOVE-' + col_name == check_col_name:\n",
				"                df_one_row = df_min_max[df_min_max['COL'] == col_name]\n",
				"                max_value = float(df_one_row['MAX'].tolist()[0])\n",
				"                min_value = float(df_one_row['MIN'].tolist()[0])\n",
				"\n",
				"                df_part['MOVE-' + col_name] = df_part[col_name] - \u25a0\u25a0\u25a0\u25a0\u25a0 + 1\n",
				"                df_target['MOVE-' + col_name] = df_part['MOVE-' + col_name].astype(float)\n",
				"\n",
				"    df_target = df_target.reset_index()\n",
				"    df_target = df_target[list_selected_var]\n",
				"    df_target.to_csv(\u25a0\u25a0\u25a0\u25a0\u25a0, index=None)\n",
				"\n",
				"    line_logging('build_dataset is finished.')\n",
				"\n",
				"warnings.filterwarnings('ignore')\n",
				"dict_args['selected_variables'] = dict_args['target_data_file_train']\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['source_data_file'] = dict_args['source_data_file_valid']\n",
				"dict_args['target_data_file'] = dict_args['target_data_file_valid']\n",
				"\n",
				"build_dataset(dict_args)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5.1. Valid \ud30c\uc77c \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_valid = pandas.read_csv(dict_args['target_data_file'])\n",
				"#display(df_valid)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3. \ubaa8\ub378\ub9c1"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.0. \ud559\uc2b5/\uac80\uc99d \ub370\uc774\ud130 \ub85c\ub529"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_train_X = pandas.read_csv(dict_args['target_data_file_train'])\n",
				"df_train_X = df_train_X.set_index([dict_args['id_col_name']])\n",
				"#display(df_train_X)\n",
				"df_train_Y = df_train_X[dict_args['target_col_name']]\n",
				"del df_train_X[dict_args['target_col_name']]\n",
				"\n",
				"df_valid_X = pandas.read_csv(\u25a0\u25a0\u25a0\u25a0\u25a0['target_data_file_valid'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_valid_X.set_index([dict_args['id_col_name']])\n",
				"#display(df_valid_X)\n",
				"df_valid_Y = df_valid_X[dict_args['target_col_name']]\n",
				"del df_valid_X[dict_args['target_col_name']]"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.1. scikit-learn"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.1.1. Logistic Regression"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sklearn.linear_model\n",
				"import sklearn.metrics\n",
				"\n",
				"df_score_LR = pandas.DataFrame(df_valid_Y)\n",
				"\n",
				"model = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"model.fit(df_train_X, df_train_Y)\n",
				"\n",
				"df_score_LR['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n",
				"score_auc = sklearn.metrics.roc_auc_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict_proba'])\n",
				"\n",
				"df_score_LR['predict'] = numpy.where(df_score_LR['predict_proba'] > 0.5, 1, 0)\n",
				"score_acc = sklearn.metrics.accuracy_score(df_score_LR[dict_args['target_col_name']], \u25a0\u25a0\u25a0\u25a0\u25a0['predict'])\n",
				"score_prc = sklearn.metrics.precision_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n",
				"score_rcl = sklearn.metrics.recall_score(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n",
				"score_f1s = sklearn.metrics.f1_score(df_score_LR[\u25a0\u25a0\u25a0\u25a0\u25a0['target_col_name']], df_score_LR['predict'])\n",
				"\n",
				"print(score_auc, score_acc, score_prc, score_rcl, score_f1s)\n",
				"sklearn.metrics.confusion_matrix(df_score_LR[dict_args['target_col_name']], df_score_LR['predict'])\n",
				"\n",
				"df_check_LR = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"df_check_LR['X_axis'] = df_check_LR['predict_proba'] * 100\n",
				"df_check_LR['X_axis'] = df_check_LR['X_axis'].astype(int)\n",
				"df_check_LR['X_axis'] = df_check_LR['X_axis'] / 10\n",
				"\n",
				"df_groupby_LR = df_check_LR.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n",
				"#display(df_groupby_LR.T)\n",
				"df_groupby_LR['Y_axis'] = numpy.log(df_groupby_LR['Y_axis'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"df_groupby_LR.plot.bar(x='X_axis',y='Y_axis',rot=270)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.1.2 Decision Tree"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sklearn.model_selection\n",
				"import sklearn.tree\n",
				"import sklearn.metrics\n",
				"\n",
				"df_score_DT = pandas.DataFrame(df_valid_Y)\n",
				"\n",
				"# GridSearchCV\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = {\n",
				"    'max_depth': [2],\n",
				"    'min_samples_split': [2],\n",
				"    'min_samples_leaf': [1],\n",
				"}\n",
				"clf = sklearn.tree.DecisionTreeClassifier(random_state=111, splitter='best', criterion='gini')\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=params, cv=2, \u25a0\u25a0\u25a0\u25a0\u25a0='roc_auc', n_jobs=8, verbose=0)\n",
				"grid_search.fit(df_train_X, df_train_Y)\n",
				"\n",
				"model = grid_search.best_estimator_\n",
				"model.fit(df_train_X, df_train_Y)\n",
				"\n",
				"df_score_DT['predict_proba'] = model.predict_proba(\u25a0\u25a0\u25a0\u25a0\u25a0)[:,1].T\n",
				"score_auc = \u25a0\u25a0\u25a0\u25a0\u25a0(df_score_DT[dict_args['target_col_name']], df_score_DT['predict_proba'])\n",
				"\n",
				"df_score_DT['predict'] = numpy.where(df_score_DT['predict_proba'] > 0.5, 1, 0)\n",
				"score_acc = sklearn.metrics.accuracy_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n",
				"score_prc = \u25a0\u25a0\u25a0\u25a0\u25a0(\u25a0\u25a0\u25a0\u25a0\u25a0[dict_args['target_col_name']], df_score_DT['predict'])\n",
				"score_rcl = sklearn.metrics.recall_score(df_score_DT[\u25a0\u25a0\u25a0\u25a0\u25a0['target_col_name']], df_score_DT['predict'])\n",
				"score_f1s = sklearn.metrics.f1_score(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n",
				"\n",
				"print(score_auc, score_acc, score_prc, score_rcl, score_f1s)\n",
				"sklearn.metrics.confusion_matrix(df_score_DT[dict_args['target_col_name']], df_score_DT['predict'])\n",
				"\n",
				"df_check_DT = df_score_DT.copy()\n",
				"df_check_DT['X_axis'] = df_check_DT['predict_proba'] * 100\n",
				"df_check_DT['X_axis'] = df_check_DT['X_axis'].astype(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df_check_DT['X_axis'] = df_check_DT['X_axis'] / 10\n",
				"\n",
				"df_groupby_DT = df_check_DT.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n",
				"#display(df_groupby_DT.T)\n",
				"df_groupby_DT['Y_axis'] = numpy.log(df_groupby_DT['Y_axis'])\n",
				"df_groupby_DT = df_groupby_DT.reset_index()\n",
				"df_groupby_DT.plot.bar(x='X_axis',y='Y_axis',rot=270)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.1.3. RandomForest"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sklearn.model_selection\n",
				"import sklearn.ensemble\n",
				"import sklearn.metrics\n",
				"\n",
				"df_score_RF = pandas.DataFrame(df_valid_Y)\n",
				"\n",
				"# GridSearchCV\n",
				"params = {\n",
				"    'max_depth': [2],\n",
				"    'min_samples_split':[3],\n",
				"    'min_samples_leaf': [2],\n",
				"}\n",
				"clf = sklearn.ensemble.RandomForestClassifier(random_state=111, criterion='gini', n_estimators=2)\n",
				"grid_search = \u25a0\u25a0\u25a0\u25a0\u25a0(estimator=clf, param_grid=params, \u25a0\u25a0\u25a0\u25a0\u25a0=2, scoring='roc_auc', n_jobs=2, verbose=0)\n",
				"grid_search.fit(df_train_X, df_train_Y)\n",
				"\n",
				"model = grid_search.best_estimator_\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(df_train_X, df_train_Y)\n",
				"\n",
				"df_score_RF['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n",
				"score_auc = sklearn.metrics.roc_auc_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict_proba'])\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['predict'] = numpy.where(df_score_RF['predict_proba'] > 0.5, 1, 0)\n",
				"score_acc = sklearn.metrics.accuracy_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n",
				"score_prc = sklearn.metrics.precision_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n",
				"score_rcl = sklearn.metrics.recall_score(df_score_RF[\u25a0\u25a0\u25a0\u25a0\u25a0['target_col_name']], df_score_RF['predict'])\n",
				"score_f1s = sklearn.metrics.f1_score(df_score_RF[dict_args['target_col_name']], df_score_RF['predict'])\n",
				"\n",
				"print(score_auc, score_acc, score_prc, score_rcl, score_f1s)\n",
				"sklearn.metrics.confusion_matrix(\u25a0\u25a0\u25a0\u25a0\u25a0[dict_args['target_col_name']], df_score_RF['predict'])\n",
				"\n",
				"df_check_RF = df_score_RF.copy()\n",
				"df_check_RF['X_axis'] = df_check_RF['predict_proba'] * 100\n",
				"df_check_RF['X_axis'] = \u25a0\u25a0\u25a0\u25a0\u25a0['X_axis'].astype(int)\n",
				"df_check_RF['X_axis'] = df_check_RF['X_axis'] / 10\n",
				"\n",
				"df_groupby_RF = df_check_RF.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n",
				"#display(df_groupby_RF.T)\n",
				"df_groupby_RF['Y_axis'] = numpy.log(df_groupby_RF['Y_axis'])\n",
				"df_groupby_RF = df_groupby_RF.reset_index()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(x='X_axis',y='Y_axis',rot=270)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.2. XGBoost"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn.model_selection\n",
				"import xgboost\n",
				"import sklearn.metrics\n",
				"\n",
				"df_score_XG = pandas.DataFrame(df_valid_Y)\n",
				"\n",
				"# GridSearchCV\n",
				"params = {\n",
				"    'max_depth': [2],\n",
				"    'reg_lambda': [0],\n",
				"    'reg_alpha': [0],\n",
				"}\n",
				"clf = xgboost.XGBClassifier(random_state=111, booster='gbtree', objective='binary:logistic', eval_metric='logloss', tree_method='auto')\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=clf, param_grid=\u25a0\u25a0\u25a0\u25a0\u25a0, \u25a0\u25a0\u25a0\u25a0\u25a0=2, scoring='roc_auc', n_jobs=1, verbose=0)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(df_train_X, df_train_Y)\n",
				"\n",
				"model = grid_search.best_estimator_\n",
				"model.fit(\u25a0\u25a0\u25a0\u25a0\u25a0, df_train_Y)\n",
				"\n",
				"df_score_XG['predict_proba'] = model.predict_proba(df_valid_X)[:,1].T\n",
				"score_auc = sklearn.metrics.roc_auc_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n",
				"\n",
				"df_score_XG['predict'] = numpy.where(\u25a0\u25a0\u25a0\u25a0\u25a0['predict_proba'] > 0.5, 1, 0)\n",
				"score_acc = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_prc = sklearn.metrics.precision_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_rcl = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_f1s = sklearn.metrics.f1_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"\n",
				"print(score_auc, score_acc, score_prc, score_rcl, \u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"\n",
				"df_check_XG = df_score_XG.copy()\n",
				"df_check_XG['X_axis'] = df_check_XG['predict_proba'] * \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"df_check_XG['X_axis'] = df_check_XG['X_axis'].astype(int)\n",
				"df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n",
				"\n",
				"df_groupby_XG = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n",
				"#display(df_groupby_XG.T)\n",
				"df_groupby_XG['Y_axis'] = numpy.log(df_groupby_XG['Y_axis'])\n",
				"df_groupby_XG = df_groupby_XG.reset_index()\n",
				"df_groupby_XG.plot.bar(x='X_axis',y='Y_axis',rot=270)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.3. Tensorflow DNN"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import tensorflow as tf\n",
				"import keras\n",
				"\n",
				"from tensorflow.keras.models import *\n",
				"from keras import *\n",
				"from keras.models import *\n",
				"from keras.layers import *\n",
				"\n",
				"max_shuffle_count = 2\n",
				"max_batch_size = 128\n",
				"\n",
				"df_Xxxx = df_train_X.copy()\n",
				"df_Yyyy = df_train_Y.copy()\n",
				"\n",
				"fc_dataset = \u25a0\u25a0\u25a0\u25a0\u25a0((\u25a0\u25a0\u25a0\u25a0\u25a0, df_Yyyy.values))\n",
				"train_fc_dataset = fc_dataset.shuffle(max_shuffle_count).batch(max_batch_size)\n",
				"\n",
				"df_test_Xxxx = df_valid_X.copy()\n",
				"df_test_Yyyy = df_valid_Y.copy()\n",
				"\n",
				"fc_test_dataset = tf.data.Dataset.from_tensor_slices((df_test_Xxxx.values, \u25a0\u25a0\u25a0\u25a0\u25a0))\n",
				"test_fc_dataset = fc_test_dataset.batch(max_batch_size)\n",
				"\n",
				"fc_model = keras.Sequential(\n",
				"    [\n",
				"        layers.Dense(100, activation=\"relu\"),\n",
				"        layers.Dropout(.3),\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0(10, \u25a0\u25a0\u25a0\u25a0\u25a0=\"relu\"),\n",
				"        layers.Dropout(.3),\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0(1, activation=\"sigmoid\"),\n",
				"    ]\n",
				")\n",
				"\n",
				"fc_optimizer = tf.keras.optimizers.Adam(\u25a0\u25a0\u25a0\u25a0\u25a0=1e-3)\n",
				"# fc_model = FCModel(df_Yyyy.shape[1])\n",
				"fc_model.compile(\n",
				"    optimizer='adam',\n",
				"    loss = 'mse',\n",
				"    metrics = ['accuracy'],\n",
				")\n",
				"history = fc_model.fit(\n",
				"    train_fc_dataset,\n",
				"    epochs=1,\n",
				")\n",
				"\n",
				"df_score_XG['predict_proba'] = \u25a0\u25a0\u25a0\u25a0\u25a0(df_valid_X)\n",
				"score_auc = sklearn.metrics.roc_auc_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict_proba'])\n",
				"\n",
				"df_score_XG['predict'] = numpy.where(df_score_XG['predict_proba'] > 0.5, 1, 0)\n",
				"score_acc = sklearn.metrics.accuracy_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_prc = sklearn.metrics.precision_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_rcl = sklearn.metrics.recall_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"score_f1s = sklearn.metrics.f1_score(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"\n",
				"print(score_auc, score_acc, \u25a0\u25a0\u25a0\u25a0\u25a0, score_rcl, score_f1s)\n",
				"sklearn.metrics.confusion_matrix(df_score_XG[dict_args['target_col_name']], df_score_XG['predict'])\n",
				"\n",
				"df_check_XG = df_score_XG.copy()\n",
				"df_check_XG['X_axis'] = df_check_XG['predict_proba'] * 100\n",
				"df_check_XG['X_axis'] = df_check_XG['X_axis'].astype(int)\n",
				"df_check_XG['X_axis'] = df_check_XG['X_axis'] / 10\n",
				"\n",
				"df_groupby_XG = df_check_XG.groupby(['X_axis']).agg( Y_axis=('predict', 'count'))\n",
				"#display(df_groupby_XG.T)\n",
				"df_groupby_XG['Y_axis'] = numpy.log(df_groupby_XG['Y_axis'])\n",
				"df_groupby_XG = df_groupby_XG.reset_index()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(x='X_axis',y='Y_axis',rot=270)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.3. \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(dict_args['base_file'])\n",
				"#display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. \ub370\uc774\ud130 \uc77c\ubd80 \ubc1c\ucdcc"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_std = df[['V2', 'Amount']]\n",
				"df_std = df_std.head(100)\n",
				"\n",
				"#plt.cla()\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"plt.rcParams['lines.linewidth'] = 7\n",
				"\n",
				"x_axis = df_std.index.tolist()\n",
				"y_axis_1 = df_std['V2'].tolist()\n",
				"y_axis_2 = df_std['Amount']\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"\n",
				"ax = sns.lineplot(x=x_axis, y=y_axis_1)\n",
				"ax = sns.lineplot(x=x_axis, y=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"#plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. \ud45c\uc900\ud654 \uc608\uc2dc"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_std = df[['V2', 'Amount']]\n",
				"\n",
				"df_std['V2'] = (df_std['V2'] - numpy.mean(df_std['V2'])) / numpy.std(df_std['V2'])\n",
				"df_std['Amount'] = (df_std['Amount'] - numpy.mean(df_std['Amount'])) / numpy.std(\u25a0\u25a0\u25a0\u25a0\u25a0['Amount'])\n",
				"\n",
				"df_std = df_std.head(100)\n",
				"\n",
				"#plt.cla()\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"plt.rcParams['lines.linewidth'] = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"\n",
				"x_axis = df_std.index.tolist()\n",
				"y_axis_1 = df_std['V2'].tolist()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_std['Amount'].tolist()\n",
				"\n",
				"ax = sns.lineplot(x=x_axis, y=y_axis_1)\n",
				"ax = sns.lineplot(x=x_axis, y=y_axis_2)\n",
				"#plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3. \uc815\uaddc\ud654 \uc608\uc2dc"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_std = df[['V2', 'Amount']]\n",
				"\n",
				"df_std['V2'] = (df_std['V2'] - numpy.min(\u25a0\u25a0\u25a0\u25a0\u25a0['V2'])) / (numpy.max(df_std['V2']) - numpy.min(df_std['V2']))\n",
				"df_std['Amount'] = (\u25a0\u25a0\u25a0\u25a0\u25a0['Amount'] - numpy.min(df_std['Amount'])) / (numpy.max(df_std['Amount']) - numpy.min(df_std['Amount']))\n",
				"\n",
				"df_std = df_std.head(100)\n",
				"\n",
				"#plt.cla()\n",
				"\n",
				"sns.set(style='darkgrid')\n",
				"plt.rcParams['lines.linewidth'] = 7\n",
				"\n",
				"x_axis = df_std.index.tolist()\n",
				"y_axis_1 = df_std['V2'].tolist()\n",
				"y_axis_2 = df_std['Amount'].tolist()\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = sns.lineplot(x=x_axis, y=y_axis_1)\n",
				"ax = sns.lineplot(x=x_axis, y=y_axis_2)\n",
				"#plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc815\ubcf4\uac00\uce58 (Information Value) \uac12\uc5d0 \ub530\ub978 \uceec\ub7fc\uc758 \ubd84\ud3ec \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"df_iv_check = pandas.read_csv(dict_args['derived_3_file_source'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.read_csv(\u25a0\u25a0\u25a0\u25a0\u25a0['information_value'])\n",
				"\n",
				"list_variables = df_iv['VAR_NAME'].tolist()\n",
				"for iv_col_name in list_variables:\n",
				"    df_iv_part = df_iv[df_iv['VAR_NAME'] == iv_col_name]\n",
				"    iv_col_value = df_iv_part['IV_VALUE']\u25a0\u25a0\u25a0\u25a0\u25a0()[0]\n",
				"    print('Column:', iv_col_name, ', IV:', iv_col_value)\n",
				"    df_iv_check_part = df_iv_check[[dict_args['id_col_name'], dict_args['target_col_name'], iv_col_name]]\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0[iv_col_name] = df_iv_check_part[iv_col_name].astype(int)\n",
				"    df_iv_group = df_iv_check_part.groupby([iv_col_name]).agg(\n",
				"        CountCustomer = ( dict_args['id_col_name'], 'count' ),\n",
				"        CountTarget = ( dict_args['target_col_name'], numpy.sum )\n",
				"    )\n",
				"    df_iv_group['CountCustomer'] = df_iv_group['CountCustomer']\u25a0\u25a0\u25a0\u25a0\u25a0(int)\n",
				"    df_iv_group['CountTarget'] = df_iv_group['CountTarget'].astype(int)\n",
				"    df_iv_group['C_RATIO'] = df_iv_group['CountCustomer'] / numpy.sum(df_iv_group['CountCustomer']) * 100\n",
				"    df_iv_group['T_RATIO'] = df_iv_group['CountTarget'] / numpy.sum(df_iv_group['CountTarget']) * 100\n",
				"    #display(df_iv_group.T)\n",
				"\n",
				"    #plt.cla()\n",
				"    sns.set(style='darkgrid')\n",
				"    plt.rcParams['lines.linewidth'] = 7\n",
				"    x_axis = df_iv_group.index.tolist()\n",
				"    y_axis_1 = df_iv_group['C_RATIO'].tolist()\n",
				"    y_axis_2 = df_iv_group['T_RATIO'].tolist()\n",
				"    ax = sns.barplot(x=x_axis, y=y_axis_1)\n",
				"    ax2 = plt.twinx()\n",
				"    ax = \u25a0\u25a0\u25a0\u25a0\u25a0(x=x_axis, y=y_axis_2, ax=ax2)\n",
				"    #plt.title = iv_col_name\n",
				"    #plt.show()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 7. \uc9c0\ub3c4 \ud559\uc2b5 \uae30\ucd08 - II. \ud68c\uadc0"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc0ac\uc6a9 \ud568\uc218 \uba85\n",
				"\n",
				"- \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d : get_corr\n",
				"- ADF Test : get_adf_test\n",
				"- Shapiro-wilk Test : get_shapiro_wilk\n",
				"- Feature Importance : get_feature_importance"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# \uc774\uc6a9 \ud30c\uc77c \uba85\n",
				"data_X.csv\n",
				"\n",
				"# \uc774\uc6a9 \uceec\ub7fc \uba85\n",
				"['eod_date', '5380-price_close']"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0. \ud658\uacbd \uc124\uc815"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.1. \ud328\ud0a4\uc9c0 \uc784\ud3ec\ud2b8"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import numpy\n",
				"import os\n",
				"import pandas\n",
				"import matplotlib.pyplot as plt\n",
				"import matplotlib.font_manager as fm\n",
				"import seaborn as sns\n",
				"import warnings"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%matplotlib inline\n",
				"import matplotlib.pyplot as plt\n",
				"\n",
				"plt.rcParams[\"figure.figsize\"] = (40, 15)\n",
				"plt.rcParams['lines.linewidth'] = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"plt.rcParams[\"axes.grid\"] = True"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%%html\n",
				"<style>\n",
				"    table { display: inline-block }\n",
				"    .rendered_html td, .rendered_html \u25a0\u25a0\u25a0\u25a0\u25a0 { text-align: left; }\n",
				"</style>"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.2. \uc870\uac74 \ubc0f \ud30c\ub77c\ubbf8\ud130 \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ================================================================================================================================\n",
				"# \uac1c\uc778\ubcc4 \ud658\uacbd\uc5d0 \ub9de\uac8c \uc870\uc815\ud558\uc2e4 \uac83\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_args = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"# \uc785\ub825 \uad00\ub828 \uc124\uc815\n",
				"# --------------------------------------------------------------------------------------------------------------------------------\n",
				"dict_args['base_file'] = os.path.abspath(\"./Data\") + '/data_X.csv'\n",
				"# \uae30\uc900\uc77c\uc790\n",
				"dict_args['date_col_name'] = 'eod_date'\n",
				"# \ud604\ub300\ucc28 \uc77c\ubcc4 \uc885\uac00 \n",
				"dict_args['target_col_name'] = '5380-price_close'\n",
				"# \ud559\uc2b5 \uad6c\uac04 (window size)\n",
				"dict_args['window_size'] = \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"# \ud14c\uc2a4\ud2b8 \ud06c\uae30\n",
				"dict_args['test_size'] = 2\n",
				"# \ud14c\uc2a4\ud2b8 \uc2dc\uc791 \uae30\uac04\n",
				"dict_args['test_date_start'] = 20140101\n",
				"# \ud14c\uc2a4\ud2b8 \uc885\ub8cc \uae30\uac04\n",
				"dict_args['test_date_finish'] = 20140410\n",
				"# \ubaa8\ub378 \ud30c\uc77c \uc800\uc7a5 \ud328\ud134\n",
				"dict_args['model_file'] = 'zulu_train_model_MODELNAME.h5'\n",
				"\n",
				"dir_work = 'work/'\n",
				"dir_refs = 'refs/'\n",
				"\n",
				"if not os.path.exists(dir_work):\n",
				"    os.mkdir(dir_work)\n",
				"if not os.path.exists(dir_refs):\n",
				"    os.mkdir(dir_refs)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.3. \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df = pandas.read_csv(dict_args['base_file'])\n",
				"df = df.set_index([dict_args['date_col_name']])\n",
				"#display(df)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 0.4. \ud568\uc218 \uc120\uc5b8"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## 0.4.1. \ud568\uc218 \uc120\uc5b8 - 01"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# data load\n",
				"def data_load(path,file_name, Date_col):\n",
				"    import pandas\n",
				"    df = pandas.read_csv(path + file_name, index_col = False, \u25a0\u25a0\u25a0\u25a0\u25a0 = False)\n",
				"    df[Date_col] = pandas.to_datetime(df[Date_col], \u25a0\u25a0\u25a0\u25a0\u25a0='%Y%m%d')\n",
				"    df = df.set_index(Date_col)\n",
				"    return df"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1.1. \uc608\uce21 \ub300\uc0c1 \uc124\uc815"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_Y = df[[dict_args['target_col_name']]]\n",
				"df_Y = df_Y[df_Y.index <= \u25a0\u25a0\u25a0\u25a0\u25a0['test_date_finish']]\n",
				"df_Y = df_Y[df_Y[dict_args['target_col_name']] > 0]\n",
				"df_Y['target'] = df_Y[dict_args['target_col_name']].shift(-1)\n",
				"df_Y = df_Y.dropna(axis='rows')\n",
				"del df_Y[dict_args['target_col_name']]\n",
				"#display(df_Y)\n",
				"df_Y.to_csv('data_Y_source.csv')\n",
				"df_plot_Y = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"df_plot_Y['datetime'] = pandas.to_datetime(df_plot_Y.index, format='%Y%m%d')\n",
				"df_plot_Y = df_plot_Y.set_index(['datetime'])\n",
				"# df_plot_Y.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 1.2. \ub370\uc774\ud130 \ucc98\ub9ac \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc704\ud55c \uc885\uac00 \ub370\uc774\ud130 \ubc1c\ucdcc"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_X = pandas.DataFrame()\n",
				"for col_name in df.columns:\n",
				"    if 'close' in col_name:\n",
				"        if df_X.shape[0] == 0:\n",
				"            df_X = df[[col_name]]\n",
				"        else:\n",
				"            df_X[\u25a0\u25a0\u25a0\u25a0\u25a0] = df[col_name]\n",
				"#display(df_X)\n",
				"df_X.to_csv('data_X_source.csv')\n",
				"\n",
				"df_source = df_Y.join(df_X, how='inner')\n",
				"df_source = df_source.fillna(1 + \u25a0\u25a0\u25a0\u25a0\u25a0-\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"#display(df_source)\n",
				"for col_name in df_source.columns:\n",
				"    if numpy.min(df_source[col_name]) == numpy.max(df_source[col_name]):\n",
				"        del df_source[col_name]\n",
				"#display(df_source)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2. \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1. \uc815\uaddc\ud654 / \ud45c\uc900\ud654 / \ub85c\uadf8 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1.1. \uc815\uaddc\ud654 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_norm = df_source.copy()\n",
				"list_min_max = list()\n",
				"for col_name in df_norm.columns:\n",
				"    val_min = \u25a0\u25a0\u25a0\u25a0\u25a0(df_norm[col_name])\n",
				"    val_max = numpy.max(df_norm[col_name])\n",
				"    if val_min == val_max:\n",
				"        del df_norm[col_name]\n",
				"        continue\n",
				"    list_min_max.append({\n",
				"        'col': \u25a0\u25a0\u25a0\u25a0\u25a0,\n",
				"        'min': val_min,\n",
				"        'max': val_max\n",
				"    })\n",
				"    df_norm[col_name] = (df_norm[col_name] - val_min) / (val_max - \u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"df_min_max = pandas.DataFrame(list_min_max)\n",
				"df_min_max.to_csv(dir_refs + 'ref_min_max.csv', index=None)\n",
				"#display(df_norm)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1.2. \ud45c\uc900\ud654 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_std = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"for col_name in df_std.columns:\n",
				"    val_std = numpy.std(df_std[col_name])\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = numpy.mean(df_std[col_name])\n",
				"    if val_std == 0:\n",
				"        del df_std[col_name]\n",
				"        continue\n",
				"    df_std[col_name] = (df_std[col_name] - val_avg) / val_std\n",
				"#display(df_std)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.1.3. \ub85c\uadf8 \ub370\uc774\ud130 \uc0dd\uc131"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_log = df_source.copy()\n",
				"df_log = numpy.log(df_log)\n",
				"#display(df_log)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2. \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2.1. \uc6d0\ucc9c \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d\n",
				"def get_corr(p_source, p_target):\n",
				"    import numpy\n",
				"    import pandas\n",
				"\n",
				"    list_corr = list()\n",
				"    for col_name in p_source.columns:\n",
				"        if col_name == 'target':\n",
				"            continue\n",
				"        if col_name == p_target:\n",
				"            continue\n",
				"\n",
				"        df_corr_check = p_source[['target', col_name]].corr(method='pearson')\n",
				"\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0({\n",
				"            'VAR_NAME': col_name,\n",
				"            'CORR': df_corr_check.head(1)[col_name].tolist()[\u25a0\u25a0\u25a0\u25a0\u25a0],\n",
				"            'CORR-ABS': numpy.abs(df_corr_check.head(1)[col_name].tolist()[0]),\n",
				"        })\n",
				"    df_corr = pandas.DataFrame(list_corr)\n",
				"    df_corr = df_corr.sort_values(by=['CORR-ABS'], ascending=False)\n",
				"\n",
				"    return df_corr\n",
				"\n",
				"df_corr_src = df_source.fillna(-1)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = get_corr(df_corr_src, dict_args['target_col_name'])\n",
				"#display(df_corr_source.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_corr = df_corr_src[['target', df_corr_source.head(1)['VAR_NAME'].tolist()[0]]]\n",
				"df_plot_corr['datetime'] = pandas.to_datetime(\u25a0\u25a0\u25a0\u25a0\u25a0, format='%Y%m%d')\n",
				"df_plot_corr = df_plot_corr.set_index(['datetime'])\n",
				"# df_plot_corr.plot()\n",
				"#display(df_plot_corr)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2.2. \ud45c\uc900\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_corr_src = df_std.copy()\n",
				"df_corr_std = get_corr(df_corr_src, dict_args['target_col_name'])\n",
				"#display(df_corr_std.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_corr = df_corr_src[['target', df_corr_std.head(1)['VAR_NAME'].tolist()[0]]]\n",
				"df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n",
				"df_plot_corr = \u25a0\u25a0\u25a0\u25a0\u25a0(['datetime'])\n",
				"# df_plot_corr.plot()\n",
				"#display(df_plot_corr)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2.3. \uc815\uaddc\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_corr_src = df_norm.copy()\n",
				"df_corr_norm = get_corr(df_corr_src, \u25a0\u25a0\u25a0\u25a0\u25a0['target_col_name'])\n",
				"#display(df_corr_norm.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_corr = df_corr_src[['target', df_corr_norm.head(1)['VAR_NAME'].tolist()[0]]]\n",
				"df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n",
				"df_plot_corr = df_plot_corr.set_index(['datetime'])\n",
				"# df_plot_corr.plot()\n",
				"#display(df_plot_corr)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.2.4. \ub85c\uadf8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_corr_src = df_log.copy()\n",
				"df_corr_log = get_corr(\u25a0\u25a0\u25a0\u25a0\u25a0, dict_args['target_col_name'])\n",
				"#display(df_corr_log.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_corr = df_corr_src[['target', df_corr_log.head(1)['VAR_NAME'].tolist()[0]]]\n",
				"df_plot_corr['datetime'] = pandas.to_datetime(df_plot_corr.index, format='%Y%m%d')\n",
				"df_plot_corr = df_plot_corr.set_index(['datetime'])\n",
				"# df_plot_corr.plot()\n",
				"#display(df_plot_corr)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3. Feature Importance"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.1. \uc6d0\ucc9c \ub370\uc774\ud130\uc5d0 \ub300\ud574 Feature Importance \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Feature Importance\n",
				"def get_feature_importance(l_data_x, p_target):\n",
				"    import numpy\n",
				"    import pandas\n",
				"    import sklearn.ensemble\n",
				"\n",
				"    l_data_y = l_data_x['target']\n",
				"    del l_data_x['target']\n",
				"    del l_data_x[p_target]\n",
				"\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = sklearn.ensemble.RandomForestRegressor(\n",
				"        max_depth = 50,\n",
				"        min_samples_split = 5,\n",
				"        min_samples_leaf = 3,\n",
				"        random_state = 0,\n",
				"        n_estimators = 100,\n",
				"    )\n",
				"    forest.fit(l_data_x, l_data_y)\n",
				"    l_importance = forest.feature_importances_\n",
				"\n",
				"    indices = numpy.argsort(l_importance)\n",
				"\n",
				"    columns = l_data_x.columns.tolist()\n",
				"    column_count = len(columns)\n",
				"\n",
				"    list_var = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"    for idx in range(column_count):\n",
				"        list_var.append({\n",
				"            'feature_name': columns[indices[idx]],\n",
				"            'feature_value': l_importance[indices[idx]]\n",
				"        })\n",
				"\n",
				"    df_fi_source = pandas.DataFrame(list_var)\n",
				"    df_fi_source = df_fi_source.sort_values(by='feature_value', ascending=False)\n",
				"\n",
				"    return df_fi_source\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = \u25a0\u25a0\u25a0\u25a0\u25a0(-1)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n",
				"#display(df_fi_source.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_fi_src = df_src[['target', \u25a0\u25a0\u25a0\u25a0\u25a0(1)['feature_name'].tolist()[0]]]\n",
				"df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n",
				"df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n",
				"# df_plot_fi_src.plot()\n",
				"#display(df_plot_fi_src)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.2. \ud45c\uc900\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Feature Importance \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_src = df_std.fillna(-1)\n",
				"df_fi_std = get_feature_importance(df_src.copy(), dict_args['target_col_name'])\n",
				"#display(df_fi_std.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"df_plot_fi_src = df_src[['target', \u25a0\u25a0\u25a0\u25a0\u25a0(1)['feature_name'].tolist()[0]]]\n",
				"df_plot_fi_src['datetime'] = pandas.to_datetime(\u25a0\u25a0\u25a0\u25a0\u25a0, format='%Y%m%d')\n",
				"df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n",
				"# df_plot_fi_src.plot()\n",
				"#display(df_plot_fi_src)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.3. \uc815\uaddc\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Feature Importance \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_src = df_norm.fillna(-1)\n",
				"df_fi_norm = \u25a0\u25a0\u25a0\u25a0\u25a0importance(df_src.copy(), dict_args['target_col_name'])\n",
				"#display(df_fi_norm.T)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0('ignore')\n",
				"df_plot_fi_src = df_src[['target', df_fi_norm.head(1)['feature_name'].tolist()[0]]]\n",
				"df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n",
				"df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n",
				"# df_plot_fi_src.plot()\n",
				"#display(df_plot_fi_src)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.3.4. \ub85c\uadf8 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Feature Importance \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_src = df_log.fillna(-1)\n",
				"df_fi_log = get_feature_importance(\u25a0\u25a0\u25a0\u25a0\u25a0(), dict_args['target_col_name'])\n",
				"#display(df_fi_log.T)\n",
				"warnings.filterwarnings('ignore')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_src[['target', df_fi_log.head(1)['feature_name'].tolist()[0]]]\n",
				"df_plot_fi_src['datetime'] = pandas.to_datetime(df_plot_fi_src.index, format='%Y%m%d')\n",
				"df_plot_fi_src = df_plot_fi_src.set_index(['datetime'])\n",
				"# df_plot_fi_src.plot()\n",
				"#display(df_plot_fi_src)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4. \uacf5\uc801\ubd84 \uac80\uc815 - ADF Test"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.1. \uc6d0\ucc9c \ub370\uc774\ud130\uc5d0 \ub300\ud574 ADF Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# ADF Test\n",
				"def get_adf_test(p_source, p_col_1, p_col_2):\n",
				"    import statsmodels.tsa.stattools\n",
				"\n",
				"    p_source['spread'] = p_source[p_col_1] - p_source[p_col_2]\n",
				"    return statsmodels.tsa.stattools.adfuller(p_source['spread'])\n",
				"\n",
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_source.fillna(-1)\n",
				"list_result = list()\n",
				"for col_name \u25a0\u25a0\u25a0\u25a0\u25a0 df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    df_adf = get_adf_test(df_src, 'target', col_name)\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = df_adf[1]\n",
				"    if adf_p_value == numpy.nan:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'adf p-value': adf_p_value,\n",
				"    })\n",
				"df_adf_src = \u25a0\u25a0\u25a0\u25a0\u25a0(list_result)\n",
				"df_adf_src = df_adf_src.sort_values(by=['adf p-value'], ascending=True)\n",
				"#display(df_adf_src.head(7))\n",
				"selected_feature = df_adf_src.head(1)['feature_name'].tolist()[0]\n",
				"df_plot_adf_src = df_src[['target', selected_feature]]\n",
				"df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n",
				"df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n",
				"# df_plot_adf_src.plot()\n",
				"#display(df_plot_adf_src)\n",
				"df_plot_adf_src['spread'] = \u25a0\u25a0\u25a0\u25a0\u25a0['target'] - df_plot_adf_src[selected_feature]\n",
				"df_plot = df_plot_adf_src[['spread']]\n",
				"# df_plot.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.2. \ud45c\uc900\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 ADF Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_std.fillna(-1)\n",
				"list_result = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"for col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = get_adf_test(df_src, 'target', col_name)\n",
				"    adf_p_value = df_adf[1]\n",
				"    if adf_p_value == numpy.nan:\n",
				"        continue\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'adf p-value': adf_p_value,\n",
				"    })\n",
				"df_adf_std = pandas.DataFrame(list_result)\n",
				"df_adf_std = df_adf_std.sort_values(by=['adf p-value'], ascending=True)\n",
				"#display(df_adf_std.head(7))\n",
				"selected_feature = df_adf_src.head(1)['feature_name'].tolist()[0]\n",
				"df_plot_adf_src = df_src[['target', selected_feature]]\n",
				"df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n",
				"df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n",
				"# df_plot_adf_src.plot()\n",
				"#display(df_plot_adf_src)\n",
				"df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n",
				"df_plot = \u25a0\u25a0\u25a0\u25a0\u25a0[['spread']]\n",
				"# df_plot.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.3. \uc815\uaddc\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 ADF Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_norm.fillna(-1)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = list()\n",
				"for col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    df_adf = get_adf_test(df_src, 'target', col_name)\n",
				"    adf_p_value = df_adf[1]\n",
				"    if adf_p_value == numpy.nan:\n",
				"        continue\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'adf p-value': adf_p_value,\n",
				"    })\n",
				"df_adf_norm = \u25a0\u25a0\u25a0\u25a0\u25a0(list_result)\n",
				"df_adf_norm = df_adf_norm.sort_values(by=['adf p-value'], ascending=True)\n",
				"#display(df_adf_norm.head(7))\n",
				"selected_feature = df_adf_norm.head(1)['feature_name'].tolist()[0]\n",
				"df_plot_adf_src = df_src[['target', selected_feature]]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['datetime'] = pandas.to_datetime(\u25a0\u25a0\u25a0\u25a0\u25a0, format='%Y%m%d')\n",
				"df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n",
				"df_plot_adf_src = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[selected_feature])]\n",
				"df_plot_adf_src.plot()\n",
				"#display(df_plot_adf_src)\n",
				"df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n",
				"df_plot = df_plot_adf_src[['spread']]\n",
				"#df_plot.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.4.4. \ub85c\uadf8 \ub370\uc774\ud130\uc5d0 \ub300\ud574 ADF Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_log.fillna(-1)\n",
				"list_result = list()\n",
				"for col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    df_adf = get_adf_test(df_src, 'target', col_name)\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = df_adf[1]\n",
				"    if adf_p_value == numpy.nan:\n",
				"        continue\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'adf p-value': adf_p_value,\n",
				"    })\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.DataFrame(list_result)\n",
				"df_adf_log = df_adf_log.sort_values(by=['adf p-value'], ascending=True)\n",
				"#display(df_adf_log.head(7))\n",
				"selected_feature = df_adf_log.head(\u25a0\u25a0\u25a0\u25a0\u25a0)['feature_name'].tolist()[0]\n",
				"df_plot_adf_src = df_src[['target', selected_feature]]\n",
				"df_plot_adf_src['datetime'] = pandas.to_datetime(df_plot_adf_src.index, format='%Y%m%d')\n",
				"df_plot_adf_src = df_plot_adf_src.set_index(['datetime'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot_adf_src[df_plot_adf_src[selected_feature] != numpy.min(df_plot_adf_src[selected_feature])]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"#display(df_plot_adf_src)\n",
				"df_plot_adf_src['spread'] = df_plot_adf_src['target'] - df_plot_adf_src[selected_feature]\n",
				"df_plot = df_plot_adf_src[['spread']]\n",
				"df_plot.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5. \uacf5\uc801\ubd84 \uac80\uc815 - Shapiro-wilk Test"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5.1. \uc6d0\ucc9c \ub370\uc774\ud130\uc5d0 \ub300\ud574 shapiro-wilk Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Shapiro wilk Test\n",
				"def \u25a0\u25a0\u25a0\u25a0\u25a0(p_source, p_col_1, p_col_2):\n",
				"    import scipy.stats\n",
				"\n",
				"    p_source['spread'] = p_source[p_col_1] - p_source[p_col_2]\n",
				"    return scipy.stats.shapiro(p_source['spread'])\n",
				"\n",
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_source.fillna(-1)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = list()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n",
				"    shapiro_wilk = df_shapiro.statistic\n",
				"    if shapiro_wilk == numpy.nan:\n",
				"        continue\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'shapiro-wilk': shapiro_wilk,\n",
				"    })\n",
				"df_shapiro_src = pandas.DataFrame(list_result)\n",
				"df_shapiro_src = df_shapiro_src.sort_values(by=['shapiro-wilk'], ascending=False)\n",
				"#display(df_shapiro_src)\n",
				"selected_feature = df_shapiro_src.head(1)['feature_name'].tolist()[0]\n",
				"df_plot = df_src[['target', selected_feature]]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0['datetime'] = pandas.to_datetime(\u25a0\u25a0\u25a0\u25a0\u25a0, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot.plot()\n",
				"#display(df_plot)\n",
				"df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n",
				"df_plot = df_plot[['spread']]\n",
				"df_plot.plot()\n",
				"#display(df_plot)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5.2. \ud45c\uc900\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Shapiro-wilk Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_std.fillna(-1)\n",
				"list_result = list()\n",
				"for col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n",
				"    shapiro_wilk = df_shapiro.statistic\n",
				"    if shapiro_wilk == numpy.nan:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'shapiro-wilk': shapiro_wilk,\n",
				"    })\n",
				"df_shapiro_std = pandas.DataFrame(list_result)\n",
				"df_shapiro_std = df_shapiro_std.sort_values(by=['shapiro-wilk'], ascending=False)\n",
				"#display(df_shapiro_std)\n",
				"selected_feature = \u25a0\u25a0\u25a0\u25a0\u25a0(1)['feature_name'].tolist()[0]\n",
				"df_plot = df_src[['target', selected_feature]]\n",
				"df_plot['datetime'] = \u25a0\u25a0\u25a0\u25a0\u25a0(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot.plot()\n",
				"#display(df_plot)\n",
				"df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n",
				"df_plot = df_plot[['spread']]\n",
				"df_plot.plot()\n",
				"#display(df_plot)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5.3. \uc815\uaddc\ud654 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Shapiro-wilk Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_norm.fillna(-1)\n",
				"list_result = list()\n",
				"for col_name \u25a0\u25a0\u25a0\u25a0\u25a0 df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = get_shapiro_wilk(df_src, 'target', col_name)\n",
				"    shapiro_wilk = df_shapiro.statistic\n",
				"    if shapiro_wilk == numpy.nan:\n",
				"        continue\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'shapiro-wilk': shapiro_wilk,\n",
				"    })\n",
				"df_shapiro_norm = pandas.DataFrame(list_result)\n",
				"df_shapiro_norm = df_shapiro_norm.sort_values(by=['shapiro-wilk'], ascending=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"#display(df_shapiro_norm)\n",
				"selected_feature = df_shapiro_norm.head(1)['feature_name'].tolist()[0]\n",
				"df_plot = df_src[['target', selected_feature]]\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot.plot()\n",
				"#display(df_plot)\n",
				"df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n",
				"df_plot = df_plot[['spread']]\n",
				"df_plot.plot()\n",
				"#display(df_plot)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.5.4. \ub85c\uadf8 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Shapiro-wilk Test \ud655\uc778"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"df_src = df_log.fillna(-1)\n",
				"list_result = list()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 col_name in df_src.columns:\n",
				"    if col_name == dict_args['target_col_name']:\n",
				"        continue\n",
				"    if col_name == 'target':\n",
				"        continue\n",
				"    df_shapiro = get_shapiro_wilk(df_src, 'target', col_name)\n",
				"    shapiro_wilk = df_shapiro.statistic\n",
				"    if \u25a0\u25a0\u25a0\u25a0\u25a0 == numpy.nan:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    list_result.append({\n",
				"        'feature_name': col_name,\n",
				"        'shapiro-wilk': shapiro_wilk,\n",
				"    })\n",
				"df_shapiro_log = pandas.DataFrame(list_result)\n",
				"df_shapiro_log = df_shapiro_log.sort_values(by=['shapiro-wilk'], \u25a0\u25a0\u25a0\u25a0\u25a0=False)\n",
				"#display(df_shapiro_log)\n",
				"selected_feature = df_shapiro_log.head(1)['feature_name'].tolist()[0]\n",
				"df_plot = df_src[['target', selected_feature]]\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot.plot()\n",
				"#display(df_plot)\n",
				"df_plot['spread'] = df_plot['target'] - df_plot[selected_feature]\n",
				"df_plot = df_plot[['spread']]\n",
				"df_plot.plot()\n",
				"#display(df_plot)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 2.6. \ubcc0\uc218 \ucde8\ud569"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"list_features = ['target']\n",
				"list_features.extend(df_corr_source.head(20)['VAR_NAME']\u25a0\u25a0\u25a0\u25a0\u25a0())\n",
				"list_features.extend(\u25a0\u25a0\u25a0\u25a0\u25a0(20)['VAR_NAME'].tolist())\n",
				"list_features.extend(df_corr_norm.head(20)['VAR_NAME'].tolist())\n",
				"list_features.extend(df_corr_log.head(20)['VAR_NAME'].tolist())\n",
				"\n",
				"list_features.extend(df_fi_source.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_fi_std.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_fi_norm.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_fi_log.head(20)['feature_name'].tolist())\n",
				"\n",
				"list_features.extend(df_adf_src.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_adf_std.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_adf_norm.head(20)['feature_name'].tolist())\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(df_adf_log.head(20)['feature_name'].tolist())\n",
				"\n",
				"list_features.extend(df_shapiro_src.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_shapiro_std.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_shapiro_norm.head(20)['feature_name'].tolist())\n",
				"list_features.extend(df_shapiro_log.head(20)['feature_name'].tolist())\n",
				"\n",
				"list_features = list(set(list_features))\n",
				"# print(list_features)\n",
				"\n",
				"df_train = df_source.fillna(\u25a0\u25a0\u25a0\u25a0\u25a0 + 1e-7)\n",
				"df_train = df_train[list_features]\n",
				"#display(df_train)\n",
				"df_train.to_csv('data_modeling.csv')\n",
				"df_plot = df_train[['target']]\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot.set_index(['datetime'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3. \ubaa8\ub378\ub9c1"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.1. linear_model.OrthogonalMatchingPursuitCV"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import sklearn.linear_model\n",
				"\n",
				"model = sklearn.linear_model.OrthogonalMatchingPursuitCV(cv=3)\n",
				"\n",
				"list_predict = list()\n",
				"for idx in range(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(\u25a0\u25a0\u25a0\u25a0\u25a0['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"    df_train_Y = \u25a0\u25a0\u25a0\u25a0\u25a0['target']\n",
				"    df_tests_Y = df_tests_X['target']\n",
				"    del df_train_X['target']\n",
				"    del df_tests_X['target']\n",
				"\n",
				"    model.fit(\u25a0\u25a0\u25a0\u25a0\u25a0, df_train_Y)\n",
				"    real_value = df_tests_Y.values.tolist()[0]\n",
				"    predict_value = model.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"df_predict = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"df_plot_value.plot()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot[['PercentError']]\n",
				"df_plot_error.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.2. tree.DecisionTreeRegressor"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn.tree\n",
				"\n",
				"model = sklearn.tree.DecisionTreeRegressor(\u25a0\u25a0\u25a0\u25a0\u25a0='squared_error', random_state=111)\n",
				"\n",
				"hyper_parameter = {\n",
				"    'max_features': ['auto'],\n",
				"    'max_depth': [3],\n",
				"    'min_samples_split': [2],\n",
				"    'min_samples_leaf': [1],\n",
				"}\n",
				"df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size'])\u25a0\u25a0\u25a0\u25a0\u25a0(dict_args['window_size'])\n",
				"df_tune_Y = df_tune_X['target']\n",
				"del df_tune_X['target']\n",
				"grid_search = \u25a0\u25a0\u25a0\u25a0\u25a0(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n",
				"grid_search.fit(df_tune_X, df_tune_Y)\n",
				"model_instance = grid_search.best_estimator_\n",
				"\n",
				"list_predict = list()\n",
				"for idx in \u25a0\u25a0\u25a0\u25a0\u25a0(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n",
				"\n",
				"    df_train_Y = df_train_X['target']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = df_tests_X['target']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 df_train_X['target']\n",
				"    del df_tests_X['target']\n",
				"\n",
				"    model_instance.fit(df_train_X, df_train_Y)\n",
				"    real_value = df_tests_Y.values.tolist()[0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"df_predict = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, \u25a0\u25a0\u25a0\u25a0\u25a0='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"df_plot_value.plot()\n",
				"df_plot_error = df_plot[['PercentError']]\n",
				"df_plot_error.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.3. ensemble.RandomForestRegressor"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn.ensemble\n",
				"\n",
				"model = sklearn.ensemble.RandomForestRegressor(criterion='squared_error', random_state=\u25a0\u25a0\u25a0\u25a0\u25a0, n_estimators=1)\n",
				"\n",
				"hyper_parameter = {\n",
				"    'max_features': ['auto'],\n",
				"    'max_depth': [2],\n",
				"    'min_samples_split': [2],\n",
				"    'min_samples_leaf': [2],\n",
				"}\n",
				"df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(\u25a0\u25a0\u25a0\u25a0\u25a0['window_size'])\n",
				"df_tune_Y = df_tune_X['target']\n",
				"del df_tune_X['target']\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n",
				"grid_search.fit(df_tune_X, df_tune_Y)\n",
				"model_instance = grid_search.best_estimator_\n",
				"\n",
				"list_predict = list()\n",
				"for idx in \u25a0\u25a0\u25a0\u25a0\u25a0(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n",
				"\n",
				"    df_train_Y = df_train_X['target']\n",
				"    df_tests_Y = df_tests_X['target']\n",
				"    del df_train_X['target']\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 df_tests_X['target']\n",
				"\n",
				"    model_instance.fit(df_train_X, \u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"    real_value = df_tests_Y.values.tolist()[\u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (\u25a0\u25a0\u25a0\u25a0\u25a0 - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"df_predict = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"df_plot_value.plot()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot[['PercentError']]\n",
				"df_plot_error.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.4. xgboost.XGBRegressor"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import xgboost\n",
				"\n",
				"model = xgboost.XGBRegressor(\u25a0\u25a0\u25a0\u25a0\u25a0='gbtree', objective='reg:squarederror', n_estimators=2)\n",
				"\n",
				"hyper_parameter = {\n",
				"    'reg_lambda': [1.0, 0.0, 0.1],\n",
				"    'reg_alpha': [0.0, 0.001, 0.01],\n",
				"}\n",
				"df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n",
				"df_tune_Y = df_tune_X['target']\n",
				"del df_tune_X['target']\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', \u25a0\u25a0\u25a0\u25a0\u25a0=-1, verbose=0)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(df_tune_X, df_tune_Y)\n",
				"model_instance = grid_search.best_estimator_\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = list()\n",
				"for idx in range(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n",
				"\n",
				"    df_train_Y = df_train_X['target']\n",
				"    df_tests_Y = df_tests_X['target']\n",
				"    del df_train_X['target']\n",
				"    del df_tests_X['target']\n",
				"\n",
				"    model_instance.fit(df_train_X, \u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"    real_value = df_tests_Y.values.tolist()[0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"df_predict = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot[['real_value', 'predict_value']]\n",
				"df_plot_value.plot()\n",
				"df_plot_error = \u25a0\u25a0\u25a0\u25a0\u25a0[['PercentError']]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.5. LightGBM"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\u25a0\u25a0\u25a0\u25a0\u25a0('ignore')\n",
				"\n",
				"import lightgbm\n",
				"\n",
				"model = lightgbm.LGBMRegressor(random_state=0, objective='regression', n_jobs=-1, max_depth=-1, subsample=0.8, min_child_weight=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"hyper_parameter = {\n",
				"    'learning_rate': [0.1],\n",
				"    'num_leaves': [2],\n",
				"}\n",
				"df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n",
				"df_tune_Y = df_tune_X['target']\n",
				"del df_tune_X['target']\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n",
				"grid_search.fit(df_tune_X, df_tune_Y)\n",
				"model_instance = grid_search.best_estimator_\n",
				"\n",
				"list_predict = list()\n",
				"for idx in range(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(dict_args['window_size'] + \u25a0\u25a0\u25a0\u25a0\u25a0 + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"\n",
				"    df_train_Y = df_train_X['target']\n",
				"    df_tests_Y = df_tests_X['target']\n",
				"    del df_train_X['target']\n",
				"    del df_tests_X['target']\n",
				"\n",
				"    model_instance.fit(df_train_X, df_train_Y)\n",
				"    real_value = df_tests_Y.values.tolist()[0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"df_plot_error = \u25a0\u25a0\u25a0\u25a0\u25a0[['PercentError']]\n",
				"df_plot_error.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.6. keras.LSTM"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.6.1. LSTM Style #1"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# window \uae30\uac04\uc5d0 \uacfc\uac70 \ub370\uc774\ud130\ub97c \ud1b5\ud55c \uc608\uce21 \ub370\uc774\ud130 \uc0dd\uc131 \ud568\uc218\n",
				"def make_dataset(data, label, \u25a0\u25a0\u25a0\u25a0\u25a0):\n",
				"    import numpy\n",
				"    feature_list = list()\n",
				"    label_list = list()\n",
				"    for i in range(len(data) - window_size):\n",
				"        feature_list.append(numpy.array(data.iloc[i:i+window_size]))\n",
				"        label_list.append(numpy.array(\u25a0\u25a0\u25a0\u25a0\u25a0[i+window_size]))\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 numpy.array(feature_list), numpy.array(label_list)\n",
				"\n",
				"def split_data(DataFrame, test_size, target_col, window_size):\n",
				"    import pandas\n",
				"    # train_test \ubd84\ub9ac\n",
				"    train = DataFrame[:-test_size]\n",
				"    test = DataFrame[-\u25a0\u25a0\u25a0\u25a0\u25a0:]\n",
				"\n",
				"    # feature, target \ubd84\ub9ac\n",
				"    feature_cols = list(DataFrame.columns)\n",
				"    feature_cols.remove(target_col)\n",
				"\n",
				"    # train_data\n",
				"    train_feature = train[feature_cols]\n",
				"    train_label = pandas.DataFrame(train[\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"\n",
				"    # test_data\n",
				"    test_feature = test[feature_cols]\n",
				"    test_label = pandas.DataFrame(\u25a0\u25a0\u25a0\u25a0\u25a0[\u25a0\u25a0\u25a0\u25a0\u25a0])\n",
				"\n",
				"    # \uc608\uce21 \uc8fc\uae30\uc5d0 \ub530\ub978 timestep \uc0dd\uc131\n",
				"    train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n",
				"    test_feature, test_label = make_dataset(test_feature, \u25a0\u25a0\u25a0\u25a0\u25a0, window_size)\n",
				"\n",
				"    # print('split_data', train_feature.shape, train_label.shape)\n",
				"    # print('split_data', test_feature.shape, test_label.shape)\n",
				"\n",
				"    return train_feature, train_label, test_feature, test_label\n",
				"\n",
				"def LSTM_MODEL(train_feature, train_label, test_size, units=4, loss=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=200, batch_size=16):\n",
				"    \n",
				"    import sklearn\n",
				"    import tensorflow.keras.models\n",
				"    import tensorflow.keras.layers\n",
				"    import tensorflow.keras.callbacks\n",
				"\n",
				"    # train_valid \ubd84\ud560\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(train_feature, train_label, test_size=test_size)\n",
				"\n",
				"    # print('LSTM_MODEL', x_train.shape, x_valid.shape)\n",
				"    # print('LSTM_MODEL', y_train.shape, y_valid.shape)\n",
				"\n",
				"    # Sequential \ubaa8\ub378 \ub808\uc774\uc5b4 \uc0dd\uc131\n",
				"    model = tensorflow.keras.models.Sequential()\n",
				"    # LSTM \ub808\uc774\uc5b4 \ucd94\uac00\n",
				"    model.add(tensorflow.keras.layers.LSTM(\n",
				"        units,\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0shape=(train_feature.shape[1], \u25a0\u25a0\u25a0\u25a0\u25a0shape[2]),\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0='relu',\n",
				"        return_sequences=False\n",
				"    ))\n",
				"    # Dense \ub808\uc774\uc5b4 \ucd94\uac00\n",
				"    model.add(tensorflow.keras.layers.Dense(32))\n",
				"    model.add(\u25a0\u25a0\u25a0\u25a0\u25a0(1))\n",
				"    # print(model.summary())\n",
				"    # \ubaa8\ub378 \ucef4\ud30c\uc77c \ud65c\uc131\ud568\uc218 loss_function \uc9c0\uc815 , \ucd5c\uc801\ud654\ud568\uc218 \uc9c0\uc815\n",
				"    model.compile(loss=loss, optimizer=optimizer)\n",
				"    # \uc5bc\ub9ac\uc2a4\ud0d1\ud551 ( val_loss\uc758 \ubcc0\ud654\ub97c \ube44\uad50, \uac19\uc740 \uac12 5\ubc88 \uc77c\uc2dc \uc911\uc9c0)\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 = tensorflow.keras.callbacks.EarlyStopping(monitor=monitor, patience=20, baseline=0.4, mode ='min')\n",
				"    # \ubaa8\ud615 \uac00\uc911\uce58 \uc800\uc7a5 \ud30c\uc77c \uacbd\ub85c \uc120\uc5b8\n",
				"    # \ubaa8\ud615 \uac00\uc911\uce58 \ud310\ub2e8 \ubc0f \ubaa8\ub378 \uc800\uc7a5\n",
				"    filename = 'refs/model_checkpoint.h5'\n",
				"    checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filename, \u25a0\u25a0\u25a0\u25a0\u25a0=monitor, verbose=1, save_best_only=True, mode='min')\n",
				"\n",
				"    # \ubaa8\ud615 \ud559\uc2b5\n",
				"    model.fit(x_train, y_train,\n",
				"              epochs=epochs,\n",
				"              batch_size=batch_size,\n",
				"              validation_data=(x_valid, y_valid),\n",
				"              callbacks=[early_stop, checkpoint],\n",
				"              verbose = 1\n",
				"              )\n",
				"\n",
				"    \u25a0\u25a0\u25a0\u25a0\u25a0 model, filename\n",
				"\n",
				"\n",
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn\n",
				"import tensorflow.keras.models\n",
				"import tensorflow.keras.layers\n",
				"import tensorflow.keras.callbacks\n",
				"\n",
				"df_test_scaled = df_source.fillna(1 + 1e-7)\n",
				"list_min_max = list()\n",
				"for col_name in df_source.columns:\n",
				"    val_min = \u25a0\u25a0\u25a0\u25a0\u25a0(df_test_scaled[col_name])\n",
				"    val_max = numpy.max(df_test_scaled[col_name])\n",
				"    if val_min == val_max:\n",
				"        del df_test_scaled[col_name]\n",
				"        continue\n",
				"    list_min_max.append({\n",
				"        'col': col_name,\n",
				"        'min': val_min,\n",
				"        'max': val_max\n",
				"    })\n",
				"    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n",
				"df_min_max = pandas.DataFrame(list_min_max)\n",
				"list_predict = list()\n",
				"for idx in range(dict_args['test_size']):\n",
				"\n",
				"    df_train_sub = df_test_scaled.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    predict_date = df_train_sub.tail(1).index.tolist()[0]\n",
				"    df_train_X, \u25a0\u25a0\u25a0\u25a0\u25a0, \u25a0\u25a0\u25a0\u25a0\u25a0, df_tests_Y = split_data(df_train_sub, 2, 'target', 1)\n",
				"\n",
				"    model_instance, filename = LSTM_MODEL(df_train_X, df_train_Y, 0.2, units=2, loss=\"mean_squared_error\", optimizer='adam', monitor=\"val_loss\", epochs=2, batch_size=128)\n",
				"    model_instance.fit(df_train_X, df_train_Y)\n",
				"\n",
				"    real_value = df_tests_Y.tolist()[0][0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0][0]\n",
				"    print(predict_date, real_value, predict_value)\n",
				"    list_predict.append({\n",
				"        'date': predict_date,\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / real_value * 100\n",
				"    })\n",
				"\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = pandas.DataFrame(list_predict)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = df_predict.set_index(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(df_plot.index, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"df_plot_value.plot()\n",
				"df_plot_error = df_plot[['PercentError']]\n",
				"df_plot_error.plot()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.6.2. LSTM Style #2"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn\n",
				"import tensorflow.keras.models\n",
				"import tensorflow.keras.layers\n",
				"import tensorflow.keras.callbacks\n",
				"import tensorflow.keras.optimizers\n",
				"import datetime\n",
				"import sys\n",
				"\n",
				"def line_logging(*messages):\n",
				"    import datetime\n",
				"    import sys\n",
				"    today = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"    log_time = today.strftime('[%Y/%m/%d %H:%M:%S]')\n",
				"    log = []\n",
				"    for message in messages:\n",
				"        log.append(str(message))\n",
				"    print(log_time + '::' + ' '.join(\u25a0\u25a0\u25a0\u25a0\u25a0) + '')\n",
				"    sys.stdout.flush()\n",
				"\n",
				"\n",
				"df_test_scaled = df_source.fillna(1 + 1e-7)\n",
				"list_min_max = list()\n",
				"for col_name \u25a0\u25a0\u25a0\u25a0\u25a0 \u25a0\u25a0\u25a0\u25a0\u25a0:\n",
				"    val_min = numpy.min(df_test_scaled[col_name])\n",
				"    val_max = numpy.max(df_test_scaled[col_name])\n",
				"    if val_min == val_max:\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0 df_test_scaled[col_name]\n",
				"        \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"    list_min_max.append({\n",
				"        'col': col_name,\n",
				"        'min': val_min,\n",
				"        'max': val_max\n",
				"    })\n",
				"    df_test_scaled[col_name] = (df_test_scaled[col_name] - val_min) / (val_max - val_min)\n",
				"df_min_max = pandas.DataFrame(list_min_max)\n",
				"\n",
				"size_row = df_test_scaled.shape[0]\n",
				"size_col = df_test_scaled.shape[1]\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = dict_args['window_size']\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = size_row - size_win + 1\n",
				"line_logging(size_row, size_col, size_win, loop_count)\n",
				"\n",
				"for col_name in df_test_scaled.columns:\n",
				"    # print(col_name, numpy.min(df_test_scaled[col_name]), numpy.max(df_test_scaled[col_name]))\n",
				"    pass\n",
				"    if numpy.min(df_test_scaled[col_name]) != 0.0:\n",
				"        print(col_name, 'min', numpy.min(df_test_scaled[col_name]))\n",
				"    if numpy.max(df_test_scaled[col_name]) != 1.0:\n",
				"        print(col_name, 'max', numpy.max(df_test_scaled[col_name]))\n",
				"line_logging('check min/max value')\n",
				"\n",
				"list_train_X = list()\n",
				"list_train_Y = \u25a0\u25a0\u25a0\u25a0\u25a0()\n",
				"list_tests_X = list()\n",
				"list_tests_Y = list()\n",
				"for idx in range(loop_count):\n",
				"    df_train_sub_X = df_test_scaled.tail(\u25a0\u25a0\u25a0\u25a0\u25a0 + idx + 1).head(size_win)\n",
				"    df_train_sub_Y = df_train_sub_X.tail(1)['target']\n",
				"    del df_train_sub_X['target']\n",
				"    if idx >= loop_count - 50:\n",
				"        list_tests_X.append(df_train_sub_X.to_numpy())\n",
				"        list_tests_Y.append(df_train_sub_Y.to_numpy())\n",
				"    else:\n",
				"        list_train_X.append(df_train_sub_X.to_numpy())\n",
				"        list_train_Y.append(df_train_sub_Y.to_numpy())\n",
				"line_logging('build data layer')\n",
				"\n",
				"train_X = numpy.array(list_train_X)\n",
				"train_Y = numpy.array(list_train_Y)\n",
				"tests_X = numpy.array(list_tests_X)\n",
				"value_real = numpy.array(list_tests_Y)[0][0]\n",
				"line_logging(train_X.shape, train_Y.shape, tests_X.shape, value_real.shape)\n",
				"\n",
				"units = 2\n",
				"loss = 'mean_squared_error'\n",
				"optimizer = tensorflow.keras.optimizers.Adam(\u25a0\u25a0\u25a0\u25a0\u25a0=0.5)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = 2\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0 = 128\n",
				"monitor = 'val_loss'\n",
				"model = tensorflow.keras.models.Sequential()\n",
				"# LSTM \ub808\uc774\uc5b4 \ucd94\uac00\n",
				"# input_shape: \uccab\ubc88\uc9f8\ub294 \uc708\ub3c4\uc6b0 \ud06c\uae30, \ub450\ubc88\uc9f8\ub294 \uceec\ub7fc\uc218\n",
				"model.add(tensorflow.keras.layers.LSTM(units, input_shape=(size_win, size_col - 1), return_sequences=False))\n",
				"# Dense \ub808\uc774\uc5b4 \ucd94\uac00\n",
				"model.add(tensorflow.keras.layers.Dense(32, activation='relu'))\n",
				"model.add(\u25a0\u25a0\u25a0\u25a0\u25a0(1, activation='sigmoid'))\n",
				"line_logging('define model')\n",
				"# \ubaa8\ub378 \ucef4\ud30c\uc77c \ud65c\uc131\ud568\uc218 loss_function \uc9c0\uc815 , \ucd5c\uc801\ud654\ud568\uc218 \uc9c0\uc815\n",
				"model.compile(loss=loss, optimizer=optimizer)\n",
				"line_logging('compile model')\n",
				"# \ubaa8\ud615 \ud559\uc2b5\n",
				"model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
				"model.summary()\n",
				"value_pred = model.predict(tests_X)[0][0]\n",
				"line_logging(value_pred)\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0(value_real)\n",
				"\n",
				"# #display(df_min_max)\n",
				"target_min = df_min_max[df_min_max['col'] == 'target']['min']\u25a0\u25a0\u25a0\u25a0\u25a0()[0]\n",
				"target_max = df_min_max[df_min_max['col'] == 'target']['max'].tolist()[0]\n",
				"\n",
				"denrm_pred =  target_min + (target_max - target_min) * value_pred\n",
				"denrm_real =  target_min + (target_max - target_min) * \u25a0\u25a0\u25a0\u25a0\u25a0\n",
				"\n",
				"line_logging('PRED', value_pred, denrm_pred)\n",
				"line_logging('REAL', value_real, denrm_real)\n",
				"line_logging('ERROR Normalize:', \u25a0\u25a0\u25a0\u25a0\u25a0((value_pred - value_real) / value_real * 10000) / 100, '%')\n",
				"line_logging('ERROR De-normalize: ', int((denrm_pred - denrm_real) / denrm_real * 10000) / 100, '%')"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# 3.7. MLPRegressor"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"warnings.filterwarnings('ignore')\n",
				"\n",
				"import sklearn.neural_network\n",
				"\n",
				"model = sklearn.neural_network.MLPRegressor(random_state=0, max_iter=2, activation='relu', solver='adam', learning_rate='constant')\n",
				"\n",
				"hyper_parameter = {\n",
				"    'hidden_layer_sizes': [(\u25a0\u25a0\u25a0\u25a0\u25a0,), (\u25a0\u25a0\u25a0\u25a0\u25a0,)],\n",
				"    'alpha': [0.001],\n",
				"}\n",
				"df_tune_X = df_train.tail(dict_args['window_size'] + idx + 1 + dict_args['test_size']).head(dict_args['window_size'])\n",
				"df_tune_Y = df_tune_X['target']\n",
				"del df_tune_X['target']\n",
				"grid_search = sklearn.model_selection.GridSearchCV(estimator=model, param_grid=hyper_parameter, cv=2, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=0)\n",
				"grid_search.fit(df_tune_X, df_tune_Y)\n",
				"model_instance = grid_search.best_estimator_\n",
				"\n",
				"list_predict = list()\n",
				"for idx in range(dict_args['test_size']):\n",
				"    df_train_X = df_train.tail(dict_args['window_size'] + idx + 1).head(dict_args['window_size'])\n",
				"    df_tests_X = df_train_X.tail(1)\n",
				"    df_train_X.drop(df_train_X.tail(1).index, inplace=True)\n",
				"\n",
				"    df_train_Y = df_train_X['target']\n",
				"    df_tests_Y = df_tests_X['target']\n",
				"    del df_train_X['target']\n",
				"    del df_tests_X['target']\n",
				"\n",
				"    model_instance.fit(df_train_X, df_train_Y)\n",
				"    real_value = \u25a0\u25a0\u25a0\u25a0\u25a0()[\u25a0\u25a0\u25a0\u25a0\u25a0]\n",
				"    predict_value = model_instance.predict(df_tests_X)[0]\n",
				"    list_predict.append({\n",
				"        'date': df_tests_X.index.tolist()[0],\n",
				"        'real_value': real_value,\n",
				"        'predict_value': predict_value,\n",
				"        'PercentError': (predict_value - real_value) / \u25a0\u25a0\u25a0\u25a0\u25a0 * 100\n",
				"    })\n",
				"\n",
				"df_predict = pandas.DataFrame(\u25a0\u25a0\u25a0\u25a0\u25a0)\n",
				"# #display(df_predict)\n",
				"\n",
				"df_plot = \u25a0\u25a0\u25a0\u25a0\u25a0(['date'])\n",
				"df_plot['datetime'] = pandas.to_datetime(\u25a0\u25a0\u25a0\u25a0\u25a0, format='%Y%m%d')\n",
				"df_plot = df_plot.set_index(['datetime'])\n",
				"df_plot_value = df_plot[['real_value', 'predict_value']]\n",
				"# df_plot_value.plot()\n",
				"df_plot_error = df_plot[['PercentError']]\n",
				"# df_plot_error.plot()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"df_tune = df_std.copy()\n",
				"#display(df_tune)\n",
				"list_features = ['target']\n",
				"list_features.extend(df_corr_std.head(20)['VAR_NAME'].tolist())\n",
				"\n",
				"df_tune = df_tune[list_features]\n",
				"df_tune['datetime'] = pandas.to_datetime(df_tune.index, format='%Y%m%d')\n",
				"df_tune = df_tune.set_index(['datetime'])\n",
				"#display(df_tune)\n",
				"# df_tune.plot()\n",
				"\u25a0\u25a0\u25a0\u25a0\u25a0('data_std.csv')"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Python 3.8.13 ('test')",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.8.13"
		},
		"vscode": {
			"interpreter": {
				"hash": "7f404382b6ee812ff854d886c5a2fbb987a19b2f69a33528c318b78ba24eea58"
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}